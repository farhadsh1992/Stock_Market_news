{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid black;background-color:#4D0033;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Chapter 005</b></h1>\n",
    "    <h4> Review Paper </h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #4D0033;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#00404D;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "  <strong> Summary: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:25%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> contents-Paper:  </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#1\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Attention Is All You Need, Ashish Vaswani  </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#2\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> sequence-to-sequence methods </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#3\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Encoder and Decoder Stacks architecture </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#E67300;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#4\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Different Attention Models </a></h6>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:5px 5px 5px 5px;padding:3px\">\n",
    "    <h6><a href=\"#5\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Encoder and Decoder Stacks architecture </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#6\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Encoder Stacks architecture:  </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#7\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> The positional encoding  </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#8\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Multi-Head Attention </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#00264D;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#9\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Attention </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#00264D;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#10\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Mulit-Heads </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#FFFF00;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#11\" style=\"position: relative;padding:5px;color:black;text-align: center;\"> ADD & Norm </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#12\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Feed-Forward Network (FNN) </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#13\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Masked Multi-Head Attention:  </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#14\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Multi-Head Attention (Decoder): </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#15\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Feed Forward (Decoder): </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#16\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Linear and Softmax: </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#17\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> N $\\times$ Encoder-Decoder Attention, Optimaztion and loss funcation </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#FFFF00;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#18\" style=\"position: relative;padding:5px;color:black;text-align: center;\">  A Transformer of 2 stacked Encoders and Decoders: </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#FFFF00;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#19\" style=\"position: relative;padding:5px;color:black;text-align: center;\"> Optimizetion funcation for Attention Model: </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#FFFF00;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#20\" style=\"position: relative;padding:5px;color:black;text-align: center;\"> Loss funcation for Attention Model: </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:white;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#21\" style=\"position: relative;padding:5px;color:white;text-align: center;\">  </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:white;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#22\" style=\"position: relative;padding:5px;color:white;text-align: center;\">  </a></h6>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"A\" style=\"width:100%;height:70px;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h3>    </h3>\n",
    "    <h5>     </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> :  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>   </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>   </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"B\" style=\"width:100%;height:70px;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h3>  Attention Is All You Need  </h3>\n",
    "    <h5> 2017, Ashish Vaswani </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "(1) Paper: https://arxiv.org/abs/1706.03762\n",
    "    \n",
    "(2) code: https://paperswithcode.com/paper/attention-is-all-you-need\n",
    "   \n",
    "(3) Youtube review the paper: https://www.youtube.com/watch?v=iDulhoQ2pro   \n",
    "    \n",
    "(4) kaggle review the paper:    https://www.youtube.com/watch?v=54uLU7Nxyv8\n",
    "    \n",
    "(5) https://www.youtube.com/watch?v=rBCqOTEfxvg\n",
    "    \n",
    "(6) https://www.youtube.com/watch?v=KMY2Knr4iAs\n",
    "    \n",
    "(7) http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/    \n",
    "    \n",
    "(8) Multi-Head Attention paper: https://arxiv.org/pdf/1810.10183.pdf  \n",
    "    \n",
    "(9) https://www.youtube.com/watch?v=5Cx6eFHp2v8    \n",
    "    \n",
    "(10) http://jalammar.github.io/illustrated-transformer/    \n",
    "    \n",
    "(11) http://www.fon.hum.uva.nl/praat/manual/Feedforward_neural_networks_1__What_is_a_feedforward_ne.html \n",
    "    \n",
    "(12) code: http://juditacs.github.io/2018/12/27/masked-attention.html    \n",
    "    \n",
    "(13) code: https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec \n",
    "    \n",
    "(14)   \n",
    "    \n",
    "Self-Attention (pydata): https://www.youtube.com/watch?v=OYygPG4d9H0\n",
    "    \n",
    "Code: http://nlp.seas.harvard.edu/2018/04/03/attention.html   \n",
    "    \n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "    \n",
    "https://www.mihaileric.com/posts/transformers-attention-in-disguise/  \n",
    "    \n",
    "Code important: http://nlp.seas.harvard.edu/2018/04/03/attention.html    \n",
    "    \n",
    "- code: https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec        \n",
    "    \n",
    "<hr>\n",
    "    \n",
    "review paper: https://www.youtube.com/watch?v=S0KakHcj_rs    \n",
    "    \n",
    "review paper:    https://www.youtube.com/watch?v=yCdl2afW88k\n",
    "    \n",
    "-    https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XMFvSi-ZMWo\n",
    "    \n",
    "-    https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html\n",
    "    \n",
    "Really base code: https://d2l.ai/chapter_attention-mechanism/transformer.html  \n",
    "    \n",
    "pytorch: https://github.com/keitakurita/Practical_NLP_in_PyTorch/blob/master/deep_dives/transformer_from_scratch.ipynb    \n",
    "    \n",
    "    \n",
    "<hr>\n",
    "    \n",
    "(8) https://dzone.com/articles/self-attention-mechanisms-in-natural-language-proc\n",
    "    \n",
    "(9) Code https://www.youtube.com/watch?v=DDv89XKdFy8\n",
    "    \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#00404D;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "  <strong> Summary: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Abstract:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #009999;box-shadow: 0 4px 8px 0 #009999, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%\">\n",
    "    \n",
    "The authors introduce a trsnformer machine base on Attention mechanisms, dispensing with recurrence and convolutions entirely. <br>\n",
    "The Model is tested two machine translation tasks (1. English- to-German translation task;2. English-to-French translation task)\n",
    "\n",
    "- The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. \n",
    "- our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs,\n",
    "\n",
    "- **BLUE** is amtrix for validation model and give us a blue score as a measurement.\n",
    "\n",
    "- Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
    "- End-to-end memory networks are based on a recurrent attention mechanism.\n",
    "\n",
    "-  the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution\n",
    "\n",
    "- a network with no recurrence that only used attention (as well as a couple of other components)  only.\n",
    "\n",
    "- Attention allows the model to focus on the relevant parts of the input sequence as needed. **[REF10]**\n",
    "- The context is a vector (an array of numbers, basically) in the case of machine processing. **[REF10]**\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px;box-shadow: 0 4px 8px 0 #1E00B3, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h4> sequence-to-sequence methods:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #1E00B3;box-shadow: 0 4px 8px 0 #1E00B3, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%\">\n",
    "    \n",
    "sequence-to-sequence tasks are performed using **an encoder-decoder model**. The basic idea is that the encoder takes the sequence of input words, converts it to some intermediate representation, then passes that representation to the decoder which produces the output sequence.RNNs seemed to be born for the encoder-decoder model.[\n",
    "\n",
    "The problem with the encoder-decoder approach  is that the decoder needs different information at different timesteps.each hidden state depends on the previous hidden state. This becomes a major pain point on GPUs. **Attention Model was born** to solve this problem. The attention mechanism allows the decoder to “look back” at the entire sentence and selectively extract the information it needs during decoding.\n",
    "\n",
    "Concretely, attention gives the decoder access to all the encoder’s hidden states. The decoder still needs to make a single prediction for the next word though, so we can’t just pass it a whole sequence: we need to pass it some kind of summary vector. So what attention does is it asks the decoder to choose which hidden states to use and which to ignore by weighting the hidden states. The decoder is then passed a weighted sum of hidden states to use to predict the next word.[Ref7]\n",
    "\n",
    "**[Ref7]**\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Encoder and Decoder Stacks architecture:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #009999;box-shadow: 0 4px 8px 0 #009999, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%\">\n",
    "    \n",
    "The **encoder** maps an input sequnese of word or symbal representations $X = [x_1,x_2, ... , x_n]$ to a sequense of contunous represntions $Z=[z_1,z_2,...,z_n]$.<br>\n",
    "Given z, the **decoder** then genterates an output sequence $Y = [y_1, y_2, ... , y_n]$ of symbols one element at a time. <br>\n",
    "Notice that **this encoder-decoder model is auto-regressive**, that is mean he previously generated symbols as additional input when generating the next. <br>\n",
    "\n",
    "A Transformer model follows this overall architecture using stacked self-attention and point-wise. <br>\n",
    "**both the encoder and decoder, shown in the left and right halves of [FIG1], respectively.**\n",
    "\n",
    "The left is encoder  and the right is decoder part of trensformer model architecture.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:500px; border: 4px solid #009999\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1090/1*HunNdlTmoPj8EKpl-jqvBA.png\" style=\"height:500px;position:absolute;border-right:4px solid #009999\">\n",
    "<div style=\"width:26%;height:478px;position:absolute;left:400px;border-right:4px solid #009999;padding:1%\"><p>\n",
    "<font><b>The encoder:</b></font><br>    \n",
    "- The encoder is composed of a stack of N=6 Layers.<br>\n",
    "Each layer has two sub-layers, <br>\n",
    "    \n",
    "1. The First is <font color=\"#E67300\">a multi-head self-atention mechanism</font> <br>\n",
    "2. The second is <b>a simple  position-wise  fully connected <font color=\"#3377FF\">feed-forward network</font>.</b><br> \n",
    "3. <font color=\"#E6E600\">(Add and Norm)</font>, We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization <br>\n",
    "\n",
    "That is, the output of each sub-layer is:<br>\n",
    "<font size=\"4\"><b>LayerNorm(x + Sublayer(x))</b></font> <br>\n",
    "where Sublayer(x) is the function implemented by the sub-layer itself.    <br>\n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$.    \n",
    "    \n",
    "</p></div>  \n",
    "    \n",
    "<div style=\"width:32%;height:500px;position:absolute;left:810px;padding:1%\"><p>\n",
    "<font><b>The decoder:</b></font><br>    \n",
    "- The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, <br>\n",
    "the decoder inserts a third sub-layer,<br> \n",
    "    \n",
    "1. The Frist performs <font color=\"#E67300\">multi-head attention</font> over the output of the encoder stack. Similar to the encoder,<br>\n",
    "2. The secound is same the first, performs <font color=\"#E67300\">multi-head attention </font> over the output of the encoder stack. <br>\n",
    "3. They alos also <font color=\"#3377FF\">modify the self-attention sub-layer</font> in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. <br>  \n",
    "    \n",
    "4. <font color=\"#E6E600\">(Add+Norm)</font> employ residual connections around each of the sub-layers,followed by layer normalization. <br>\n",
    "\n",
    "</p></div> \n",
    "    \n",
    "</div>\n",
    "<div style=\"height:25px;\">\n",
    "<div style=\"width:260px;position:absolute;left:60px;border:4px solid #009999;padding:0.1%;text-align:center\"> FIG1</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#E67300 ;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Attention  architecture Models:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:300px; border: 4px solid #E67300\">\n",
    "<img  style=\"width:500px;position:absolute;\"  src=\"data:image/png;base64,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\">\n",
    "    \n",
    "<img src=\"https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png\" style=\"position:absolute;left:40%;height:300px;border-left:3px solid #E67300\">\n",
    "    \n",
    "</div>  \n",
    "\n",
    "</div>\n",
    "<div style=\"height:25px;\">\n",
    "<div style=\"width:260px;position:absolute;left:40px;border:4px solid #E67300;padding:0.1%;text-align:center\"> FIG2</div>\n",
    "<div style=\"width:260px;position:absolute;left:700px;border:4px solid #E67300;padding:0.1%;text-align:center;border-reduce:30px\"> FIG3</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"height:240px\">\n",
    "\n",
    "<div style=\"height:210px;border:3px solid #E67300; padding:1%;width:20%;position:absolute\">\n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "   \n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;border:3px solid black;background-color:black\"></div>      \n",
    "<div style=\"height:40px;width:90px;position:absolute;left:55px;top:20px;\">------------></div>\n",
    "<div style=\"height:40px;width:44px;position:absolute;left:12px;border:3px solid black;background-color:black\"></div>     \n",
    "</div> \n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:65px;border:3px solid black;background-color:black\"></div> \n",
    "<div style=\"height:50px;width:40px;position:absolute;left:35px;top:40px;border-left:3px solid black;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:35px;top:78px;\">----------------></div> \n",
    "<div style=\"height:50px;width:40px;position:absolute;left:20px;top:40px;border-left:3px solid black;\"></div>     \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:120px;border:3px solid black;background-color:black\"></div>\n",
    "<div style=\"height:42px;width:40px;position:absolute;left:20px;top:90px;border-left:3px solid black;\"></div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:20px;top:120px;\">------------------></div>     \n",
    "</div>\n",
    "</div>  \n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:30px;width:200px;position:absolute;left:20px;top:190px;border:3px solid #E67300;text-align:center\"> Encoder-decoder Attention</div>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "    \n",
    "<div style=\"height:210px;border:3px solid #E67300; padding:1%;width:20%;position:absolute;left:23%;top:0px\">\n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;border:3px solid green;background-color:green\"></div>     \n",
    "<div style=\"height:40px;width:96px;position:absolute;left:110px;top:30px;color:black\" ><-----</div>\n",
    "<div style=\"height:40px;width:96px;position:absolute;left:90px;top:30px;color:green\"><-----</div>\n",
    "<div style=\"height:40px;width:96px;position:absolute;left:55px;top:10px;color:#4D0033\">-------------></div>  \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:190px;top:30px;color:blue\"><-------</div>\n",
    "</div> \n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:65px;border:3px solid #1E00B3;background-color:#1E00B3\"></div> \n",
    "\n",
    "<div style=\"height:30px;width:40px;position:absolute;left:110px;top:40px;border-left:3px solid green;\"></div> \n",
    "<div style=\"height:87px;width:40px;position:absolute;left:90px;top:40px;border-left:3px solid green;\"></div> \n",
    "\n",
    "<div style=\"height:40px;width:96px;position:absolute;left:110px;top:58px;color:green;\">-----></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:40px;top:78px;color:#4D0033\">---------------></div> \n",
    "<div style=\"height:70px;width:40px;position:absolute;left:60px;top:20px;border-left:3px solid #4D0033;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:190px;top:68px;color:blue\">-------></div>\n",
    "<div style=\"height:102px;width:40px;position:absolute;left:203px;top:40px;border-right:3px solid blue;\"></div> \n",
    "  \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:120px;border:3px solid #4D0033;background-color:#4D0033\"></div>\n",
    "<div style=\"height:62px;width:40px;position:absolute;left:60px;top:90px;border-left:3px solid #4D0033;\"></div> \n",
    "<div style=\"height:62px;width:40px;position:absolute;left:40px;top:90px;border-left:3px solid #4D0033;\"></div> \n",
    "<div style=\"height:40px;width:96px;position:absolute;left:90px;top:110px;color:#4D0033;text-size:6px;color:green\"> --------></div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:60px;top:140px;color:#4D0033;/\"><------------------ </div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:40px;top:140px;color:#4D0033\"> <----------- </div>  \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:190px;top:130px;color:blue\"><-------</div>\n",
    "</div>\n",
    "</div>  \n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:30px;width:180px;position:absolute;left:24%;top:190px;border:3px solid #E67300;text-align:center\"> Encoder Self-attention</div>\n",
    "</div> \n",
    "    \n",
    "<div style=\"height:210px;border:3px solid #E67300; padding:1%;width:20%;position:absolute;left:46%;top:0px\">\n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "   \n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;border:3px solid #009999;background-color:#009999\"></div>      \n",
    "\n",
    "<div style=\"height:40px;width:98px;position:absolute;left:55px;top:15px;color:#009999\"><-------------</div> \n",
    "<div style=\"height:40px;width:94px;position:absolute;left:87px;top:35px;color:#009999\"><--------</div> \n",
    "</div> \n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:66px;border:3px solid #009999;background-color:#009999\"></div> \n",
    "<div style=\"height:38px;width:40px;position:absolute;left:87px;top:45px;border-left:3px solid #009999;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:87px;top:68px;color:#009999\">--------></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:87px;top:88px;color:#009999\"><--------</div> \n",
    "<div style=\"height:70px;width:40px;position:absolute;left:55px;top:26px;border-left:3px solid #009999;\"></div>     \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:120px;border:3px solid #009999;background-color:#009999\"></div>\n",
    "<div style=\"height:65px;width:40px;position:absolute;left:55px;top:90px;border-left:3px solid #009999;\"></div> \n",
    "<div style=\"height:35px;width:40px;position:absolute;left:87px;top:100px;border-left:3px solid #009999;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:87px;top:120px;color:#009999\">--------></div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:55px;top:140px;color:#009999\">-------------></div>     \n",
    "</div>\n",
    "</div>  \n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:30px;width:190px;position:absolute;left:48%;top:190px;border:3px solid #E67300;text-align:center\">MaskDecoder self-attention</div>\n",
    "</div>     \n",
    "    \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**:\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\" **[FIG2]**.<br>\n",
    "The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.\n",
    "Computing the dot products of the query with all keys,divide each by values. $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values\n",
    "\n",
    "Computing the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . compute the matrix of outputs as:\n",
    "\n",
    "## $Attention(Q,K,V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V = \\displaystyle\\sum_i \\frac{e^{q.k_i}}{\\sum_j e^{q.k_j}}V_i$\n",
    "\n",
    "Q is a matrix of all of words, and K is words embedding that was generted before. V is same K. In attention model, we would like to find most similarity key (old memmpty words) with attention input. This fucnation gives you a mask, gives a probability distribution over keys which is peaked at ones that are similar to the query and this mask gives to matrix multiplied with values which is the same as summing over values multiplied by this mask and there you are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h4> Encoder and Decoder Stacks architecture:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "        \n",
    "Code: https://github.com/tensorflow/nmt     \n",
    "    \n",
    "(11) http://www.fon.hum.uva.nl/praat/manual/Feedforward_neural_networks_1__What_is_a_feedforward_ne.html    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;width:50%\">\n",
    "<img src=\"https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/encoder.png\" style=\"height:600px\">\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%\">\n",
    "    \n",
    "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.<br>\n",
    "An attention model differs from a classic sequence-to-sequence model in two main ways:**[REF10]**\n",
    "- First, the encoder passes a lot more data to the decoder, Instead of passing the last hidden state of the encoding stage, \n",
    "- Second, an attention decoder does an extra step before producing its output. **In order to focus on the parts of the input that are relevant to this decoding time step,**\n",
    "\n",
    "**the Attention Model does the following:****[REF10]**\n",
    "1. Look at the set of encoder hidden states it received – each encoder hidden states is most associated with a certain word in the input sentence.\n",
    "2. Give each hidden states a score (let’s ignore how the scoring is done for now).\n",
    "3. Multiply each hidden states by its softmaxed score,\n",
    "4. amplifying hidden states with high scores,\n",
    "5. drowning out hidden states with low scores\n",
    "6. sum all vector and extract as one vector.\n",
    "(This scoring exercise is done at each time step on the Attention Model (encoder-decoder) side.)\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:480px;\">\n",
    "<img src=\"picture/Screenshot 2019-04-26 at 18.42.30.png\" style=\"height:460px;width:80%;position:absolute;border:5px solid #1E00B3\">    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h4> Encoder Stacks architecture:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 1 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  The positional encoding </h5>\n",
    "</div>    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.3%\">\n",
    "<div style=\"height:150px\">\n",
    "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_49_0.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">    \n",
    "</div>\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:100px;text-align:center;position:absolute;left:12%;border:3px solid #1E00B3\"><b>fig8</b></div> \n",
    "<div style=\"width:100px;text-align:center;position:absolute;left:50%;border:3px solid #1E00B3\"><b>fig9</b></div> \n",
    "<div style=\"width:100px;text-align:center;position:absolute;left:78%;border:3px solid #1E00B3\"><b>fig10</b></div>    \n",
    "</div>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:2%\">\n",
    "    \n",
    "## $PE_{position,2i} = \\sin(\\frac{position}{10000^{(2i/d_{model})}})$\n",
    "\n",
    "## $PE_{position,2i+1} = \\cos(\\frac{position}{10000^{(2i/d_{model})}})$\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%\">\n",
    "    \n",
    "The words as a input gives to embedding layer and embedding layer encoded to a numric vectors of embedding  in the sequence. Then the embedding vectors add by Positional Encoding funcation. <br>\n",
    "**The positional encoding** must be preserved to represent the time in some way to preserve the positional encoding. In case of the Transformer authors propose to encode time as sine wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing2. we give a value to postion of values vectors\n",
    "\n",
    "Now, Probably **we asked that why we should used?** So, the answer is that we have varity input (for translate: varity langauge), every this input has differnt sequenses. but we have fix recurrent embedding and so length of fix  embedding vectors as output of embedding .Then we should give a effect of this varity dimensions of words. See figure9, <font color=\"green\">je</font> has two dimensions and after embedding it is took a four vector and alos <font color=\"green\">suis</font> and <font color=\"green\">etudiant</font> with more than two dimension lenght have a 4 dimension length. we can add this different lenght of words by add a Positional Vector. look the  Positional Vectors in fig9. they are change by dimension of words and you can see effect of posistion of vectors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure10** shows a heatmap of Positional encoder. top of picture is for two dimensions and increses the dimensions when we are going down picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stage1_out = Embedding512 + TokenPositionEncoding512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 2 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Multi-Head Attention </h5>\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px;box-shadow:  0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "        \n",
    "http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/     \n",
    "    \n",
    "(10) http://jalammar.github.io/illustrated-transformer/        \n",
    "    \n",
    "Multi-Head Attention paper: https://arxiv.org/pdf/1810.10183.pdf    \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stage 2 and stage 3 f two sub-layers composed of encoder are  made up of a stack of N=6 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"picture/Screenshot 2019-04-25 at 22.12.09.png\" style=\"height:400px;border:3px solid #1E00B3\">\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" style=\"height:400px;border:3px solid #1E00B3\">\n",
    "    \n",
    "</div>  \n",
    "<div style=\"height:50px\">\n",
    "<div style=\"width:200px;text-align:center;position:absolute;left:12%;border:3px solid #1E00B3; box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\"><b>fig11</b></div> \n",
    "<div style=\"width:200px;text-align:center;position:absolute;left:50%;border:3px solid #1E00B3;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\"><b>fig12</b></div> \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/attention_path_length.png?w=1680\" style=\"height:330px;border:3px solid #1E00B3\">\n",
    "<img src=\"https://i0.wp.com/mlexplained.com/wp-content/uploads/2017/12/attention_concept.png?resize=768%2C453\" style=\"height:330px;border:3px solid #1E00B3\">\n",
    "    \n",
    "</div>  \n",
    "<div style=\"height:50px\">\n",
    "<div style=\"width:200px;text-align:center;position:absolute;left:12%;border:3px solid #1E00B3; box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\"><b>fig13</b></div> \n",
    "<div style=\"width:200px;text-align:center;position:absolute;left:50%;border:3px solid #1E00B3;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\"><b>fig14</b></div> \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:20%;height:28px;border: 4px solid black;background-color:#00264D;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:0%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Attention </h5>\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:5px solid #1E00B3; padding: 1%\">\n",
    "    \n",
    "**The first step** in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.**[REF10]**\n",
    "\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:7px solid #660044; padding: 1%\">\n",
    "    \n",
    "Let's answer this question, **What the “query”, “key”, and “value” vectors are?**<br>\n",
    "    \n",
    "The first step is to calculate the <font color=\"#9500B3\" >Query</font>, <font color=\"#FF8000\" >Key</font>, and <font color=\"#198CFF\" >Value</font> matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (<font color=\"#9500B3\" size=\"4px\">$W_Q$</font>, <font color=\"#FF8000\" size=\"4px\">$W_K$</font>, <font color=\"#198CFF\" size=\"4px\">$W_V$</font>).    \n",
    "    \n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:5px solid #00AAFF; padding: 1%\">\n",
    "\n",
    "**The second step** in calculating self-attention is to calculate a score. calculating the self-attention for the target word We need to score each word of the input sentence against this tagrget word. **The score determines how much focus to place on other parts of the input sentence** as we encode a word at a certain position. The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring.**[REF10]**\n",
    "\n",
    "So if we’re processing the self-attention for the tagrget word in position $t$, the first score would be the dot product of $q_1$ and $k_1$. The second score would be the dot product of $q_t$ and $k_2$.**[REF10]**\n",
    "\n",
    "</div>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:5px solid #99EEFF; padding: 1%\">\n",
    "\n",
    "**The third step** is to divide the scores by the square root of the dimension of the key vectors used. This leads to having more stable gradients.\n",
    "\n",
    "**The forth step** is to pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.**[REF10]**\n",
    "    \n",
    "</div>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:5px solid #00FFD5; padding: 1%\">\n",
    "    \n",
    "**The fifth step** is to multiply each value vector by the softmax score. The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).**[REF10]**\n",
    "    \n",
    "**The sixth step** is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word). That concludes the self-attention calculation.\n",
    "\n",
    "**[REF10]**    \n",
    "\n",
    "</div>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector is one we can send along to the feed-forward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:3%\">\n",
    "    \n",
    "## $Attention(Q,K,V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V = \\displaystyle\\sum_i \\frac{e^{q.k_i}}{\\sum_j e^{q.k_j}}V_i$\n",
    "    \n",
    "</div>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "<div style=\"hegiht:100px\">\n",
    "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" style=\"position:absoluate;height:260px;border:3px solid #1E00B3;width:14%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_score.png\" style=\"position:absoluate;height:260px;border:3px solid #00AAFF;width:29%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/self-attention_softmax.png\" style=\"position:absoluate;height:260px;border:3px solid #99EEFF;width:26%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/self-attention-output.png\" style=\"position:absoluate;height:260px;border:3px solid #00FFD5;width:28%\">\n",
    "</div>\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:13.6%;text-align:center;position:absolute;left:0.5%;border:3px solid #1E00B3;\"><b>step 1</b></div> \n",
    "<div style=\"width:28.5%;text-align:center;position:absolute;left:14.9%;border:3px solid #00AAFF;\"><b>step 2</b></div> \n",
    "<div style=\"width:12.4%;text-align:center;position:absolute;left:44.2%;border:3px solid #99EEFF;\"><b>step 3</b></div>\n",
    "<div style=\"width:12.4%;text-align:center;position:absolute;left:57.3%;border:3px solid #99EEFF;\"><b>step 4</b></div>\n",
    "<div style=\"width:13.4%;text-align:center;position:absolute;left:70.4%;border:3px solid #00FFD5;\"><b>step 5</b></div>   \n",
    "<div style=\"width:13.4%;text-align:center;position:absolute;left:84.4%;border:3px solid #00FFD5;\"><b>step 6</b></div>      \n",
    "</div>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"hegiht:100px;border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" style=\"position:absoluate;height:240px;border:3px solid #00AAFF\">\n",
    "<img src=\"\" style=\"position:absoluate;height:30px;border:3px solid #00AAFF\">\n",
    "<img src=\"\" style=\"position:absoluate;height:40px;border:3px solid #99EEFF\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "<div style=\"height:30px;border:3px solid #1E00B3;width:100%;text-align:center\"><b>\n",
    "    calculate the <font color=\"#9500B3\" >Query</font>, <font color=\"#FF8000\" >Key</font>, and <font color=\"#198CFF\" >Value</font> matrices\n",
    "</div></b>\n",
    "<div style=\"hegiht:100px\">\n",
    "<img src=\"\" style=\"position:absoluate;height:40px;border:3px solid #1E00B3\">\n",
    "<img src=\"\" style=\"position:absoluate;height:230px;border:3px solid #00AAFF\">\n",
    "<img src=\"\" style=\"position:absoluate;height:240px;border:3px solid #99EEFF\">\n",
    "<img src=\"\" style=\"position:absoluate;height:40px;border:3px solid #00FFD5\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:20%;height:28px;border: 4px solid black;background-color:#00264D;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:0%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Mulit-Heads </h5>\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"border: 5px solid #00264D;padding:1%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "\n",
    "we can improve the performance of the attention layer by adding a other mechanism called “multi-headed” attention to Attention model.\n",
    "The addition abilities of multi-headed than  a simple Attention model are:\n",
    "1. It expands the model’s ability to focus on different positions. z1 could be dominated by the the actual word itself.( It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.)    \n",
    "2. It gives the attention layer multiple “representation subspaces”. We have multiple sets of <font color=\"#9500B3\" >Query</font>, <font color=\"#FF8000\" >Key</font>, and <font color=\"#198CFF\" >Value</font> weight matrices. (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n",
    "    \n",
    "<hr>\n",
    "    \n",
    "**So, let see attention model and change every 6 step to arrive the Multi-Head Attention:**<br>\n",
    "- **(Step-1),** we produce n-th series of  <font color=\"#9500B3\" >Query</font>, <font color=\"#FF8000\" >Key</font>, and <font color=\"#198CFF\" >Value</font> weight matrices. (in fgure 8 series of them), instead of just one series.<br>\n",
    "- **(Step-2) to (step-6),** we do  all 5 steps from 2 to 6 for all n-th series vectors.<br>\n",
    "- <font color=\"#990066\" ><b>(step-7),</b></font> we need this extra step, because there is n(=8) <font color=\"#B3001E\" size=\"3px\">z</font> vectors and the feed-forward layer is not expecting n matrices. it’s expecting a single matrix (a vector for each word). So we need a way to condense these n down into a single matrix. This leaves us with a bit of a challenge.  We concat the matrices  <br>\n",
    "- <font color=\"#990066\" ><b>(step-8),</b></font> Finally, the Concatenate matrix of all attention head <font color=\"#B3001E\" size=\"3px\">(z)</font>multiple them by an  additional new weights matrix <font color=\"#FF33FF\" size=\"3px\">$W_O$</font>.    \n",
    "    \n",
    "(see all steps for Multi-head in figure 8,below)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"border: 5px solid #00264D;padding:1%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\" style=\"position:absoluate;height:260px;border:3px solid #00264D;width:34%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_z.png\" style=\"position:absoluate;height:260px;border:3px solid #00264D;width:29%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" style=\"position:absoluate;height:260px;border:3px solid #00264D;width:34%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" style=\"position:absoluate;height:360px;border:3px solid #00264D;width:64%\"> <img style=\"position:static;height:360px;border:3px solid #00264D;width:34%\"  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"border: 5px solid #00264D;padding:1%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "Let’s revisit our example to see where the different between Attention and Multi-Head attention:<br><br>\n",
    "\n",
    "<div style=\"height:300px\">    \n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png\" style=\"position:absolute;height:280px;border:3px solid #1E00B3;width:34%;left:4%\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png\" style=\"position:absolute;height:280px;border:3px solid #00AAFF;width:34%;left:50%\">\n",
    "</div>\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:38%;text-align:center;position:absolute;left:2%;border:3px solid #1E00B3;\"><b>Attention</b></div> \n",
    "<div style=\"width:38%;text-align:center;position:absolute;left:48%;border:3px solid #00AAFF;\"><b>Multi-Head attention</b></div> \n",
    "    \n",
    "</div>\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:20%;height:28px;border: 4px solid black;background-color:#FFFF00;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:0%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);color:#00264D\">\n",
    "    <h5>  ADD & Norm </h5>\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"border: 5px solid #00264D;padding:1%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "\n",
    "## Layer-Normalize($X_{input}+Z$)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 3 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Feed-Forward Network (FNN) </h5>\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "(11) http://www.fon.hum.uva.nl/praat/manual/Feedforward_neural_networks_1__What_is_a_feedforward_ne.html    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"border: 5px solid #1E00B3;padding:1%;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "<div style=\"width:48%;position:absolute;\">\n",
    "    \n",
    "A feedforward neural network is a biologically inspired classification algorithm. <br>\n",
    "It consist of a (possibly large) number of simple neuron-like processing units, organized in layers. Every unit in a layer is connected with all the units in the previous layer. These connections are not all equal: each connection may have a different strength or weight. The weights on these connections encode the knowledge of a network. <br>\n",
    "\n",
    "Data enters at the inputs and passes through the network, layer by layer, until it arrives at the outputs. During normal operation, that is when it acts as a classifier, there is no feedback between layers. This is why they are called feedforward neural networks.**[REF11]**\n",
    "</div>\n",
    "<div style=\"width:48%;height:300px\">\n",
    "<img src=\"http://www.fon.hum.uva.nl/praat/manual/Feedforward_neural_networks_1__What_is_a_feedforward_ne_1.png\" style=\"position:absolute;width:40%;height:300px;left:50%\">    \n",
    "</div>    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h4> Decoder Stacks architecture:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#00404D;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #00404D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 1 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  The positional encoding </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.3%\">\n",
    "    \n",
    "## Like encoder:    \n",
    "<div style=\"height:150px\">\n",
    "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_49_0.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">    \n",
    "</div>\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:100px;text-align:center;position:absolute;left:12%;border:3px solid #1E00B3\"><b>fig8</b></div> \n",
    "<div style=\"width:100px;text-align:center;position:absolute;left:50%;border:3px solid #1E00B3\"><b>fig9</b></div> \n",
    "<div style=\"width:100px;text-align:center;position:absolute;left:78%;border:3px solid #1E00B3\"><b>fig10</b></div>    \n",
    "</div>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "<b>What is Outputs Shifted right that is as inputs here?</b>\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#00404D;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #00404D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 2 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Masked Multi-Head Attention: </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "(12) code: http://juditacs.github.io/2018/12/27/masked-attention.html\n",
    "    \n",
    "(13) code: https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec     \n",
    "    \n",
    "(14) \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "We most often have to deal with variable length sequences in Attention but we require each sequence in the same batch (or the same dataset) to be equal in length if we want to represent them as a single tensor. Padding shorter sentences to the same length as the longest one in the batch is the most common solution for this problem.**[REF12]**<br> Mask is a Matrix that assign non-zero attention weights to real symbols, and zero weights to pad symbols, see figure 24.  **[REF13]**\n",
    "\n",
    "In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.**[REF10]**    \n",
    "    \n",
    "    \n",
    "Masking plays an important role in the transformer. It serves two purposes:\n",
    "1. In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences.\n",
    "2. In the decoder: To prevent the decoder ‘peaking’ ahead at the rest of the translated sentence when predicting the next word.**[REF13]**    \n",
    "\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:70%;border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "<div style=\"height:154px\">\n",
    "<img src=\"http://juditacs.github.io/assets/padded_sequence.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "<img src=\"http://juditacs.github.io/assets/softmax_before_after.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "   \n",
    "</div>\n",
    "\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:22%;text-align:center;position:absolute;left:1.3%;border:3px solid #1E00B3\"><b>Mask(or Padding)</b></div> \n",
    "<div style=\"width:22.2%;text-align:center;position:absolute;left:24.1%;border:3px solid #1E00B3\"><b>Whithout mask</b></div>\n",
    "<div style=\"width:21.2%;text-align:center;position:absolute;left:47.1%;border:3px solid #1E00B3\"><b>with mask</b></div>     \n",
    "   \n",
    "</div>\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:67%;text-align:center;position:absolute;left:1.3%;border:3px solid #00264D\"><b>fig24</b></div>     \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#00404D;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #00404D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 3 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Multi-Head Attention (Decoder): </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:395px\">\n",
    "<div style=\"position:absolute;width:43%;border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "    \n",
    "<div style=\"height:315px\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_decoding_2.gif\" style=\"position:absoluate;height:310px;border:3px solid #1E00B3\">\n",
    "</div>\n",
    "\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:94%;text-align:center;position:absolute;left:1.3%;border:3px solid #1E00B3\"><b>(A) Multi-Head Attention (Decoder)</b></div> \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:94%;text-align:center;position:absolute;left:1.3%;border:3px solid #00264D\"><b>fig24</b></div>     \n",
    "</div>\n",
    "</div>\n",
    "    \n",
    "<div style=\"position:absolute;left:46%;width:50.5%;border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "The Architecture od Multi-head is same encoder, but there is differnt in input that is feeded to model. Get vectors as input  from encoder output, and also the output of each step is fed to the bottom decoder in the next time step and the decoders excute their decoding results just like the encoders did, see  Fig25.(just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.)\n",
    "\n",
    "The self attention layers in the decoder operate in a slightly different way than the one in the encoder:    \n",
    "**The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.**  <br>  \n",
    "**[REF11]**  \n",
    "    \n",
    "</div> \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#00404D;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #00404D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 4 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Feed Forward (Decoder): </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "## Like Encoder part\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#00404D;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #00404D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Stage 5 </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>  Linear and Softmax: </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "The decoder stack outputs a vector of floats and we sould turn that into a word by the final Linear layer which is followed by a Softmax Layer.<br>\n",
    "The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.<br>\n",
    "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.    \n",
    "\n",
    "**[REF10]**\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h4> N $\\times$ Encoder-Decoder Attention, Optimaztion and loss funcation:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "(10) http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "(15) https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\n",
    " \n",
    "(16) https://colah.github.io/posts/2015-09-Visual-Information/    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #990000, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#FFFF00;color:black;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 #FFFF00, 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n",
    "    <h5>  A Transformer of 2 stacked Encoders and Decoders: </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:52%;border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>    \n",
    "    \n",
    "<div style=\"height:404px\">\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" style=\"position:absoluate;height:400px;border:3px solid #1E00B3\">\n",
    "\n",
    "   \n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:51.1%;text-align:center;position:absolute;left:1.3%;border:3px solid #00264D\"><b>Fig25</b></div>     \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #990000, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#FFFF00;color:black;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 #FFFF00, 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n",
    "    <h5>  Optimizetion funcation for Attention Model: </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "They used Kullback–Leibler divergence as a optimaize funcation.\n",
    "\n",
    "**[REF]**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #990000, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#FFFF00;color:black;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 #FFFF00, 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n",
    "    <h5>  Loss funcation for Attention Model: </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "\n",
    "They used cross-entropy as a loss funcation.\n",
    "\n",
    "**[REF]**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"B\" style=\"width:100%;height:70px;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h3>  Multi-Head Attention with Disagreement Regularization  </h3>\n",
    "    <h5> 2018, Jian Li </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "Multi-Head Attention paper: https://arxiv.org/pdf/1810.10183.pdf          \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#00404D;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "  <strong> Summary: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:30px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:1px;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.19), 0 6px 20px 0 #009999\">\n",
    "    <h4> Abstract:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
