{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid black;background-color:#4D0033;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Chapter 003</b></h1>\n",
    "    <h4> Review Paper </h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #4D0033;padding:9px;box-shadow: 0 4px 8px 0 #661100, 0 6px 20px 0 rgba(0, 0, 0, 0.19)'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#00404D;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "  <strong> Summary: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:25%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> contents-Paper:  </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"A\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Universal Sentence Encoder </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"B\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Attention Is All You Need  </a></h6>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>   </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>   </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"A\" style=\"width:100%;height:70px;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h3>  Universal Sentence Encoder  </h3>\n",
    "    <h5>  2018 ,by Google Research </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "(1) Paper: [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)\n",
    "    \n",
    "(2) code: https://tfhub.dev/google/universal-sentence-encoder/2\n",
    "   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#00404D;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "  <strong> Summary: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Abstract:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- present Two models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks.\n",
    "- Our pre-trained sentence encoding models are made freely available for download and on TF Hub.\n",
    "- They include experiments with varying amounts of transfer task training data to illustrate the relation- ship between transfer task performance and training set size.\n",
    "\n",
    "1. One makes use of the **transformer** <font color=\"blue\">(Vaswani et al., 2017),Universal 03 (Attention Is All You Need).pdf\n",
    " </font>\n",
    "2. other is formulated as a **deep averaging network (DAN)** <font color=\"blue\">(Iyyer et al., 2015).</font>\n",
    "3. Both models are implemented in TensorFlow <font color=\"blue\">(Abadi et al., 2016)</font> and are available to download from TF Hub:1\n",
    "\n",
    "- **The models take as input English strings and produce as output a fixed dimensional embedding representation of the string.**\n",
    "\n",
    "- The sources are Wikipedia, web news, web question-answer pages and discussion forums.\n",
    "- Ÿèthey augment unsupervised learning with training on supervised data from the Stanford Nat- ural Language Inference (SNLI) corpus <font color=\"blue\">(Bowman et al., 2015), SNLI_corpus.pdf,SNLI_corpus02.pdf</font>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Transformer:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used a sub graph that This sub-graph uses at- tention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words.\n",
    "\n",
    "- The context aware word representations are con- verted to a fixed length sentence encoding vector by computing the element-wise sum of the repre- sentations at each word position.\n",
    "- The encoder takes as input a lowercased PTB tokenized string.\n",
    "- outputs a 512 dimensional vector as the sen- tence embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets that were used in transform task:\n",
    "\n",
    "1. **MR** : Movie review snippet sentiment on a five star scale (Pang and Lee, 2005).\n",
    "2.  **CR** : Sentiment of sentences mined from cus- tomer reviews (Hu and Liu, 2004).\n",
    "3.  **SUBJ** : Subjectivity of sentences from movie re- views and plot summaries (Pang and Lee, 2004).\n",
    "4. **MPQA** : Phrase level opinion polarity from news data (Wiebe et al., 2005).\n",
    "5. **TREC** : Fine grained question classification sourced from TREC (Li and Roth, 2002).\n",
    "6. **SST** : Binary phrase level sentiment classifica- tion (Socher et al., 2013).\n",
    "7. **STS Benchmark** : Semantic textual similar- ity (STS) between sentence pairs scored by Pear- son correlation with human judgments (Cer et al., 2017).\n",
    "8. **WEAT** : Word pairs from the psychology liter- ature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they apply a word-level embedding model. This word embedding is contain **a word2vec skip-gram model trained** on a corpus of news data.<br>\n",
    "The pretrained word embeddings are included as input to two model types:\n",
    "1. a convolutional neural network models (CNN) <font color=\"blue\">(Kim, 2014)</font>;\n",
    "2. a DAN\n",
    "\n",
    "The baselines that use pretrained word embeddings allow us to contrast word versus sentence level transfer. <br>\n",
    "Baseline CNN and DNA models trained without using any pretrained word or sentence embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Deep Averaging Network (DAN):  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second encoding model makes use of a deep averaging network (DAN) (Iyyer et al., 2015) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network (DNN) to produce sentence embeddings.\n",
    "\n",
    "-  the DAN encoder takes as input a lowercased PTB tokenized string.\n",
    "- outputs a 512 dimensional sentence embedding.\n",
    "\n",
    "\n",
    "The primary advantage of the DAN encoder is that compute time is linear in the length of the in- put sequence.<br>\n",
    "Thier results demonstrate that DANs achieve strong base- line performance on text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> EXperiment:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"B\" style=\"width:100%;height:70px;border: 4px solid black;background-color:#4D0033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h3>  Attention Is All You Need  </h3>\n",
    "    <h5> 2017 ,by Ashish Vaswani (google)   </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "(1) Paper: https://arxiv.org/abs/1706.03762\n",
    "    \n",
    "(2) code: https://paperswithcode.com/paper/attention-is-all-you-need\n",
    "   \n",
    "(3) Youtube review the paper: https://www.youtube.com/watch?v=iDulhoQ2pro   \n",
    "    \n",
    "(4) kaggle review the paper:    https://www.youtube.com/watch?v=54uLU7Nxyv8\n",
    "    \n",
    "-    https://www.youtube.com/watch?v=rBCqOTEfxvg\n",
    "    \n",
    "-    https://www.youtube.com/watch?v=KMY2Knr4iAs\n",
    "    \n",
    "Self-Attention (pydata): https://www.youtube.com/watch?v=OYygPG4d9H0\n",
    "    \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#00404D;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "  <strong> Summary: </strong><br>\n",
    "        \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Abstract:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors introduce a trsnformer machine base on Attention mechanisms, dispensing with recurrence and convolutions entirely. <br>\n",
    "The Model is tested two machine translation tasks (1. English- to-German translation task;2. English-to-French translation task)\n",
    "\n",
    "- The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. \n",
    "- our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs,\n",
    "\n",
    "- **BLUE** is amtrix for validation model and give us a blue score as a measurement.\n",
    "\n",
    "- Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
    "- End-to-end memory networks are based on a recurrent attention mechanism.\n",
    "\n",
    "-  the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#009999;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Encoder and Decoder Stacks architecture:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **encoder** maps an input sequnese of word or symbal representations $X = [x_1,x_2, ... , x_n]$ to a sequense of contunous represntions $Z=[z_1,z_2,...,z_n]$.<br>\n",
    "Given z, the **decoder** then genterates an output sequence $Y = [y_1, y_2, ... , y_n]$ of symbols one element at a time. <br>\n",
    "Notice that **this encoder-decoder model is auto-regressive**, that is mean he previously generated symbols as additional input when generating the next. <br>\n",
    "\n",
    "A Transformer model follows this overall architecture using stacked self-attention and point-wise. <br>\n",
    "**both the encoder and decoder, shown in the left and right halves of [FIG1], respectively.**\n",
    "\n",
    "The left is encoder  and the right is decoder part of trensformer model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:500px; border: 4px solid #009999\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1090/1*HunNdlTmoPj8EKpl-jqvBA.png\" style=\"height:500px;position:absolute;border-right:4px solid #009999\">\n",
    "<div style=\"width:26%;height:478px;position:absolute;left:400px;border-right:4px solid #009999;padding:1%\"><p>\n",
    "<font><b>The encoder:</b></font><br>    \n",
    "- The encoder is composed of a stack of N=6 Layers.<br>\n",
    "Each layer has two sub-layers, <br>\n",
    "    \n",
    "1. The First is <font color=\"#E67300\">a multi-head self-atention mechanism</font> <br>\n",
    "2. The second is <b>a simple  position-wise  fully connected <font color=\"#3377FF\">feed-forward network</font>.</b><br> \n",
    "3. <font color=\"#E6E600\">(Add and Norm)</font>, We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization <br>\n",
    "\n",
    "That is, the output of each sub-layer is:<br>\n",
    "<font size=\"4\"><b>LayerNorm(x + Sublayer(x))</b></font> <br>\n",
    "where Sublayer(x) is the function implemented by the sub-layer itself.    <br>\n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$.    \n",
    "    \n",
    "</p></div>  \n",
    "    \n",
    "<div style=\"width:32%;height:500px;position:absolute;left:810px;padding:1%\"><p>\n",
    "<font><b>The decoder:</b></font><br>    \n",
    "- The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, <br>\n",
    "the decoder inserts a third sub-layer,<br> \n",
    "    \n",
    "1. The Frist performs <font color=\"#E67300\">multi-head attention</font> over the output of the encoder stack. Similar to the encoder,<br>\n",
    "2. The secound is same the first, performs <font color=\"#E67300\">multi-head attention </font> over the output of the encoder stack. <br>\n",
    "3. They alos also <font color=\"#3377FF\">modify the self-attention sub-layer</font> in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. <br>  \n",
    "    \n",
    "4. <font color=\"#E6E600\">(Add+Norm)</font> employ residual connections around each of the sub-layers,followed by layer normalization. <br>\n",
    "\n",
    "</p></div> \n",
    "    \n",
    "</div>\n",
    "<div style=\"height:25px;\">\n",
    "<div style=\"width:260px;position:absolute;left:60px;border:4px solid #009999;padding:0.1%;text-align:center\"> FIG1</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#E67300 ;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Attention  architecture Models:  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:300px; border: 4px solid #E67300\">\n",
    "<img  style=\"width:500px;position:absolute;\"  src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUQAAACcCAMAAAAwLiICAAAB71BMVEX///8AAAC7izzw8PDqr0zl5eXq6uoYGBiysrL7+/v4+PjY2NhmZmajo6N5eXnpqjzg4OC8vLyTk5PAzOPCwsLprEO3hCpeXl7Ozs5vb2/S0tKbm5svLy/99+3z9fqKiopUVFQ+Wo4LCwuAgIB3kcRhgLtERERNiUOnp6f78eFQUFAlJSViYmJaqEv45cj4//9WeLekr8ZOZpW/x9f67Nbu4tHVuY4xUYn23rkcHBzxzJHo2MFEGXY9gTFeL5q2giOwv9zd4/DN1uiDm8nst2A4ODiTp8/l3u5jeKFPLH1KcLRYJJeZx5GrlYGjinpohr58pnWuoMHd6NvRytyJa7Tyz5nuvnLMqXPCl1HGnl/eyKf01qiesNS8sc2Mmrd5iq1CUWW0p5dteId4bWR9kaHRwK1VbHuQiHvi0MV0XUm6ydVMX2spMzsdFiBrsF1dP4evnMrH2MSrxaeUgq4+CnPToUx1UKiRw4gveh9nOqDV6NGXfL2827dSpUAkQVovKkONemhTMypmRy+MoLKMdWVgVF0/XXrAs545Jio7QVB2ZVFvip8iCwteVEdEIQBFOi8ACiIONk5TOxoAKEoAF0A/Jj7Psat1XZeMsIeMWIBUD5dcd3FkmFupip67kH+vq8F6lXyxwpG0q19+T43bu6Bd97nFAAATyUlEQVR4nO1di18aSbYuFZFulIeiPLQbUGOE+EKEGDAqxKgZhySaiK8kCoIazHWicSaTjNlxQ2Yd47iZ2dfk7s3evXvv/qH3VAMKUv2AOFFMf/n90q9TRfP1eVXR1kFIhgwZMmTI+FxAMYwDwKRBZXbwyb7j846UEA07fX19sOfBYJizvv2zBOPo6l/A6O/iKGGorCsLPgC+4vfPc/D7PUBf1/RIvSIc/nrOO+/B5NEOv3eufM7rd5zhFzkTMH39076wb3qhy3FMXOZa10J4ZMS30HdCv2jGM/+1QqHVfr2A+aKyyaM83vKhcq//ZF8XFJSj3zcyEp7ud5Bs0NEfVmjDefyl0AXXFNMOTN7sbIa8+bmhhpl5vO+ZGRoqn/H8pnd/DoBNUavw9fO4sD6fQlvv6+LRpgXQQUX/UVd+b3kDMIm7wlrYMANaSM+XDw3Nzv8Gd35e0Lcwoq0P9/N5Lsc0kBTmYxD1K+rr6xdyzzH+mfKhWa+HSvHIaeF8Q0PDrP807/vcgOryYUvs473ePwJqtsAbZB1hLVg50f4zWoj8s5wWMnMN5Q0z9Knc9nlCV1iLfRnvdWa6Xqsd6eLvoF9br6jv57ua5hHUcQ7U0Mt4gcXyi+Ua02YqILEAhqoQEvBpFYp6Xi0GUP45iCleBnlmgcb5+fLy8qEL5BkdPlCysBABfYp6hXZaKDUJYw7FkkDHzFBDAzjFeVDD2RlgscFbxO2eRzBhbb0whVjL6hViEuIc4g+baWgYmnNgn8jhgrAoaqeIgcFHPTFgHHcCHGoFOzkCGDNQR3nTLA5dgCDtAIK0PkGCGAXmULgXrIc+qZ/pHYKY4phPs1he8kPqfqxBC4IijEIhxiEaARmF9MGcH/hr8Kd1sWFGcrvzCcxhvYgVgqoqRoRFunA3vMkNARyL8zNpgy7lOYkun68ea5BPQBMXfGEsE/YJMO3zcYoYDhdAxjzH4kWILQqOw5PDtBww9SkZITXr03IigjliHjgtTKtiQyENzxs4KxwR1h8pMpxTGCksPjBDwN5smsSSHriE67WiIdUnTUYk8uRjrvwIDSWd5TjEggqGQkLEGJku+LO9MHaBhBGjxAd/UkxQikwRU9Wp311SKPlMUYaMzwPBs76Bi4C6s76Bi4A8EitMAIhEOtio4RgfKs/gxj4aFc1VVWUmhPRlVVVX4NjWXFVmPSFzE861I6QGkTL4ktayqmbbCRFzWVWZJr+3HOSRqL7Z1nazAqFa2Grw58Ch4dS+2adEJYBGiMZbOFTCpuKESPoclRapgM1Jhck0O9FbDqSZM31RfrMyndTFfCirxbtx5vIhjURroySxcwVdnsIA9MbsI6WO1O5a9hFtIfVdlqvSeSTWkj67FEnUkO5Z35F9pCIpnS7H5VnyHCCGGIlNJOpLkcSWGsLJXE1UmQkiuZqobiL1LUbiTTXphmpJXZ1vEEm05GgDkURlS/YRmUSziE+8SdJENcl5nHMQzTkXRHPOhaVKwkflkdhB0sRShISMQoIIdTIrIuEzG7FQEnI1CbSdEJFGouWiqGduYCEiN7CQIRZYiLCWYGBRkcZZuSmOqZ0gkpviVBKzczES20hKV5IpjoQ8kZjinEKeSIzOJUkiMU+0ZR+RSWzOPjrFPPHCkKjMUREiibQp+8hCJNGQ+4OLtBFLKf49QaNKVMQg7up1EqYj8klUleTk4dniM8sTdeJToxXiGozai5kKq5HQ8XkDRfJBuXkiUSQ3sCBidi4WWEykhL0UA4uVlASeSHE0BJHcFEfdRupbdCrswkRnK5410anVajxDChs1KJUqdyrMqANdVMIlXVoEyFHnjFgsV3RwrgJfy+pNNMXR4CfYWFtrteB8HbYUT+J6ztHe1ASsma80XYFBB93c1HQNchdVzgDE1FQFlDReaWrC3LaBCHx3Sw7PlU3XwI0arjXl9NaUa+Tsyc9WWXF+ZQXAZ+parNYWIFF1UcbOMmTIkCFDhgwZMmTIkCFDhgwZnwyUWm3J/R1EeTRBq1OfnG7JetuQu5Q1l1uZN4Pz+cCz31ib+sOJDCFv2Mzuu1rNfjd3QKUvL3dnrjnKgmiXhWOau+xP7rEZyc8PngNlJTq07aO3tljc2cbeNv7MLlcn41VJhH5AaDd4u2MFrrsitv3uN8Y/U4dG11pVDKFQexT9FPujarc6xtx0xkJLq8q3xgNHkzN21t/oDODZb9Gwf0W39QfIdNjtj66jXcOS/knE5WhX/9CNIqqf9e9qk8jkRHFNFJn31lV/iQO/6CdDc/du9/LUL/om0woyrrlCqgP02BJF35/1NzoDeDAjf0UhfRQ1HnavAYlvDCuV1khCqQISqXe6Xyob25Oo0YbiLSuoeu9AF427EIqvGFaTu91vp5Yqreooqk6TqD7A6vvZwXPF2Zb4AUXYN23JtbZ9NuR0ssvOWNwN1x63talQyLmOltsS8bb17lXnOnUIYnBtN4CYpdBBKLbqjDIx1OJ40sIut8UcMUR6l0KGDBkyZMiQIUOGDBkyZMiQIUOGDBkyZMiQIUOGjM8H9NQmYGqKnpp6+XJn5+UUhaiKqRQ8fr/f09fHVcvjTrzkgFdrdTgYGlGOrv7+rlT9PBD1Oz7HdxLozfcf7m5tVqS+O3C3+eXz5xsbH7bmU7X0HEzeXzNSFM2RvfOvf/3v1yO+8HS/h0n9pRPF+L1zc1zJqM8H1Obd6x+2Un8vN7W59f791o7hn0+f/eNv8a2Ny/8Te/rs4Ri5IXtv8tk//rt8zj9lUD16Bg3mvd5M7UHHfHnDxS/7lgG9df3bu1N4Z3Pr7of3m1PM2NMX30yupRZV3tl4tbF579eBZ/dO6tXY5K2vJv+t0I6k17Lv1v3z/ovv3FN+78xcqgKcf26o/NOub02xbOAYbAoUzW2UqZPBIPynZIl/Jlw0Ni9f/wAUTm19uPx+sxs5Fv79f3+/fy9LYKfz1fMp9tn2rcns19gefrX9bKxrRJtbq4Z9OjEY/XJnyuMtn8V1CP3lQ7OfQhvZQLBuGFBXF8TUCRGEKQUq3W4XwB1UnsLSlPTd69e3gMHL199PdeNaXIr/nBi8lyvDPO/s3EGYxqMLY19t/zqGfNr85e7Zp38f/K/OjR3G420o9zLUTMNvWoGCDgSHH4w+GK4D7gpvzQbcrkePFh+5gx/zmuPU5euXtzY/fHt3Ew76R7Thvw1O3M9/jl92dj4H5dvefpY6ntzensQVkYi1asZeTHyztfHq+UtszrO4vuPcb7LcOh0YHm298SCYsVpsyIXoFcsZPHYAQZdrcXHR5Q4UdR9T1y9f/nD52/fYIy4otOG+hxMT35EEUyzeGxj4FR/9uj1wD1dE0pJLBrDfTLwYewm+9CUuXYbLJ8ye9nKubPBB650bw2CWD0ZHhzEXABorFpAREHV30PzG6DDQn24WdIE6gkqOQ+NCb5W+DOA8IurnAsTYxOB9sujztC5iFn8d2L6Hq/nwVoxiXwwOsuhlJ6YR19wCXSzwzgQReNDa0zocQOxw66XRPENk3eP2R0LLl7N1rT2kZou9i+Aoe+2L7oIWhLkLiggeEZxtmPNu1CD+8kTQnZ2vXiL0bGD74eTAwCRC0/UCBZHgYXwD3T3vfPUlQl6uQk8h9yUANjja03MDUxC48cUlnmVIAo+u2l08XwQ3GyarKuuy211K1jV+tdcl2bKxMV8HZ8iAd6vHhTqeTkw85JGlwKA36G524BbgK8Q4tIr6fn6zeTo4AUR340bd3Vydo1Mx6MBozxetdZge9kZPj8BKLuwjIIJ0Hh4BD4XcZRewDx/j6r06zvcUTuB9ikOuzJZ2xDH23YsXg/d50uqdV50bna92QBVv3Rp4mKpDyF/s6N79wRcvvgO/CI3ADeCaoqegimCGX4ymVGQY1FH4S7rt9vE8darr6WkVbhbotfeCL6Dc41fti1LUERTxLt72aRXalCIODvJpIni4zk6wzXvbt25ts5xHFCp29HBicOIp4gJSJ7hciC0f7RXr7vT0jKYZADUcFpNXjtvt7txToIYPxJqxi3Y7p8NBaD8uWhpi6tvr325ye+F0+cv7L3j0kBPf2MC3tj0wsI1wdS7hYkdjg6ko/3ID4hFXokfsdoQRbAUlymiGFA6BjvHeXBZvSOAQACw+4nbcvXb7oohRV2xtpcfLTFqnhPNVCkvRDycnOW0Vq/2W6YvmKoDPz3+cOT/ouXTMGyjUqJRGrL3XnqVK0McNKc0oID/lT9lxe2+vW0S8VMC29ly6dERHXc+lO9LauYGDo4AWhD6kRbcAkJ9W+kf2DKE84GpP6BEy4C0cm1MlKfhgbU6JdaSqWShxK/41dJU34XItQpVYTIdQY6qYRTGou3PnEiBtzCx76VJPncTZgyOdQix955JQXM7Bo97excyuCIu1LcqTZSrUpDX70yiz5FazgI2J/6b0TuVxgQuKK3ChK3aFJEzhUTAe7sGHEmsBYVVM7dXhZj0Sm2FVzHi23t4jtSShtrDVfq9JWfH6CIYOcRmpwAQc54TgEe9InS5gjw0TPOIdyRl073FIws9hnF+ywBm1ArPl05yuu5OTE965If1Weu1XM2y0imSI2TgK0PAcrtrtVwua4zGZxGWy0F4QsTp9QZ1ngRU4EobbfTQ1IzjJeAJBt/vI8qELd0EkWsVrzmSjrKCReu7KxgWgAkDjH34AcEint9JaoY9uJagp7fmhWMhNGvNvoZmfRFNL3il9sW7yGgR4FUKWdD7QUlbVLGG5Xpwe1ID+41YQEGsJ9a8IcDbnJBU10Iq4DGYGBMaESCSonYAmEtROwpL5ZBA0kSavFk9olauJFrGFKImtdPy0EBhrFPiMqnzGmvhJNOQzZjjVvwSVUIaEgOIepMHJe6nAFIegdgKuumgHSOiKtOo5eaHzLJDX8BYj0UIMCiZ+EvWFRcuWgvJEUbuRjpp894rESSRrqhiJZJ0T0MRSAZlEsdXiySSKlbwz2EhnLQUNWGuIlsMLZ2GDmCLHzmQSxVCczyxY5wjFjYTqvhCCiEB0pvP5LTY668XiMBGmovwJ2ScKoDa/QYEpjkCeqM9/pEXniecZBMaENJHAmIAmElKcovNEIsQCCxnFPUiTjfcSgUSDQH0FQp5o5feJBBLVRZklX001MRLJrcQeJLk4m0CK89sOlE8vK6ghBqRPmeIIkJhf4lcQAjOwBFSc3hr1PHmiSPTlIdEm3Iqc4hgEh88nIWWeIwuFPQW6yHI9ZBJ1IrN2ZBIrRerBkElUFjQsOZdTYY1F5ZeqokJIYTqHiDOwQikOYQZWIMUhzMAWm+JUFuUZlEW1qiCWkxZAaeeJlYV+XQ7KolpV8LsOUp4oYM5V+f6ywDzRxt954ShyKqyoexAw8torYKCNHdUdmExjdbXRhJQCTq4ZxCgbiKkxRbCFzI8/YBvwvLAexPB31VRXd6hQRZH1ldVEPRBLccglxcRSHHLgEUhxlCog0aRScUVHYaMSDrYG/F3aQQyI1mFxQWlaVXEspodN8SlPkXki0Xmcep5YKvik84k24tmLSqJYpWgyiWKtyCRKqUp9ztFe3HxiURMUZBIvBlQ1NTXgkunGmppGcMkWOJQwm6EHMfD3FGxqwN+rpbUygJg+3QpCoQ5vL8QfKdZqNBqIULRZozFX4t8NNRoJ6tkIrdrTrYAOk7RW7dCqEUiEjRmioQVaaS4EiTJkyJAAWqmszLX449EdkxqRUCxSszkXUpALUWcQWdeYc18cO0pHPU9q/wNf8sTi60BYvKwbVaOj0cNecLX7093m+UYoQbOMzRZjjM5gxBhFyx1LTIcxELqZQPEVVs1GOmKe5ON14Gt51eUpa6yyLHck1ozOxOODVXrZ6Ma7n/qe8SvS1PF+GhSV97NK1musSjYtnnpPiV+wGET2zVHH79D+notRLaHbtTFUHYlG1t/gdxwjh08qfza80yf3EiYD8/3an9EPyBxf1/8lHnOsRIJvIy7k3Et6oh91A4Vj7xeE/pR5/3U38w6nIxZyGvdzjSpy9KbmcvU77ll3ME14+0dQkWA805RJxj9OEULwOY4V9EMkgdrXUciaROZQ0lSzy2IlRZGaXyyNBiCxvSYSbT8MYBKjlljchTgS65BtL8F8chLLAo6yunh1FK1WB3cDGmWoI8FUHyZvBxA80o4oihiTTHUHu9ZxM7Bn5M4w71gGZMzscuQJULdmXkKPD17H1jpijNmYjP+sCsaNMUZjE/xrC4E7emJrs6ygaqbNmVg1LtGHxnWmzeZaBRKV7+DJ7nYseZK4rsIhi9YOHicO9bYOIJGJxv+wrATBeIL51KVQ9qzJiNUdUb9OfG9IhJqGPUumJ6vutRiQ6Im9Trxp30eNu0HPiq37jemJ5ff6FTDeNVtTYjcZOVj2xJQW9k3idSAUDKmvmA4Nv0PfM9E9aLMLlljojwN5oFC63M7xK3M0Ir3vTh/JF/oy/OkgkjCa4+7HNY/d+tVoyHzgXzfU7gYcmMRI8vf6dv0KWltmmaVqFNHvG2oNSTDYGGKW3rpU7mVlLN5iuVK7e7AXjOieGBpNYIWgETUrYHVRdBHf0SAj4g6Bbby2/qhvazm4zT5OODUrnv3DZKjNttQdWW8LPDYvre0fukPVPwXetiyt4dIvb9fbkvH1wwSQCKoMZ36M/GEvumu+WQkkoh+tiTfmfV0UFTVbXpLAoZSiKCX3IyxXxgrv0hTFBV78srAy9eNvepezK+64AkyHyhhYBfxTUtwZOiNY+lNeMmTIkHE2+H+7AzqpnWIUzAAAAABJRU5ErkJggg==\">\n",
    "    \n",
    "<img src=\"https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png\" style=\"position:absolute;left:40%;height:300px;border-left:3px solid #E67300\">\n",
    "    \n",
    "</div>  \n",
    "\n",
    "</div>\n",
    "<div style=\"height:25px;\">\n",
    "<div style=\"width:260px;position:absolute;left:40px;border:4px solid #E67300;padding:0.1%;text-align:center\"> FIG2</div>\n",
    "<div style=\"width:260px;position:absolute;left:700px;border:4px solid #E67300;padding:0.1%;text-align:center;border-reduce:30px\"> FIG3</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:240px\">\n",
    "\n",
    "<div style=\"height:210px;border:3px solid #E67300; padding:1%;width:20%;position:absolute\">\n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "   \n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;border:3px solid black;background-color:black\"></div>      \n",
    "<div style=\"height:40px;width:90px;position:absolute;left:55px;top:20px;\">------------></div>\n",
    "<div style=\"height:40px;width:44px;position:absolute;left:12px;border:3px solid black;background-color:black\"></div>     \n",
    "</div> \n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:65px;border:3px solid black;background-color:black\"></div> \n",
    "<div style=\"height:50px;width:40px;position:absolute;left:35px;top:40px;border-left:3px solid black;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:35px;top:78px;\">----------------></div> \n",
    "<div style=\"height:50px;width:40px;position:absolute;left:20px;top:40px;border-left:3px solid black;\"></div>     \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:120px;border:3px solid black;background-color:black\"></div>\n",
    "<div style=\"height:42px;width:40px;position:absolute;left:20px;top:90px;border-left:3px solid black;\"></div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:20px;top:120px;\">------------------></div>     \n",
    "</div>\n",
    "</div>  \n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:30px;width:200px;position:absolute;left:20px;top:190px;border:3px solid #E67300;text-align:center\"> Encoder-decoder Attention</div>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "    \n",
    "<div style=\"height:210px;border:3px solid #E67300; padding:1%;width:20%;position:absolute;left:23%;top:0px\">\n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;border:3px solid green;background-color:green\"></div>     \n",
    "<div style=\"height:40px;width:96px;position:absolute;left:110px;top:30px;color:black\" ><-----</div>\n",
    "<div style=\"height:40px;width:96px;position:absolute;left:90px;top:30px;color:green\"><-----</div>\n",
    "<div style=\"height:40px;width:96px;position:absolute;left:55px;top:10px;color:#4D0033\">-------------></div>  \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:190px;top:30px;color:blue\"><-------</div>\n",
    "</div> \n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:65px;border:3px solid #1E00B3;background-color:#1E00B3\"></div> \n",
    "\n",
    "<div style=\"height:30px;width:40px;position:absolute;left:110px;top:40px;border-left:3px solid green;\"></div> \n",
    "<div style=\"height:87px;width:40px;position:absolute;left:90px;top:40px;border-left:3px solid green;\"></div> \n",
    "\n",
    "<div style=\"height:40px;width:96px;position:absolute;left:110px;top:58px;color:green;\">-----></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:40px;top:78px;color:#4D0033\">---------------></div> \n",
    "<div style=\"height:70px;width:40px;position:absolute;left:60px;top:20px;border-left:3px solid #4D0033;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:190px;top:68px;color:blue\">-------></div>\n",
    "<div style=\"height:102px;width:40px;position:absolute;left:203px;top:40px;border-right:3px solid blue;\"></div> \n",
    "  \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:120px;border:3px solid #4D0033;background-color:#4D0033\"></div>\n",
    "<div style=\"height:62px;width:40px;position:absolute;left:60px;top:90px;border-left:3px solid #4D0033;\"></div> \n",
    "<div style=\"height:62px;width:40px;position:absolute;left:40px;top:90px;border-left:3px solid #4D0033;\"></div> \n",
    "<div style=\"height:40px;width:96px;position:absolute;left:90px;top:110px;color:#4D0033;text-size:6px;color:green\"> --------></div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:60px;top:140px;color:#4D0033;/\"><------------------ </div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:40px;top:140px;color:#4D0033\"> <----------- </div>  \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:190px;top:130px;color:blue\"><-------</div>\n",
    "</div>\n",
    "</div>  \n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:30px;width:180px;position:absolute;left:24%;top:190px;border:3px solid #E67300;text-align:center\"> Encoder Self-attention</div>\n",
    "</div> \n",
    "    \n",
    "<div style=\"height:210px;border:3px solid #E67300; padding:1%;width:20%;position:absolute;left:46%;top:0px\">\n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "   \n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;border:3px solid #009999;background-color:#009999\"></div>      \n",
    "\n",
    "<div style=\"height:40px;width:98px;position:absolute;left:55px;top:15px;color:#009999\"><-------------</div> \n",
    "<div style=\"height:40px;width:94px;position:absolute;left:87px;top:35px;color:#009999\"><--------</div> \n",
    "</div> \n",
    "    \n",
    "<div style=\"height:42px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:66px;border:3px solid #009999;background-color:#009999\"></div> \n",
    "<div style=\"height:38px;width:40px;position:absolute;left:87px;top:45px;border-left:3px solid #009999;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:87px;top:68px;color:#009999\">--------></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:87px;top:88px;color:#009999\"><--------</div> \n",
    "<div style=\"height:70px;width:40px;position:absolute;left:55px;top:26px;border-left:3px solid #009999;\"></div>     \n",
    "</div>\n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:40px;width:40px;position:absolute;left:150px;top:120px;border:3px solid #009999;background-color:#009999\"></div>\n",
    "<div style=\"height:65px;width:40px;position:absolute;left:55px;top:90px;border-left:3px solid #009999;\"></div> \n",
    "<div style=\"height:35px;width:40px;position:absolute;left:87px;top:100px;border-left:3px solid #009999;\"></div> \n",
    "<div style=\"height:40px;width:194px;position:absolute;left:87px;top:120px;color:#009999\">--------></div> \n",
    "<div style=\"height:40px;width:130px;position:absolute;left:55px;top:140px;color:#009999\">-------------></div>     \n",
    "</div>\n",
    "</div>  \n",
    "    \n",
    "<div style=\"height:40px\">\n",
    "<div style=\"height:30px;width:190px;position:absolute;left:48%;top:190px;border:3px solid #E67300;text-align:center\">MaskDecoder self-attention</div>\n",
    "</div>     \n",
    "    \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scaled Dot-Product Attention**:\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\" **[FIG2]**.<br>\n",
    "The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.\n",
    "Computing the dot products of the query with all keys,ivide each by values. $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values\n",
    "\n",
    "Computing the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . compute the matrix of outputs as:\n",
    "\n",
    "## $Attention(Q,K,V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V = \\displaystyle\\sum_i \\frac{e^{q.k_i}}{\\sum_j e^{q.k_j}V_i}$\n",
    "\n",
    "Q is a matrix of all of words, and K is words embedding that was generted before. V is same K. In attention model, we would like to find most similarity key (old memmpty words) with attention input. This fucnation gives you a mask, gives a probability distribution over keys which is peaked at ones that are similar to the query and this mask gives to matrix multiplied with values which is the same as summing over values multiplied by this mask and there you are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-head Attention:**\n",
    "\n",
    "It's same Dot-Product Attention with this note that Multi-head Attention attented to order of words ,too. In language analysis is important that we knwo where is the words (Query) and similarity of other word (K) in the sentense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
