{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid black;background-color:#E6BF00;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Session 002</b></h1>\n",
    "    <h4> Basic Natural language processing </h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #E6BF00;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "    \n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    \n",
    "paper: https://arxiv.org/abs/1405.4053\n",
    "    \n",
    "tensoflow, word2vec: https://www.tensorflow.org/tutorials/representation/word2vec\n",
    "    \n",
    "Word2Vec theory (by ship_grams aaproch):  http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    \n",
    "gensim code: https://rare-technologies.com/doc2vec-tutorial/\n",
    "    \n",
    "<hr>\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "  \n",
    "https://towardsdatascience.com/using-scikit-learn-to-find-bullies-c47a1045d92f\n",
    "    \n",
    "https://www.kaggle.com/eswarbabu88/toxic-comment-glove-logistic-regression\n",
    "    \n",
    "https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "    \n",
    "https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector\n",
    "    \n",
    "https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python\n",
    "    \n",
    "https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/\n",
    "    \n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "    \n",
    "https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html\n",
    "    \n",
    "https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n",
    "    \n",
    "Named Entity Recognition: https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> introduction to Doc2Vec <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec representation is created using 2 algorithms: \n",
    "1. Continuous Bag-of-Words model (CBOW),\n",
    "2. the Skip-Gram model.\n",
    "\n",
    "**Continuous bag of words:**\n",
    "\n",
    "**Skip_grams:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2vec:**\n",
    "\n",
    "the goal of doc2vec is to create a numeric representation of a document, regardless of its length.\n",
    "\n",
    "The concept that Mikilov and Le have used was simple, yet clever: they have used the word2vec model, and added another vector (Paragraph ID below), like so:\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*x-gtU4UlO8FAsRvL'>\n",
    "\n",
    "when training the word vectors W, the document vector D is trained as well, and in the end of training, it holds a numeric representation of the document.\n",
    "\n",
    "The model above is called Distributed Memory version of Paragraph Vector (PV-DM).\n",
    "\n",
    "Dic2word has two models:\n",
    "1.  Distributed Memory version of Paragraph Vector (PV-DM).\n",
    "2. Words version of Paragraph Vecto  (PV-DBOW),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(PV-DM):** it is like Continuous bag of words, \n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*x-gtU4UlO8FAsRvL.'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(PV-DBOW):** it is like skip_grams, \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*NtIsrbd4VQzUKVKr.\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fast:**\n",
    "\n",
    "in word2vec, another algorithm, which is similar to skip-gram may be used Distributed Bag of Words version of Paragraph Vector (PV-DBOW), this algorithm is actually faster (as opposed to word2vec) and consumes less memory, since there is no need to save the word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc2vec models may be used in the following way: \n",
    "1. for training, a set of documents is required, A word vector W is generated for each word, and a document vector D is generated for each document.\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyCode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "from gensim.models.doc2vec import TaggedLineDocument\n",
    "import pprint\n",
    "from farhad.Farhadcolor import tcolors,bcolors\n",
    "from farhad_DL.utils import Estimate_Deeptime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def light_Cleaner(documents):\n",
    "    # remove common words and tokenize\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    docs = [ [token for token in text if frequency[token] > 1] for text in texts]\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            texts.append(word)\n",
    "    \n",
    "    return texts, docs\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        for uid, line in enumerate(self.filename):\n",
    "            yield LabeledSentence(words=line, labels=['SENT_%s' % uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(filename):\n",
    "    for uid, line in enumerate(filename):\n",
    "        yield LabeledSentence(words=line, labels=['SENT_%s' % uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=b(croups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interface', 'computer', 'survey', 'user', 'computer', 'system', 'response', 'time', 'eps', 'user', 'interface', 'system', 'system', 'human', 'system', 'eps', 'user', 'response', 'time', 'trees', 'graph', 'trees', 'graph', 'minors', 'trees', 'graph', 'minors', 'survey']\n",
      "\u001b[41m            \t                 \t                 \t                 \t       \u001b[0m\n",
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "dictionary, croups = light_Cleaner(documents)\n",
    "print(dictionary)     \n",
    "print(bcolors.RED, 4*'           \\t      ',tcolors.ENDC)\n",
    "pprint.pprint(croups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-13 22:14:59,010 [WARNING]  consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-03-13 22:14:59,015 [INFO]  collecting all words and their counts\n",
      "2019-03-13 22:14:59,016 [INFO]  PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-03-13 22:14:59,018 [INFO]  collected 12 word types and 9 unique tags from a corpus of 9 examples and 29 words\n",
      "2019-03-13 22:14:59,022 [INFO]  Loading a fresh vocabulary\n",
      "2019-03-13 22:14:59,024 [INFO]  effective_min_count=1 retains 12 unique words (100% of original 12, drops 0)\n",
      "2019-03-13 22:14:59,026 [INFO]  effective_min_count=1 leaves 29 word corpus (100% of original 29, drops 0)\n",
      "2019-03-13 22:14:59,028 [INFO]  deleting the raw counts dictionary of 12 items\n",
      "2019-03-13 22:14:59,031 [INFO]  sample=0.001 downsamples 12 most-common words\n",
      "2019-03-13 22:14:59,033 [INFO]  downsampling leaves estimated 3 word corpus (12.1% of prior 29)\n",
      "2019-03-13 22:14:59,034 [INFO]  estimated required memory for 12 words and 5 dimensions: 6660 bytes\n",
      "2019-03-13 22:14:59,035 [INFO]  resetting layer weights\n",
      "2019-03-13 22:14:59,042 [INFO]  training model with 4 workers on 12 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-03-13 22:14:59,051 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:14:59,052 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:14:59,053 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:14:59,054 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:14:59,055 [INFO]  EPOCH - 1 : training on 29 raw words (13 effective words) took 0.0s, 2548 effective words/s\n",
      "2019-03-13 22:14:59,059 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:14:59,060 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:14:59,061 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:14:59,062 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:14:59,063 [INFO]  EPOCH - 2 : training on 29 raw words (12 effective words) took 0.0s, 2862 effective words/s\n",
      "2019-03-13 22:14:59,069 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:14:59,073 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:14:59,075 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:14:59,076 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:14:59,079 [INFO]  EPOCH - 3 : training on 29 raw words (11 effective words) took 0.0s, 1176 effective words/s\n",
      "2019-03-13 22:14:59,084 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:14:59,086 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:14:59,087 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:14:59,088 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:14:59,089 [INFO]  EPOCH - 4 : training on 29 raw words (9 effective words) took 0.0s, 1792 effective words/s\n",
      "2019-03-13 22:14:59,100 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:14:59,101 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:14:59,102 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:14:59,104 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:14:59,105 [INFO]  EPOCH - 5 : training on 29 raw words (15 effective words) took 0.0s, 3018 effective words/s\n",
      "2019-03-13 22:14:59,106 [INFO]  training on a 145 raw words (60 effective words) took 0.1s, 965 effective words/s\n",
      "2019-03-13 22:14:59,107 [WARNING]  under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "croups_tag = [TaggedDocument(doc, [i]) for i, doc in enumerate(croups)]\n",
    "model = Doc2Vec(croups_tag,vector_size=5, window=2, min_count=1, workers=4, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.b(self)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-13 22:15:45,026 [INFO]  training model with 4 workers on 12 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-03-13 22:15:45,035 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,036 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,038 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,039 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,040 [INFO]  EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,042 [WARNING]  EPOCH - 1 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,047 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,048 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,048 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,049 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,050 [INFO]  EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,051 [WARNING]  EPOCH - 2 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,063 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,064 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,065 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,066 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,067 [INFO]  EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,068 [WARNING]  EPOCH - 3 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,071 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,074 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,077 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,078 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,079 [INFO]  EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,081 [WARNING]  EPOCH - 4 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,085 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,086 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,087 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,089 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,090 [INFO]  EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,091 [WARNING]  EPOCH - 5 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,097 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,100 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,101 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,102 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,104 [INFO]  EPOCH - 6 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,105 [WARNING]  EPOCH - 6 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,113 [INFO]  worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-13 22:15:45,115 [INFO]  worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-13 22:15:45,116 [INFO]  worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-13 22:15:45,117 [INFO]  worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-13 22:15:45,119 [INFO]  EPOCH - 7 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-13 22:15:45,120 [WARNING]  EPOCH - 7 : supplied example count (0) did not equal expected count (9)\n",
      "2019-03-13 22:15:45,121 [INFO]  training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2019-03-13 22:15:45,122 [WARNING]  under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "epochs_size = 7\n",
    "#estimetor = Estimate_Deeptime(epochs_size)\n",
    "for epoch in range(1):\n",
    "    model.train( d , total_examples= model.corpus_count,epochs=epochs_size)\n",
    "    #model.aplpha -= 0.002\n",
    "    model.min_alpha = model.alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model:\n",
    "from gensim.test.utils import get_tmpfile\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  LabeledLineSentence(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Doc2Vec(alpha=0.025, min_alpha=0.025)\n",
    "model2.build_vocab(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
