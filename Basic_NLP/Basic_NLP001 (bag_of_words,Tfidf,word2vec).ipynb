{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid black;background-color:#E6BF00;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Session 001</b></h1>\n",
    "    <h4> Basic Natural language processing </h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #E6BF00;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors\n",
    "    \n",
    "https://radimrehurek.com/gensim/tut2.html\n",
    "    \n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Corpora_and_Vector_Spaces.ipynb\n",
    "    \n",
    "<hr>\n",
    "    \n",
    "https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    \n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "  \n",
    "https://towardsdatascience.com/using-scikit-learn-to-find-bullies-c47a1045d92f\n",
    "    \n",
    "https://www.kaggle.com/eswarbabu88/toxic-comment-glove-logistic-regression\n",
    "    \n",
    "https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "    \n",
    "https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector\n",
    "    \n",
    "https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python\n",
    "    \n",
    "https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/\n",
    "    \n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "    \n",
    "https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html\n",
    "    \n",
    "https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> Bag of Words <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'I am Farhad Shadmand',\n",
    "    'I am from Iran'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_count_vector = CountVectorizer(ngram_range=(1,2))\n",
    "data_vectorized_vc = vectorizer_count_vector.fit_transform(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vectorized_vc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'am farhad',\n",
       " 'am from',\n",
       " 'farhad',\n",
       " 'farhad shadmand',\n",
       " 'from',\n",
       " 'from iran',\n",
       " 'iran',\n",
       " 'shadmand']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:110px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1>  term-frequency times inverse document-frequency  </h1>\n",
    "    <h2> (TF_IDF) </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Used sklearn TfidfVectorizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tiidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "out = tiidf.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33517574, 0.47107781, 0.        , 0.47107781, 0.47107781,\n",
       "        0.        , 0.        , 0.        , 0.47107781],\n",
       "       [0.33517574, 0.        , 0.47107781, 0.        , 0.        ,\n",
       "        0.47107781, 0.47107781, 0.47107781, 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Used sklearn TfidfTransformer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_count_vector = CountVectorizer(ngram_range=(1,2))\n",
    "data_vectorized_vc = vectorizer_count_vector.fit_transform(texts)\n",
    "\n",
    "Tfidf = TfidfTransformer()\n",
    "data_vectorized = Tfidf.fit_transform(data_vectorized_vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33517574 0.47107781 0.         0.47107781 0.47107781 0.\n",
      "  0.         0.         0.47107781]\n",
      " [0.33517574 0.         0.47107781 0.         0.         0.47107781\n",
      "  0.47107781 0.47107781 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> Used gensim <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "(1): https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Corpora_and_Vector_Spaces.ipynb\n",
    "\n",
    "\n",
    "(2): https://radimrehurek.com/gensim/tut2.html\n",
    "    \n",
    "(3): https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html\n",
    "    \n",
    "(3) part one: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "    \n",
    "(3) part two: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec = ''\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in smart_open('datasets/mycorpus.txt', 'rb'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Ti_TDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (4, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "vec = [(0, 1), (4, 1)]\n",
    "print(tfidf[vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA and LSI:** https://radimrehurek.com/gensim/tut2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/wiki.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Theory:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Skip grams approch:**\n",
    "**Basic Level:**\n",
    "\n",
    "Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\n",
    "\n",
    "We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word.<br>\n",
    "(\"nearby\" is mean actually a \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).)\n",
    "\n",
    "The below example shows some of the training samples: \n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/training_data.png\" style=\"border: 4px solid black\">\n",
    "\n",
    "for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Medium level:**\n",
    "\n",
    "First, we should convert string into numerical sequesnes. So, we build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words. \n",
    "We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.\n",
    "\n",
    "The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n",
    "\n",
    "Here’s the architecture of our neural network:\n",
    "\n",
    "There is no activation function on the hidden layer neurons, but the output neurons use softmax.\n",
    "\n",
    "When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word.\n",
    "But when you evaluate the trained network on an input word, the output vector will actually be a probability distributio.\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" style=\"border: 4px solid black;width:700px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer:**\n",
    "\n",
    "we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" style=\"border: 4px solid black;width:320px\">\n",
    "\n",
    "\n",
    "example of something happend in hidden layer (for 5 row and 3 hidden layer or features):\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" style=\"border: 4px solid black\">\n",
    "\n",
    "Note that neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let's say that in our training corpus, every single occurrence of the word 'York' is preceded by the word 'New'. That is, at least according to the training data, there is a 100% probability that 'New' will be in the vicinity of 'York'. However, if we take the 10 words in the vicinity of 'York' and randomly pick one of them, the probability of it being 'New' is not 100%; you may have picked one of the other words in the vicinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **pyCode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "def light_cleaner(documents):\n",
    "    \"\"\" \n",
    "    input:\n",
    "         documents: a list of text\n",
    "    --------------------------------------------------------\n",
    "    output:\n",
    "         texts : a list of tokenize text\n",
    "         docs: just a list as all unic words that use in data\n",
    "    ---------------------------------------------------------\n",
    "    info:\n",
    "        Get a list of text (string),first split statment into words, \n",
    "        Then remove the stop words. \n",
    "    \"\"\"\n",
    "    \n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    docs = [ [token for token in text if frequency[token] > 1] for text in texts]\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            texts.append(word)\n",
    "    \n",
    "    return texts, docs\n",
    " \n",
    "    \n",
    "texts, croups = light_cleaner(documents)    \n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = W2VTransformer(size=3, min_count=1, seed=1)\n",
    "wordvecs = model.fit(texts).transform(['graph', 'system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
       "       [-0.10132009,  0.04960125,  0.1154057 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlist = {}\n",
    "for label,text in enumerate(texts):\n",
    "    seqlist[str(label)] = TaggedDocument(words=[model.fit(texts).transform(text)],tags=['Sent_{}'.format(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': TaggedDocument(words=[array([[-0.10260514, -0.03398572,  0.03800404],\n",
      "       [-0.15766233, -0.16358677, -0.01114298],\n",
      "       [ 0.15043013,  0.13922882,  0.05463114]], dtype=float32)], tags=['Sent_0']),\n",
      " '1': TaggedDocument(words=[array([[-0.07389324,  0.02091417, -0.02648208],\n",
      "       [-0.01611834,  0.1363751 ,  0.14928658],\n",
      "       [ 0.15043013,  0.13922882,  0.05463114],\n",
      "       [-0.10132009,  0.04960125,  0.1154057 ],\n",
      "       [ 0.15449473,  0.09626675,  0.0608709 ],\n",
      "       [-0.14735284, -0.10749265, -0.03824941]], dtype=float32)], tags=['Sent_1']),\n",
      " '2': TaggedDocument(words=[array([[ 0.00326326, -0.08766717,  0.03419405],\n",
      "       [-0.01611834,  0.1363751 ,  0.14928658],\n",
      "       [-0.15766233, -0.16358677, -0.01114298],\n",
      "       [-0.10132009,  0.04960125,  0.1154057 ]], dtype=float32)], tags=['Sent_2']),\n",
      " '3': TaggedDocument(words=[array([[-0.10132009,  0.04960125,  0.1154057 ],\n",
      "       [-0.10260514, -0.03398572,  0.03800404],\n",
      "       [-0.10132009,  0.04960125,  0.1154057 ],\n",
      "       [ 0.00326326, -0.08766717,  0.03419405]], dtype=float32)], tags=['Sent_3']),\n",
      " '4': TaggedDocument(words=[array([[-0.01611834,  0.1363751 ,  0.14928658],\n",
      "       [ 0.15449473,  0.09626675,  0.0608709 ],\n",
      "       [-0.14735284, -0.10749265, -0.03824941]], dtype=float32)], tags=['Sent_4']),\n",
      " '5': TaggedDocument(words=[array([[-0.08721146, -0.12780839,  0.1613874 ]], dtype=float32)], tags=['Sent_5']),\n",
      " '6': TaggedDocument(words=[array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
      "       [-0.08721146, -0.12780839,  0.1613874 ]], dtype=float32)], tags=['Sent_6']),\n",
      " '7': TaggedDocument(words=[array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
      "       [ 0.11606643, -0.1640675 , -0.00758726],\n",
      "       [-0.08721146, -0.12780839,  0.1613874 ]], dtype=float32)], tags=['Sent_7']),\n",
      " '8': TaggedDocument(words=[array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
      "       [ 0.11606643, -0.1640675 , -0.00758726],\n",
      "       [-0.07389324,  0.02091417, -0.02648208]], dtype=float32)], tags=['Sent_8'])}\n"
     ]
    }
   ],
   "source": [
    "pprint(seqlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
