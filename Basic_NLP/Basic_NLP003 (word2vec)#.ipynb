{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid black;background-color:#E6BF00;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Session 001-2</b></h1>\n",
    "    <h4> Basic Natural language processing </h4>\n",
    "</header>\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #E6BF00;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors\n",
    "    \n",
    "https://radimrehurek.com/gensim/tut2.html\n",
    "    \n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Corpora_and_Vector_Spaces.ipynb\n",
    "    \n",
    "<hr>\n",
    "    \n",
    "https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    \n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "  \n",
    "https://towardsdatascience.com/using-scikit-learn-to-find-bullies-c47a1045d92f\n",
    "    \n",
    "https://www.kaggle.com/eswarbabu88/toxic-comment-glove-logistic-regression\n",
    "    \n",
    "https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "    \n",
    "https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector\n",
    "    \n",
    "https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python\n",
    "    \n",
    "https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/\n",
    "    \n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "    \n",
    "https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html\n",
    "    \n",
    "https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> [3] Word2Vec <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "(1): https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Corpora_and_Vector_Spaces.ipynb\n",
    "\n",
    "\n",
    "(2): https://radimrehurek.com/gensim/tut2.html\n",
    "    \n",
    "(3): https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html\n",
    "    \n",
    "(3) part one: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "    \n",
    "(3) part two: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> Theory: <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mikolov et al. (2013c) introduced a new evalua- tion scheme based on word analogies that probes the finer structure of the word vector space by ex- amining not the scalar distance between word vec- tors, but rather their various dimensions of dif- ference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of mean- ing, thereby capturing the multi-clustering idea of distributed representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec has two model:\n",
    "\n",
    "1. Skip-gram,\n",
    "2. Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Skip grams approch: (PV-DBWM) </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Basic Level:**\n",
    "\n",
    "Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\n",
    "\n",
    "We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word.<br>\n",
    "(\"nearby\" is mean actually a \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).)\n",
    "\n",
    "The below example shows some of the training samples: \n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/training_data.png\" style=\"border: 4px solid black\">\n",
    "\n",
    "for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Medium level:**\n",
    "\n",
    "First, we should convert string into numerical sequesnes. So, we build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words. \n",
    "We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.\n",
    "\n",
    "The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n",
    "\n",
    "Here’s the architecture of our neural network:\n",
    "\n",
    "There is no activation function on the hidden layer neurons, but the output neurons use softmax.\n",
    "\n",
    "When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word.\n",
    "But when you evaluate the trained network on an input word, the output vector will actually be a probability distributio.\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" style=\"border: 4px solid black;width:700px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer:**\n",
    "\n",
    "we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" style=\"border: 4px solid black;width:320px\">\n",
    "\n",
    "\n",
    "example of something happend in hidden layer (for 5 row and 3 hidden layer or features):\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" style=\"border: 4px solid black\">\n",
    "\n",
    "Note that neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let's say that in our training corpus, every single occurrence of the word 'York' is preceded by the word 'New'. That is, at least according to the training data, there is a 100% probability that 'New' will be in the vicinity of 'York'. However, if we take the 10 words in the vicinity of 'York' and randomly pick one of them, the probability of it being 'New' is not 100%; you may have picked one of the other words in the vicinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Continuous bag of words (CBOW) PV-DM </h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "https://www.youtube.com/watch?v=ycXWAtm22-w\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/800/1*UVe8b6CWYykcxbBOR6uCfg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\frac{1}{T} \\sum_{i=1}^{T} (log P (w_t|c_t)) $\n",
    "\n",
    "\n",
    "# $ P(w_t|c_t) = \\frac{exp(e^{'}(w_t)^T x)}{\\sum_{t=1}^{|v|}exp(e^{'}(w_t)^T x)}$ , $ x = \\sum_{i \\in c}e(w_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ct$ is other words ( $w_{t-1}, ...$ and $w_{t+1}, ...$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we jsut consider a Nerual netwrok with one hidden layer with softmax activation layer,  we can conut word2vec here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png' style='width:400px;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> python code: <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> with gensim pakage </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "def light_cleaner(documents):\n",
    "    \"\"\" \n",
    "    input:\n",
    "         documents: a list of text\n",
    "    --------------------------------------------------------\n",
    "    output:\n",
    "         texts : a list of tokenize text\n",
    "         docs: just a list as all unic words that use in data\n",
    "    ---------------------------------------------------------\n",
    "    info:\n",
    "        Get a list of text (string),first split statment into words, \n",
    "        Then remove the stop words. \n",
    "    \"\"\"\n",
    "    \n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    docs = [ [token for token in text if frequency[token] > 1] for text in texts]\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            texts.append(word)\n",
    "    \n",
    "    return texts, docs\n",
    " \n",
    "    \n",
    "texts, croups = light_cleaner(documents)    \n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = W2VTransformer(size=3, min_count=1, seed=1)\n",
    "wordvecs = model.fit(texts).transform(['graph', 'system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
       "       [-0.10132009,  0.04960125,  0.1154057 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlist = {}\n",
    "for label,text in enumerate(texts):\n",
    "    seqlist[str(label)] = TaggedDocument(words=[model.fit(texts).transform(text)],tags=['Sent_{}'.format(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': TaggedDocument(words=[array([[-0.10260514, -0.03398572,  0.03800404],\n",
      "       [-0.15766233, -0.16358677, -0.01114298],\n",
      "       [ 0.15043013,  0.13922882,  0.05463114]], dtype=float32)], tags=['Sent_0']),\n",
      " '1': TaggedDocument(words=[array([[-0.07389324,  0.02091417, -0.02648208],\n",
      "       [-0.01611834,  0.1363751 ,  0.14928658],\n",
      "       [ 0.15043013,  0.13922882,  0.05463114],\n",
      "       [-0.10132009,  0.04960125,  0.1154057 ],\n",
      "       [ 0.15449473,  0.09626675,  0.0608709 ],\n",
      "       [-0.14735284, -0.10749265, -0.03824941]], dtype=float32)], tags=['Sent_1']),\n",
      " '2': TaggedDocument(words=[array([[ 0.00326326, -0.08766717,  0.03419405],\n",
      "       [-0.01611834,  0.1363751 ,  0.14928658],\n",
      "       [-0.15766233, -0.16358677, -0.01114298],\n",
      "       [-0.10132009,  0.04960125,  0.1154057 ]], dtype=float32)], tags=['Sent_2']),\n",
      " '3': TaggedDocument(words=[array([[-0.10132009,  0.04960125,  0.1154057 ],\n",
      "       [-0.10260514, -0.03398572,  0.03800404],\n",
      "       [-0.10132009,  0.04960125,  0.1154057 ],\n",
      "       [ 0.00326326, -0.08766717,  0.03419405]], dtype=float32)], tags=['Sent_3']),\n",
      " '4': TaggedDocument(words=[array([[-0.01611834,  0.1363751 ,  0.14928658],\n",
      "       [ 0.15449473,  0.09626675,  0.0608709 ],\n",
      "       [-0.14735284, -0.10749265, -0.03824941]], dtype=float32)], tags=['Sent_4']),\n",
      " '5': TaggedDocument(words=[array([[-0.08721146, -0.12780839,  0.1613874 ]], dtype=float32)], tags=['Sent_5']),\n",
      " '6': TaggedDocument(words=[array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
      "       [-0.08721146, -0.12780839,  0.1613874 ]], dtype=float32)], tags=['Sent_6']),\n",
      " '7': TaggedDocument(words=[array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
      "       [ 0.11606643, -0.1640675 , -0.00758726],\n",
      "       [-0.08721146, -0.12780839,  0.1613874 ]], dtype=float32)], tags=['Sent_7']),\n",
      " '8': TaggedDocument(words=[array([[-0.09071979, -0.0590962 ,  0.01287825],\n",
      "       [ 0.11606643, -0.1640675 , -0.00758726],\n",
      "       [-0.07389324,  0.02091417, -0.02648208]], dtype=float32)], tags=['Sent_8'])}\n"
     ]
    }
   ],
   "source": [
    "pprint(seqlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> with sklearn </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> other approch (isn't really famous) <h1>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-throughs:\n",
    "\n",
    "ship-gram is base on words, but Skip-throughs is base on sentences. Skip-throughs uses RNN sequence (Encoder-Decooder) to model sentences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
