{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid black;background-color:#E6BF00;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Session 004</b></h1>\n",
    "    <h4> Basic Natural language processing </h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #E6BF00;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "orginal page for corpus: https://nlp.stanford.edu/projects/glove/\n",
    "    \n",
    "paper (theory): http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "<hr>\n",
    "    \n",
    "Code: http://www.foldl.me/2014/glove-python/  \n",
    "    \n",
    "http://dsnotes.com/post/fast-parallel-async-adagrad/\n",
    "    \n",
    "<hr>    \n",
    "\n",
    "PyCon APAC 2018: https://www.youtube.com/watch?v=Y8gKX5zMRyQ\n",
    "\n",
    "Word2Vec and GloVe with gensim (code,ready-package): https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python\n",
    "    \n",
    "Stanford University (Theory,ready-package): https://www.youtube.com/watch?v=ASn7ExxLZws\n",
    "    \n",
    "Glove Vector, find similarty (code,ready-package): https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector\n",
    "    \n",
    "glove (code,ready-package): https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "\n",
    "glove (code),ready-package: https://www.kaggle.com/eswarbabu88/toxic-comment-glove-logistic-regression\n",
    "    \n",
    "\n",
    "RNN W2L08: https://www.youtube.com/watch?v=yjm19W5QDtA\n",
    "    \n",
    "What is GloVe? Part I: https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970\n",
    "    \n",
    "<hr> \n",
    "    \n",
    "word2vec: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "\n",
    "Nearest Neighbour Graph: https://markhneedham.com/blog/2018/05/19/interpreting-word2vec-glove-embeddings-sklearn-neo4j-graph-algorithms/\n",
    "    \n",
    "Word2Vec: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "    \n",
    "Word2Vec (orginal gensim page): https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html\n",
    "    \n",
    "Basic NLP: Bag of Words, TF-IDF, Word2Vec, LSTM: https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n",
    "\n",
    "Topic Model: https://medium.com/@tomar.ankur287/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045\n",
    "    \n",
    "Deep Learning for NLP at Oxford with Deep: https://www.youtube.com/watch?v=RP3tZFcC2e8\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> Global vectors for word representation <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> Theory behind GloVe <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "(1) paper01: https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "<hr>\n",
    "    \n",
    "**(Pennington, Jeffrey, Richard Socher, and Christopher Manning. \"Glove: Global vectors for word representation.\" Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.)**\n",
    "<hr>\n",
    "    \n",
    "(2) source code: http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "(3) https://www.youtube.com/watch?v=ycXWAtm22-w\n",
    "    \n",
    "(4) http://building-babylon.net/2015/07/29/glove-global-vectors-for-word-representations/\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:70%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> Abstract: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is The result of a new global log-bilinear regression model that combines the advantages of the two major model families in the literature:\n",
    "\n",
    "1. **global matrix factorization methods,**\n",
    "2. **local context window methods,**\n",
    "\n",
    "The model produces a vector space with meaningful substructure.<br>\n",
    "The model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so.<br>\n",
    "The model propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics.The model produces a word vector space with meaningful substructure. **[REF01]**\n",
    "\n",
    "### **1. Matrix Factorization Methods:** \n",
    "**A). LSA (semantic analysis):** <br>\n",
    "Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as latent semantic analysis **(LSA)**. <br>\n",
    "In LSA, we count the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus.\n",
    "\n",
    "**B). the Hyperspace Analogue to Language (HAL):**<br>\n",
    "In contrast LSA, HAL utilizes matrices of “term-term” type, i.e., **the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word.** <br>\n",
    "A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure. On ther otehr hand, some two words co-ooccer with high ferquency like stop words that will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. a lot fo ways are there as solution, like remove stop words and etc, or used  **the COALS method**, in which the co-occurrence matrix is first transformed by an entropy or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. \n",
    "\n",
    "### **2. Shallow Window-Based Methods:** \n",
    "Another approach is to learn word representations that aid in making predictions within local context windows that introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. <br>\n",
    "the examples of networks are  The **skip-gram** and **continuous bag-of-words (CBOW)** models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors.<br>\n",
    "Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric.<br>\n",
    "\n",
    "Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the en- tire corpus, which fails to take advantage of the vast amount of repetition in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> The GloVe Model: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model.\n",
    "\n",
    "start with describe some methods:\n",
    "- The matrix of word-word co-occurrence counts be denoted by X, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.\n",
    "- $ Xi = 􏰅\\sum_{d} X_{id} $    be the number of times any word appears in th context(d) of word $i$.\n",
    "- $ P_{ij} = p(j|i)= \\frac{X_{ij}}{X_i}$  be the probability that word j appear in the context of word $i$.\n",
    "\n",
    "Now, Consider two words i and j that exhibit a particular aspect of interest; we are interested in the concept of thermodynamic phase, for which we might take i = ice and j = steam. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, k. For words k related to ice but not steam, say k = solid, we expect the ratio Pik/Pjk will be large. Similarly, for words k related to steam but not ice, say k = gas, the ratio should be small. For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one.<br>\n",
    "The table shows below these probabilities and their ratios for a large corpus, and the numbers confirm these expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/1600/1*tQ3K3F7l685c4e04Hjczow.png' style=\"border:2px solid #AB274F;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last line, $ {}^{P_{ki}}/_{P_{kj}}$  is **ratio** for word with ice comparied with stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.\n",
    "\n",
    "\n",
    "The above argument suggests that the appropriate starting point for word vector learning should be with **ratios of co-occurrence probabilities** rather than the probabilities themselves. Noting that the ratio $ \\frac{P_{ik}}{P_{jk}} $ depends on three words $i$, $j$, and $k$, the most general model takes the form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ F(w_i,w_j,\\widetilde{w}_k) = \\frac{P_{ik}}{P_{jk}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $ w \\in R^{d} $ are word vectors and $\\widetilde{w} \\in R^{d}$ are **separate context word vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, the right-hand side is extracted from the corpus, and $F$ may depend on some as-of-yet unspecified parameters.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with thid discription utiil now, we would like  $F$ ratios of co-occurrence probabilities is unique for every  word $w_k$ to expresse a word vector, **but the number of possibilities for F is vast. But by enforcing a few desiderata we can select a unique choice.**<br>\n",
    "First, we would like F to encode the information present the ratio $\\frac{P_{ik}}{{Pjk}}$ in the word vector space. Since vector spaces are inherently linear structures, With this aim, we can restrict our consideration to those functions $F$ that **depend only on the difference of the two target words**, modifying Eqn. (1) to,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ F(w_i - w_j,\\widetilde{w}_k) = \\frac{P_{ik}}{P_{jk}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the arguments of F in Eqn. (2) are vectors while the right-hand side is a scalar. So, $F$ can be became very compilicate. To avoid this issue,**we can first take the dot product of the arguments,**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ F((w_i - w_j)^T\\widetilde{w}_k) = \\frac{P_{ik}}{P_{jk}} (= \\frac{ F(w^{T}_{i} \\widetilde{w}_{k} ) }{ F(w^{T}_{j} \\widetilde{w}_{k}) }) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$F$ be a homomorphismZ** between the groups $(\\mathbb{R},+)$ and $(\\mathbb{R}_{>0}, ×)$, i.e.,<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ F(w^{T}_{i} \\widetilde{w}_{k} ) = P_{ik} = \\frac{X_{ik}}{ X_i } $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider **F as power low fucation ($ F(x) = exp(x) $ )**  , we can write,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ w^{T}_{i} \\widetilde{w}_{k} = log(P_{ik}) = log(X_{ik}) - log(X_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All part of equestion above is symmetry except $ log(X_i) $. **$log(X_i)$ is constant, so we show them by $b_i$** (this term is independent of $k$ so it can be absorbed into a bias bi for $w_i$). Finally, adding an additional bias b ̃k for w ̃ k restores the symmetry,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $w^{T}_{i} \\widetilde{w}_{k} + b_i + \\widetilde{b}_{k} = log(X_{ik}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equestion is a drastic simplification over First equestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange $ w ↔ \\widetilde{w} $ but also $ X ↔ X^T$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is actually ill-defined since the logarithm diverges whenever its argument is zero. One reso- lution to this issue is to include an additive shift in the logarithm, $log(X_{ik} ) → log(1 + X_{ik} )$, which maintains the sparsity of X while avoiding the divergences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have $X_{ik}$, we have $b_i$ and $\\widetilde{b}_{k}$, So we can count $w^{T}_{i}$ and $\\widetilde{w}_{k}$ as words vector for representing  words in context and finally reprsent the co-occurrence matrix for words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments. LSA model is that it weighs all co-occurrences equally, even those that happen rarely or never.Such rare co-occurrences are noisy and carry less information than the more frequent ones,\n",
    "\n",
    "but for GloVe is **proposed a new weighted least squares regression model** that addresses these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting last Eqn.  as a least squares problem and introducing a weighting function $f (X_{ij} )$ into the cost function gives us the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ J(X, w)= \\sum_{i=1}^{V} \\sum_{j=1}^{V} f(X_{ij}) (w^{T}_{i} \\widetilde{w}_{j} + b_{i} + b_{j} - log X_{ij} )^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- where V is the size of the vocabulary. It used  as loss funcation below in GloVe.is vocabulary (unique words in corpus)<br>\n",
    "- Note, **$f (X_{ij} )$ is wieght as a solution for rare probliltly that carry less information**.<br>\n",
    "- $w_i,\\widetilde{w}_j$ are the two layers of word vectors,$\\widetilde{w}_j$ context word vectors\n",
    "- $b_i, b_j$ are bias terms.\n",
    "- $X$ denotes the word co-occurrence matrix (so $X_{i,j}$ is the number of times that word $j$ occurs in the context of word $i$)\n",
    "\n",
    "\n",
    "The weighting function should obey the following properties:\n",
    "1. $f (0) = 0$. If $f$ is viewed as a continuous function, it should vanish as $x → 0$ fast enough that the ($ lim_{x→0} f(x) log^{2}x $ ) is finite.\n",
    "2. $f(x)$ should be non-decreasing so that rare co-occurrences are not overweighted.\n",
    "3. $f ( x )$ should be relatively small for large values of x, so that frequent co-occurrences are not overweighted.\n",
    "\n",
    "Of course a large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ f(x) = \\begin{cases} (x/x_{max})^{\\alpha}       & \\quad \\text{if } x< \\text{ x_max }\\\\ 1   &\\quad \\text{ otherwise.} \\end{cases} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://image.slidesharecdn.com/understandingglove-180102123818/95/understanding-glove-7-638.jpg?cb=1514896799'  style=\"width:450px;border:2px solid #AB274F;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model depends weakly on the cutoff, which they fix to $x_{max} = 100 $ for all thier experiments in orginal article.**Founding that $ α = 3/4$ gives a modest improvement over a linear version with $ α = 1$**. Although we offer only empirical motivation for choosing the value $ 3/4 $, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a).[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the product is only over pairs $i, j$ for which $X_{i,j}$ is non-zero. This means that GloVe (in contrast to word2vec with negative sampling) trains only “positive samples” and also that we don’t have to worry about the logarithm of zero.<br>\n",
    "This is essentially just weighted matrix factorisation with bias terms:[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://building-babylon.net/wp-content/uploads/2016/02/glove-matrix-factorisation-5.jpg' style='width:500px;'>\n",
    "\n",
    "**[3]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the implementation (see below), the $X_{i,j}$ are not raw co-occurrence counts, but rather the accumulated inverse distance between the two words, i.e.[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>  More inforamtion about $X$ Co-occurrence Matrix : </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we sould consider distance of two words when counting the word2vec in all corpus (Co-Occurrence Matrix) , so we count:\n",
    "\n",
    "$X_{ij} = \\sum_{ij} [ ditance between word_i and word_j ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the computational complexity of the model depends on the number of nonzero elements in the matrix $X$. **As the number of nonzero elements is always less than the total number of entries of the matrix (all of words)**, word dictionary covers allo vocabularies  of corpus.  typical vocabularies have hundreds of thousands of words, so that $|V|^2$ can be in the hundreds of billions, which is actually much larger than most corpora.  For this reason it is important to determine whether a tighter bound can be placed on the number of nonzero elements of $X$.\n",
    " \n",
    " In order to make any concrete statements about the number of nonzero elements in X, it is necessary to **make some assumptions about the distribution of word co-occurrences.** Inparticular, assumed that the number of co-occurrences of word $i$ with word $j$, $X_{ij}$ , can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$ [1]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ X_{ij} = \\frac{k}{(r_{ij})^{\\alpha}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of words in the corpus is proportional to the sum over all elements of the co-occurrence matrix X,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ |c| \\sim \\sum_{ij}X_{ij} = \\sum_{r=1}^{|x} \\frac{k}{r^{\\alpha}} = kH_{|x|,\\alpha}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have rewritten the last sum in terms of the generalized harmonic number $H_{n,m}$. <br>\n",
    "such that $X_{ij} ≥ 1$, i.e.,$ |X| = k^{1/α}$. Therefore we can write Eqn. (18) as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ |c| \\sim |x|^{\\alpha} H_{|x|,\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in how |X| is related to |C| when both numbers are large; therefore we are free to expand the right hand side of the equation for large |X|. For this purpose we use the expansion of gen- eralized harmonic numbers,[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They trained our model on five corpora of varying sizes:\n",
    "- a 2010 Wikipedia dump with 1 billion to- kens; \n",
    "- a 2014 Wikipedia dump with 1.6 billion to- kens; \n",
    "- Gigaword 5 which has 4.3 billion tokens; \n",
    "- the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Craw\n",
    "\n",
    "- We tokenize and lowercase each corpus with the Stanford to- kenizer, build a vocabulary of the 400,000 most frequent words.<br>\n",
    "- Then construct a matrix of co-occurrence counts $X$. \n",
    "\n",
    "In constructing X , we must choose how large the context window should be and whether to distinguish left context from right context. We explore the effect of these choices below. In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute $1/d$ to the total count. \n",
    "[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Arrive Loss function by  Shallow Window-Based Methods : </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in loss equestion again.<br>\n",
    "The starting point for the skip-gram or ivLBL methods is a model $Q_{ij}$ for the probability that word $j$ appears in the context of word $i$. For concreteness, let us assume that $Q_{ij}$ is a softmax,[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ Q_{ij} = \\frac{  exp(   w_{i}^{T}   \\widetilde{w}_{j} )  }{   \\sum_{k=1}^{V} exp( w_{i}^{T}   \\widetilde{w}_{k}  )       }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the implied global objective function can be written as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ J = - \\sum_{\\substack{\n",
    "   i \\in croups \\\\\n",
    "   j \\in context(i)\n",
    "  }}  log(Q_{ij}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he sum in Equestion above can be evaluated much more efficiently if we first group together those terms that have the same values for $i$ and $j$, With used the fact that the number of like terms is given by the co-occurrence matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ J = - \\displaystyle\\sum_{i=1}^{10} \\displaystyle\\sum_{i=1}^{10} X_{ij} log Q_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling our notation for $X_i = \\sum_k X_{ik}$ and $P_{ij} = X_{ij}/X_i$, we can rewrite $J$ as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ J = - \\sum_{i=1}^{V} (X-i) \\sum_{j=1}^{V} (P_{ij}log(Q_{ij})) = \\sum_{i=1}^{V} (X_i H(P_i,Q_i)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $H(P_i,Q_i)$ is the cross entropy,\n",
    "\n",
    "it is possible to optimize Eqn. above directly as opposed to the on-line training methods used in the skip-gram and ivLBL models. One could interpret this objective as a “global skip-gram” model, On the other hand, Eqn. above exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word\n",
    "vectors.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this part haven't been complite yet ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Optimization: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "        \n",
    "(1) http://www.foldl.me/2014/glove-python/    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad is a modified form of stochastic gradient descent which attempts to guide learning in the proper direction by weighting rarely occurring features more often than those that always fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{align*}J &= \\sum_{i=1}^V \\sum_{j=1}^V \\; f\\left(X_{ij}\\right) \\left( \\vec{w}_i^T \\vec{w}_j + b_i + b_j - \\log X_{ij} \\right)^2 \\\\ \\nabla_{\\vec{w}_i} J &= f\\left(X_{ij}\\right) \\vec{w}_j \\odot \\left( \\vec{w}_i^T \\vec{w}_j + b_i + b_j - \\log X_{ij}\\right) \\\\ \\nabla_{\\vec{w}_j} J &= f\\left(X_{ij}\\right) \\vec{w}_i \\odot \\left( \\vec{w}_i^T \\vec{w}_j + b_i + b_j - \\log X_{ij}\\right) \\\\ \\frac{\\partial J}{\\partial b_i} &= f\\left(X_{ij}\\right) \\left(\\vec w_i^T \\vec w_j + b_i + b_j - \\log X_{ij}\\right) \\\\ \\frac{\\partial J}{\\partial b_j} &= f\\left(X_{ij}\\right) \\left(\\vec w_i^T \\vec w_j + b_i + b_j - \\log X_{ij}\\right) \\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> python code for GloVe , 0 to 10% <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "        \n",
    "(1) http://www.foldl.me/2014/glove-python/    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove Model follow three parts:\n",
    "1.build_cooccur accepts a corpus and yields a list of co-occurrence blobs (the $X_{ij}$ values). It calculates co-occurrences by moving a sliding n-gram window over each sentence in the corpus.\n",
    "2. train_glove, which prepares the parameters of the model and manages training at a high level, and\n",
    "3. run_iter, which runs a single parameter update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccur(vocab, corpus, window_size=10, min_count=None):\n",
    "    vocab_size = len(vocab)\n",
    "    id2word = dict((i, word) for word, (i, _) in vocab.iteritems())\n",
    "\n",
    "    # Collect cooccurrences internally as a sparse matrix for\n",
    "    # passable indexing speed; we'll convert into a list later\n",
    "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),\n",
    "                                      dtype=np.float64)\n",
    "    \n",
    "    # -- continued --\n",
    "    for i, line in enumerate(corpus):\n",
    "        tokens = line.strip().split()\n",
    "        token_ids = [vocab[word][0] for word in tokens]\n",
    "        \n",
    "    for center_i, center_id in enumerate(token_ids):\n",
    "            # Collect all word IDs in left window of center word\n",
    "            context_ids = token_ids[max(0, center_i - window_size)\n",
    "                                    : center_i]\n",
    "            contexts_len = len(context_ids)\n",
    "            \n",
    "    for left_i, left_id in enumerate(context_ids):\n",
    "                # Distance from center word\n",
    "                distance = contexts_len - left_i\n",
    "\n",
    "                # Weight by inverse of distance between words\n",
    "                increment = 1.0 / float(distance)\n",
    "\n",
    "                # Build co-occurrence matrix symmetrically (pretend\n",
    "                # we are calculating right contexts as well)\n",
    "                cooccurrences[center_id, left_id] += increment\n",
    "                cooccurrences[left_id, center_id] += increment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_glove(vocab, cooccurrences, vector_size=100, iterations=25, **kwargs):\n",
    "    vocab_size = len(vocab)\n",
    "    W = ((np.random.rand(vocab_size * 2, vector_size) - 0.5)/ float(vector_size + 1))\n",
    "    biases = ((np.random.rand(vocab_size * 2) - 0.5)/ float(vector_size + 1))\n",
    "    \n",
    "    gradient_squared = np.ones((vocab_size * 2, vector_size),dtype=np.float64)\n",
    "    gradient_squared_biases = np.ones(vocab_size * 2,dtype=np.float64)\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        cost = run_iter(vocab, data, **kwargs)\n",
    "    global_cost = 0\n",
    "\n",
    "    # Iterate over data in random order\n",
    "    shuffle(data)\n",
    "    for (v_main, v_context, b_main, b_context, gradsq_W_main,\n",
    "         gradsq_W_context, gradsq_b_main, gradsq_b_context,\n",
    "         cooccurrence) in data:\n",
    "\n",
    "        # Calculate weight function $f(X_{ij})$\n",
    "        weight = ((cooccurrence / x_max) ** alpha\n",
    "                  if cooccurrence < x_max else 1)\n",
    "\n",
    "        # Compute inner component of cost function, which is used in\n",
    "        # both overall cost calculation and in gradient calculation\n",
    "        #\n",
    "        #   $$ J' = w_i^Tw_j + b_i + b_j - log(X_{ij}) $$\n",
    "        cost_inner = (v_main.dot(v_context)\n",
    "                      + b_main[0] + b_context[0]\n",
    "                      - log(cooccurrence))\n",
    "\n",
    "        # Compute cost\n",
    "        #\n",
    "        #   $$ J = f(X_{ij}) (J')^2 $$\n",
    "        cost = weight * (cost_inner ** 2)\n",
    "\n",
    "        # Add weighted cost to the global cost tracker\n",
    "        global_cost += cost\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> python code for GloVe,  made dictionary and deep learning,  used glove pakage <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "(1) https://github.com/maciejkula/glove-python\n",
    "    \n",
    "(2) https://www.youtube.com/watch?v=Y8gKX5zMRyQ\n",
    "    \n",
    "(3) c-code: http://dsnotes.com/post/fast-parallel-async-adagrad/    \n",
    "    \n",
    "(4) http://www.foldl.me/2014/glove-python/    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2816 entries, 0 to 2815\n",
      "Data columns (total 5 columns):\n",
      "created_at          2816 non-null object\n",
      "clean_text          2816 non-null object\n",
      "Price_label(0,1)    2816 non-null int64\n",
      "text                2816 non-null object\n",
      "label               2816 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 132.0+ KB\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/apple/Documents/Programming/pyfile/DataBase/project_data/Clean/2Tesla_label_from_2010-06-29_to_2019-02-26_2019227.csv\"\n",
    "df_data = pd.read_csv(path)\n",
    "df_data = df_data.dropna()\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "from glove import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    \"\"\"\n",
    "    Read corpus from regular text file \n",
    "    \"\"\"\n",
    "    \n",
    "    delchars = [chr(c) for c in range(256)]\n",
    "    delchars = [x for x in delchars if not x.isalnum() ] \n",
    "    delchars.remove(' ')\n",
    "    delchars = ''.join(delchars)\n",
    "    table = str.maketrans(dict.fromkeys(delchars))\n",
    "    with open(filename, 'r') as datafile:\n",
    "        for line in datafile:\n",
    "            yield line.lower().translate(table).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path\n",
    "get_data = read_corpus(file_path)\n",
    "corpus_model = Corpus()\n",
    "corpus_model.fit(get_data, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Nerune network for glove by glvoe pakage: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10 training epochs with 8 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n"
     ]
    }
   ],
   "source": [
    "epochs =10\n",
    "no_threads = 8\n",
    "\n",
    "glove1 = Glove(no_components=100, learning_rate=0.05)\n",
    "glove1.fit(corpus_model.matrix, epochs=epochs, no_threads=no_threads, verbose=True)\n",
    "glove1.add_dictionary(corpus_model.dictionary)\n",
    "\n",
    "# glove.save('model/articles_glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('price', 0.9923117028946787),\n",
       " ('use', 0.9918404442559923),\n",
       " ('sell', 0.9910183531883359),\n",
       " ('roadster', 0.9906272228332904)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar('tesla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model:\n",
    "glove.save('model/articles_glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model:\n",
    "from glove import Glove\n",
    "glove = Glove()\n",
    "glove = glove.load('model/articles_glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.59905363e-03,  4.33933734e-03, -8.26979341e-06, ...,\n",
       "        -2.66736795e-04,  1.20865229e-03, -3.55844323e-03],\n",
       "       [-8.33066929e-04, -4.07489482e-03,  2.74375639e-03, ...,\n",
       "         3.49241259e-03,  3.64937571e-03,  4.41169333e-03],\n",
       "       [ 8.35388952e-03, -8.55032595e-03, -6.29496506e-04, ...,\n",
       "        -4.33693457e-03, -1.01679835e-02, -2.84578046e-03],\n",
       "       ...,\n",
       "       [ 3.90595194e-03, -1.27885576e-03, -1.11353398e-03, ...,\n",
       "        -3.18586296e-03, -1.08159986e-02, -7.27336574e-03],\n",
       "       [ 6.87972373e-03, -4.55004980e-05,  5.07972048e-03, ...,\n",
       "        -2.81529755e-03, -3.06297970e-03, -2.95463692e-03],\n",
       "       [ 6.77672095e-03, -2.33436081e-03, -1.30529082e-03, ...,\n",
       "        -3.80685153e-03,  1.13821470e-03, -9.61998990e-03]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove1.word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Load pre_existing model for glove by glvoe pakage: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "glove = Glove()\n",
    "\n",
    "path_file = '/anaconda3/lib/python3.6/farhad/data/Glove/glove.first-100k.6B.50d.txt'\n",
    "stanford = glove.load_stanford(path_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.95897  ,  0.86149  , -0.53064  , -0.19908  ,  0.42945  ,\n",
       "        0.93177  ,  0.067319 , -0.21413  ,  0.39488  , -0.53561  ,\n",
       "        0.42881  , -1.3334   , -0.038192 , -0.15667  ,  0.94351  ,\n",
       "       -0.21873  , -0.15586  ,  0.084439 , -0.058604 , -0.55145  ,\n",
       "       -0.53281  ,  1.2434   ,  0.63441  ,  0.79234  ,  0.0097936,\n",
       "       -1.7124   , -0.77291  , -1.0024   , -0.69472  , -0.50487  ,\n",
       "        3.0517   ,  1.4981   , -0.32957  , -0.53871  , -0.21201  ,\n",
       "       -0.14259  , -0.02706  ,  0.5858   , -0.56642  , -0.55984  ,\n",
       "       -0.60905  , -0.57062  ,  1.3338   ,  0.67097  ,  1.0643   ,\n",
       "       -0.4181   , -0.44273  , -1.0158   , -0.35795  , -0.31111  ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford.word_vectors[stanford.dictionary['women']][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('volt', 0.6956429105311589),\n",
       " ('westinghouse', 0.678256119091891),\n",
       " ('ev1', 0.6599545102145767),\n",
       " ('sandisk', 0.6567678114984614)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford.most_similar('tesla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AA00FF;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> improve my_pakage for glove by glove pakage,pre_existing dictionar and read dictionary: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "from glove import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from farhad.TextAwsome import Fglove_nltk, Fglove_glove\n",
    "#from farhad.AwesomeTextTools import * # imrove for text Embadding with used keras and tensorflow pakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Fglove_glove():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __info__():\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def Choose_knid_of_StanfordModel(self,resourse=('wikipedia'  or 'twitter'), dim=('100d' or '50d' or '25d')):\n",
    "        if resourse=='wikipedia':\n",
    "            if knid=='100d':\n",
    "                pass\n",
    "            elif knid=='50d':\n",
    "                self.model_kind = '/anaconda3/lib/python3.6/farhad/data/Glove/glove.first-100k.6B.50d.txt'\n",
    "                self.size = 50\n",
    "            elif knid=='25d':\n",
    "                pass\n",
    "            \n",
    "        elif mode=='twitter':\n",
    "            if knid=='100d':\n",
    "                pass\n",
    "            elif knid=='50d':\n",
    "                self.model_kind = '/anaconda3/lib/python3.6/farhad/data/Glove/glove.twitter.27B.50d.txt'\n",
    "                self.size = 50\n",
    "            elif knid=='25d':\n",
    "                self.model_kind = '/anaconda3/lib/python3.6/farhad/data/Glove/glove.twitter.27B.25d.txt'\n",
    "                self.size = 25\n",
    "               \n",
    "        else:\n",
    "            print(\"not existed as resourse in database \\n you should choose other resourse or add your new dataset\")\n",
    "    \n",
    "        \n",
    "    def read_corpus(filename):\n",
    "    \"\"\"\n",
    "    Read corpus from regular text file \n",
    "    \"\"\"\n",
    "    \n",
    "    delchars = [chr(c) for c in range(256)]\n",
    "    delchars = [x for x in delchars if not x.isalnum() ] \n",
    "    delchars.remove(' ')\n",
    "    delchars = ''.join(delchars)\n",
    "    table = str.maketrans(dict.fromkeys(delchars))\n",
    "    with open(filename, 'r') as datafile:\n",
    "        for line in datafile:\n",
    "            yield line.lower().translate(table).split(' ')\n",
    "            \n",
    "    def Myself_Model(self, cropus_path, save=None, back_corpus=None,  epochs =10, no_threads = 8, no_components=100, learning_rate=0.05 ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.get_data = read_corpus(cropus_path)\n",
    "        corpus_model = Corpus()\n",
    "        corpus_model.fit(self.get_data, window=10)\n",
    "        if back_corpus != None:\n",
    "            yield corpus_model\n",
    "\n",
    "        #self.glove = Glove()\n",
    "        glove = Glove(no_components=no_components, learning_rate=learning_rate)\n",
    "        glove.fit(corpus_model.matrix, epochs=epochs, no_threads=no_threads, verbose=True)\n",
    "        glove.add_dictionary(corpus_model.dictionary)\n",
    "        \n",
    "        if save!= None :\n",
    "            #save = 'model/articles_glove.model'\n",
    "            self.glove.save(save)\n",
    "            \n",
    "        self.model = glove\n",
    "        return  self.glove\n",
    "    \n",
    "    def glove_sent2vec(self,sent, number=0, model=self.model):\n",
    "        words = str(sent).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        M = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                ww = model.word_vectors[model.dictionary[w]][:]\n",
    "                M.append(ww)\n",
    "            except:\n",
    "                continue\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis=0)\n",
    "        if type(v) != np.ndarray:\n",
    "            self.problem[number] = M\n",
    "            return np.zeros(self.size)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    \n",
    "    def extract_vec(self, model=self.model):\n",
    "        #self.open_glove_file()\n",
    "        self.data_glove = [self.glove_sent2vec(x, num, model) for num,x in enumerate(self.get_data)]\n",
    "        print('Lenght of data:',len(self.data_glove))\n",
    "        print('lenght of features:',len(self.data_glove[11]))\n",
    "        return self.data_glove\n",
    "\n",
    "    \n",
    "    def Pre_Existing_Model(self,cropus_path):\n",
    "        \n",
    "        \n",
    "        self.get_data = read_corpus(cropus_path)\n",
    "        \n",
    "        glove = Glove()\n",
    "        stanford = glove.load_stanford(self.model_kind)\n",
    "        self.model = stanford\n",
    "        \n",
    "        return stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyTweets ::\n",
    "cropus_path = \"/Users/apple/Documents/Programming/pyfile/DataBase/project_data/Clean/2Tesla_label_from_2010-06-29_to_2019-02-26_2019227.csv\"\n",
    "\n",
    "# BigTweets ::\n",
    "#cropus_path = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Fglove_glove()\n",
    "mymodel = glove.Myself_Model(cropus_path, save=None, back_corpus=None,  epochs =10, no_threads = 8, no_components=100, learning_rate=0.05 )\n",
    "vectors = glove.extract_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2 = Fglove_glove()\n",
    "\n",
    "#resourse=('wikipedia' or 'twitter'), dim=('100d' or '50d' or '25d')\n",
    "glove2.Choose_knid_of_StanfordModel(self,resourse='twitter', dim='50d')\n",
    "\n",
    "stanford = glvoe2.Pre_Existing_Model(self,cropus_path)\n",
    "vectors  = glvoe2.extract_vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> python code for GloVe by pre_existing dictionary <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import punkt  # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Load Data: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2816 entries, 1 to 3274\n",
      "Data columns (total 3 columns):\n",
      "created_at          2816 non-null object\n",
      "clean_text          2816 non-null object\n",
      "Price_label(0,1)    2816 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 88.0+ KB\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/apple/Documents/Programming/pyfile/DataBase/project_data/Clean/2Tesla_label_from_2010-06-29_to_2019-02-26_2019227.csv\"\n",
    "df_data = pd.read_csv(data_path)\n",
    "df_data = df_data.dropna()\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_file = '/anaconda3/lib/python3.6/farhad/data/Glove/glove.first-100k.6B.50d.txt'\n",
    "embeddings_index = {}\n",
    "with open(path_file, encoding='UTF8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except ValueError:\n",
    "            pass\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AB274F;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> Glove: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glove = [sent2vec(x) for x in df_data.clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1777471e-02,  3.8854595e-02,  1.3073294e-01,  8.8154934e-02,\n",
       "        7.5900324e-02,  6.7043714e-02, -8.8376492e-02, -3.2602184e-02,\n",
       "       -4.5013085e-02,  9.6964978e-02,  5.8779243e-02,  1.0359993e-01,\n",
       "       -5.8047850e-02, -1.3063227e-01,  1.3075033e-01,  1.5174074e-01,\n",
       "       -3.6956672e-02, -1.6907761e-02,  1.7658418e-02, -1.0020012e-01,\n",
       "        2.5143586e-02,  3.2064557e-02,  1.6299194e-01,  2.0805253e-02,\n",
       "        5.5056568e-02, -4.1618153e-01, -6.9847889e-02, -7.7121764e-02,\n",
       "        6.3571259e-02, -1.5618420e-01,  6.6336137e-01,  7.0837297e-02,\n",
       "       -2.2342558e-01, -2.3988081e-02, -7.3592146e-03, -1.7250635e-01,\n",
       "        2.0996990e-02, -1.1576416e-01,  6.3237943e-02,  3.0372256e-02,\n",
       "        5.3108312e-02, -3.3725314e-02, -9.9265322e-02,  1.4586908e-01,\n",
       "        7.8060992e-02, -3.8546309e-02, -4.7269735e-02,  1.4707953e-01,\n",
       "       -3.9680864e-04,  2.1965217e-02], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_glove[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of data: 2816\n",
      "lenght of features: 50 50\n"
     ]
    }
   ],
   "source": [
    "print('Lenght of data:',len(data_glove))\n",
    "print('lenght of features:',len(data_glove[1]),len(data_glove[11]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:40px;border: 4px solid black;background-color:#AA00FF;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4> My package for Glove, used pre_existing dictionary pakage that ready by NLP group by Stanford University: </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farhad.TextAwsome import Fglove_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "        ------------------------------------------------------------------- \n",
      "\n",
      "        Funcation:\n",
      "        0. model_GLove = Text_Embadding(df.text) \n",
      "        \n",
      "        \n",
      "\u001b[92mChoose one of dicationary below:\u001b[0m\n",
      "        1. model_GLove.Glove_100k_6B_50d()\n",
      "        2. model_GLove.Glove_twitter_27B_25d()\n",
      "        3. model_GLove.Glove_twitter_27B_50d()\n",
      "        \n",
      "\u001b[92mNext:\u001b[0m\n",
      "        4. data_embedding = model_GLove.data2vec()  \n",
      "\n",
      "        or \n",
      "\n",
      "        5. sent_embedding = model_GLove.sent2vec(text) just for a sentences\n",
      "        \n",
      "        \n",
      "\u001b[92mFor saving file:\u001b[0m \n",
      "\n",
      "        6. model_GLove.save_file(name_file_save)\n",
      "\n",
      "        ------------------------------------------------------------------------\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "glove_model = Fglove_nltk()\n",
    "glove_model.__info__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_model = Text_Embadding(df_data.clean_text)\n",
    "glove_model.Glove_100k_6B_50d()\n",
    "data_embedding = glove_model.data2vec()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of data: 2816\n",
      "lenght of features: 50 50\n"
     ]
    }
   ],
   "source": [
    "print('Lenght of data:',len(data_embedding))\n",
    "\n",
    "print('lenght of features:',len(data_embedding[1]),len(data_embedding[11]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1777471e-02,  3.8854595e-02,  1.3073294e-01,  8.8154934e-02,\n",
       "        7.5900324e-02,  6.7043714e-02, -8.8376492e-02, -3.2602184e-02,\n",
       "       -4.5013085e-02,  9.6964978e-02,  5.8779243e-02,  1.0359993e-01,\n",
       "       -5.8047850e-02, -1.3063227e-01,  1.3075033e-01,  1.5174074e-01,\n",
       "       -3.6956672e-02, -1.6907761e-02,  1.7658418e-02, -1.0020012e-01,\n",
       "        2.5143586e-02,  3.2064557e-02,  1.6299194e-01,  2.0805253e-02,\n",
       "        5.5056568e-02, -4.1618153e-01, -6.9847889e-02, -7.7121764e-02,\n",
       "        6.3571259e-02, -1.5618420e-01,  6.6336137e-01,  7.0837297e-02,\n",
       "       -2.2342558e-01, -2.3988081e-02, -7.3592146e-03, -1.7250635e-01,\n",
       "        2.0996990e-02, -1.1576416e-01,  6.3237943e-02,  3.0372256e-02,\n",
       "        5.3108312e-02, -3.3725314e-02, -9.9265322e-02,  1.4586908e-01,\n",
       "        7.8060992e-02, -3.8546309e-02, -4.7269735e-02,  1.4707953e-01,\n",
       "       -3.9680864e-04,  2.1965217e-02], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_embedding[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid black;background-color:#E6BF00;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> LogisticRegression <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, xtrain_glove, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(xtrain_glove, train_target)\n",
    "    print('Training LogisticRegression Classifier for {} is complete!!'.format(class_name))\n",
    "    submission[class_name] = classifier.predict_proba(xtest_glove)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "submission.to_csv('submission_glove_LogisticRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud_I = WordCloud(max_font_size=None, stopwords=stop,scale = 2,colormap = 'Dark2').generate(q_I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
