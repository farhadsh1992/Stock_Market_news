{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text classfication</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><h2><b>Refrences:</b></h2></font>\n",
    "\n",
    "https://developers.google.com/machine-learning/guides/text-classification/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 1: Gather Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data from imdb and thier forms are Word Vectors for Sentiment Analysis.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 2: Explore Your Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraiing and testing have two lenght, \\nfirst part is text word of inteviews and\\nsecond part is word vector of them\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"\"\n",
    "training, testing = load_imdb_sentiment_analysis_dataset(data_path, seed=123)\n",
    "\n",
    "\"\"\"\n",
    "traiing and testing have two lenght, \n",
    "first part is text word of inteviews and\n",
    "second part is word vector of them\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"POSSIBLE SPOILERS<br /><br />The Spy Who Shagged Me is a muchly overrated and over-hyped sequel. International Man of Mystery came straight out of the blue. It was a lone star that few people had heard of. But it was stunningly original, had sophisticated humour and ample humour, always kept in good taste, and had a brilliant cast. The Spy Who Shagged Me was a lot more commercially advertised and hyped about.<br /><br />OK I'll admit, the first time I saw this film I thought it was very funny, but it's only after watching it two or three times that you see all the flaws. The acting was OK, but Heather Graham cannot act. Her performance didn't seem very convincing and she wasn't near as good as Liz Hurley was in the first one. Those characters who bloomed in the first one, (Scott Evil, Number 2 etc.) are thrown into the background hear and don't get many stand-alone scenes. The film is simply overrun with cameos.<br /><br />In particular, I hated the way they totally disregarded some of the scenes in IMOM. When they killed off Vanessa at the start and had Basil sat that he knew she was a fembot all along. What was the point of that? They killed off Number 2 in the first one, and now they bring him back with no explanation whatsoever. This is supposed to be a spy-spoof, I don't think any of the characters even hold a gun in the film. It just goes on a trail, further and further away from the point.<br /><br />The new characters are very unwelcome. The whole Mini-Me `make fun of my size' joke gets old very quickly. Fat Bastard is just a lame excuse for gross-out humour. In total there's about two or three good jokes. The rest are either tasteless or rehashed from IMOM.<br /><br />If this were the first movie of the series then I'd probably be easier on it. But the series started on a note of dry wit and then plummeted down to a level of gross out humour. So I say, only watch this film if you haven't seen its predecessor, because The Spy Who Shagged Me is one ultimate disappointment.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Returns the median number of words per sample given corpus.\n",
    "\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_words_per_sample(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a88a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAHMCAYAAACDeo0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xm4HUWZ+PHvS2JYRAhLREyQNSqI\nynIFlBnBBQggmwIDiETlR0QWRREFRwUVR1xhEMRhUQIuiLiACgMIqOMoS1AQgWGIuBBEiKwiDgq8\nvz+qjukc7nJuyEn6Jt/P85zndldXd1dv1W/XqdM3MhNJkiRJ7bLM4i6AJEmSpKcyUJckSZJayEBd\nkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SxriI+GFE/L86/MaIuGwhLvvmiNi2Dh8XEV9e\niMt+f0ScubCWN4r17hERd0bEIxGx6aJevyT1avziLoAk9SIifgusATzRSH5+Zv5h8ZSonTLzK8BX\nRsoXEWcDczLzAyMs70ULo1w12P9yZk5pLPvfFsayF8CngcMy88LFtH5J6okt6pLGkl0yc8XG5ylB\nekTYALEQLOH7cW3g5sWx4iV8v0payAzUJY1pEbFORGREHBgRvweurOlbRcRPI+LBiLix032jTls3\nIn4UEX+OiMsj4pROl46I2DYi5nSt47cR8do6vExEHB0Rv46I+yLi/IhYtass0yPi9xHxp4j418Zy\nxtXuHr+u674+ItaKiFMj4jNd67woIt41xDZvFxH/ExEPRcQpQDSmvTkiflKHIyJOjIh7I+LhiLgp\nIjaOiBnAG4H31u4f321s5/si4pfAXyJifHPbq+Ui4uu1/D+PiJc21p0RsUFj/OyIOD4inglcAjy3\nru+RiHhud1eaiNi1drV5sHbn2bDrGLwnIn5Zt/vrEbHcEPtnmYj4QET8rm77ORGxckQsGxGPAOOA\nGyPi10PMnxFxcETcXstyakTEYHlr/u0j4rZars/Xc6vTFenNEfHf9TjcBxwXEetHxJX1/PlTRHwl\nIiZ2betRdVv/EhFnRcQaEXFJ3e8/iIhVat7lIuLLdVkPRsR1EbHGUGWVNLYYqEtaUmwDbAjsEBGT\nge8DxwOrAu8BvhkRk2rerwLXA6sDHwWmj2I9hwO71/U9F3gAOLUrzz8BLwBeA3yoEXC+G9gX2AlY\nCXgr8CgwE9g3IpYBiIjVgdfWcs6nTvsW8IFa/l8DWw9R1u2BVwLPB1YG9gbuy8zTKd1jPlm/mdil\nMc++wM7AxMx8fJBl7gZ8g7Jfvwp8JyKeMcT6AcjMvwA7An8Y6tuQiHg+8DXgCGAScDHw3YiY0Mi2\nNzANWBd4CfDmIVb55vp5FbAesCJwSmY+lpkr1jwvzcz1hyn264CX1fXsDewwWKZ6PC4AjgFWA24D\nXtGVbUvgDkrXrY9RHqw+Tjl/NgTWAo7rmucNwHaUY7cL5UHn/ZR9swzwjppvOuXYrlXXfzDw12G2\nS9IYYqAuaSz5Tm01fDAivtM17bjM/Etm/hXYH7g4My/OzCcz83JgFrBTRDyPEoB9sAZuPwa+O4oy\nHAz8a2bOyczHKAHWnjF/l4YPZ+ZfM/NG4Eag0+r8/4APZOZtWdyYmfdl5rXAQ5TAHmAf4IeZec8g\n698JuDkzL8jMvwMnAX8coqx/B54FvBCIzLw1M+8eYftOzsw7634czPWNdX8WWA7YaoRl9uJfgO9n\n5uV12Z8Glmf+oPfkzPxDZt5POWabDLGsNwKfzcw7MvMRShC9T4yu28kJmflgZv4euGqYdXWOx7fq\ng83JPPV4/CEzP5eZj9fzYnbdzscycy5lP27TNc/nMvOezLwL+C/gmsz8RWb+H/BtoPMj2L9TAvQN\nMvOJzLw+Mx8exXZKajEDdUljye6ZObF+du+admdjeG1gr0ZQ/yCllXtNait4beXt+N0oyrA28O3G\ncm+l/MC12d2gGag9SmnRhdLqOWh3C0qr+v51eH/g3CHyPZfGtmZmMv+205h2JXAKpcX/3og4PSJW\nGmK5HYMua7DpmfkkMKeW6el6Lo3jUJd9JzC5kWeo/TrssurweOY/RiMZdF21a06n+84/M/jxmK/r\nFF37tHZjOS8i7oqIh4EvU74daWo+pP11kPHOtp8LXAqcFxF/iIhPjvQNh6Sxw0Bd0pIiG8N3Auc2\ngvqJmfnMzDwBuBtYpfab7nheY/gvwAqdkYgYR+lu0Fz2jl3LXq62fI7kTmCo7hZfBnarfb43BLq/\nMei4mxLwd8oXzfFumXlyZm4ObETpRnFUZ9JQswxZ+qK57mWAKUCnG8ujNPYd8JxRLPcPlIegzrI7\n29XLfh12WZTj+zjzB7sLJDNf1Oi+81+U4/GPN9nUck/pnq1r/N9q2oszcyXKg9mQfeBHKM/fM/PD\nmbkR5duH1wEHLMiyJLWPgbqkJdGXgV0iYocoP+BcLsqPRKdk5u8o3WA+HBETIuKfKH2AO/6X8oPJ\nnWvL5AeAZRvTvwB8LCLWBoiISRGxW4/lOhP4aERMjeIlEbEaQGbOAa6jtJB+c5iuJ98HXhQRr69d\nOd7B/AHxP0TEyyJiy7odfwH+D3iyTr6H0n97tDZvrPsI4DHg6jrtBmC/us+nMX93jnuA1SJi5SGW\nez6wc0S8ppb3yLrsny5AGb8GvCvKj4ZXpATGXx+iz/3T9X3gxRGxe90nhzLE8Wh4FvAI8FD9PcVR\nI+QfUkS8KiJeXB8oH6Z0hXlyhNkkjREG6pKWOJl5J+VHj+8H5lJaso9iXp23H+UHfvcDxwLnNOZ9\nCDiEElTfRQlwm10Z/h24CLgsIv5MCVK37LFon6UEpJdRgqqzKP2wO2YCL2bobi9k5p+AvYATgPuA\nqcB/D5F9JeAMyg9ef1fzf6pOOwvYaIj+/sO5kNKf/AHgTcDra59ygHdSHnoepPQT/8dyM/N/KAH0\nHXWd83WXyczbKC3LnwP+VJezS2b+bRRl6/giZR/+GPgN5QHl8AVYzogax+OTlP27EeVB8LFhZvsw\nsBnldwnfp/w4eEE9h/Jj1ocp3bB+xDDnj6SxJUp3OklaekXEcZQf4+0/Ut4+l+OVlG8D1k4r5zGp\ndgeaA7wxM69a3OWRNLbZoi5JLVC7e7wTONMgfWypXawmRsSylG9xgnndgSRpgRmoS9JiVt+z/iDl\nrTQnLebiaPReTnmbT6fLzu7D/MZAknpm1xdJkiSphWxRlyRJklrIQF2SJElqodH8O+VRiYjlKK/G\nWrau54LMPDYizqa8W/ehmvXNmXlD/ScR/075d8yP1vSf12VNp7zLGOD4zJxZ0zcHzqa83uxi4J2Z\nmRGxKvB1YB3gt8DemfnAcOVdffXVc5111nn6Gy5JkiQN4/rrr/9TZk4aKV/fAnXKO2RfnZmP1LcZ\n/CQiLqnTjsrMC7ry70h5H/BUyjuJTwO2rEH3scAA5T+5XR8RF9XA+zTgIOAaSqA+DbgEOBq4IjNP\niIij6/j7hivsOuusw6xZs572RkuSJEnDiYjf9ZKvb11fsnikjj6jfob75epuwDl1vquBiRGxJrAD\ncHlm3l+D88uBaXXaSpl5dX2V2TnA7o1lzazDMxvpkiRJ0pjQ1z7q9d9I3wDcSwm2r6mTPhYRv4yI\nE+t7ZwEmU/57YMecmjZc+pxB0gHWyMy76/AfgTUW1jZJkiRJi0JfA/XMfCIzNwGmAFtExMbAMcAL\ngZcBqzJCl5SFUIZkiJb8iJgREbMiYtbcuXP7WQxJkiRpVBbJW18y80HgKmBaZt5du7c8BnwJ2KJm\nuwtYqzHblJo2XPqUQdIB7qldY6h/7x2iXKdn5kBmDkyaNGJ/fkmSJGmR6VugHhGTImJiHV4e2A74\nn0YAHZS+47+qs1wEHBDFVsBDtfvKpcD2EbFKRKwCbA9cWqc9HBFb1WUdAFzYWNb0Ojy9kS5JkiSN\nCf1868uawMyIGEd5IDg/M78XEVdGxCQggBuAg2v+iymvZpxNeT3jWwAy8/6I+ChwXc33kcy8vw4f\nwrzXM15SPwAnAOdHxIHA74C9+7aVkiRJUh9E6cKtgYGB9PWMkiRJ6reIuD4zB0bK538mlSRJklrI\nQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJa\naPziLsDSLqK3fP4DWUmSpKWLLeqSJElSCxmoS5IkSS1koC5JkiS1kIG6JEmS1EIG6pIkSVILGahL\nkiRJLWSgLkmSJLWQgbokSZLUQgbqkiRJUgsZqEuSJEktZKAuSZIktZCBuiRJktRCBuqSJElSCxmo\nS5IkSS1koC5JkiS1kIG6JEmS1EIG6pIkSVILGahLkiRJLWSgLkmSJLWQgbokSZLUQgbqkiRJUgsZ\nqEuSJEktZKAuSZIktZCBuiRJktRCBuqSJElSCxmoS5IkSS1koC5JkiS1kIG6JEmS1EIG6pIkSVIL\nGahLkiRJLWSgLkmSJLWQgbokSZLUQgbqkiRJUgsZqEuSJEktZKAuSZIktVDfAvWIWC4iro2IGyPi\n5oj4cE1fNyKuiYjZEfH1iJhQ05et47Pr9HUayzqmpt8WETs00qfVtNkRcXQjfdB1SJIkSWNFP1vU\nHwNenZkvBTYBpkXEVsAngBMzcwPgAeDAmv9A4IGafmLNR0RsBOwDvAiYBnw+IsZFxDjgVGBHYCNg\n35qXYdYhSZIkjQl9C9SzeKSOPqN+Eng1cEFNnwnsXod3q+PU6a+JiKjp52XmY5n5G2A2sEX9zM7M\nOzLzb8B5wG51nqHWIUmSJI0Jfe2jXlu+bwDuBS4Hfg08mJmP1yxzgMl1eDJwJ0Cd/hCwWjO9a56h\n0lcbZh3d5ZsREbMiYtbcuXOfzqZKkiRJC1VfA/XMfCIzNwGmUFrAX9jP9Y1WZp6emQOZOTBp0qTF\nXRxJkiTpHxbJW18y80HgKuDlwMSIGF8nTQHuqsN3AWsB1OkrA/c107vmGSr9vmHWIUmSJI0J/Xzr\ny6SImFiHlwe2A26lBOx71mzTgQvr8EV1nDr9yszMmr5PfSvMusBU4FrgOmBqfcPLBMoPTi+q8wy1\nDkmSJGlMGD9ylgW2JjCzvp1lGeD8zPxeRNwCnBcRxwO/AM6q+c8Czo2I2cD9lMCbzLw5Is4HbgEe\nBw7NzCcAIuIw4FJgHPDFzLy5Lut9Q6xDkiRJGhOiNEBrYGAgZ82atcjXG9FbPg+TJEnSkiEirs/M\ngZHy+Z9JJUmSpBYyUJckSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1\nSZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJaiED\ndUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWoh\nA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElq\nIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJ\naiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQW6lugHhFrRcRVEXFL\nRNwcEe+s6cdFxF0RcUP97NSY55iImB0Rt0XEDo30aTVtdkQc3UhfNyKuqelfj4gJNX3ZOj67Tl+n\nX9spSZIk9UM/W9QfB47MzI2ArYBDI2KjOu3EzNykfi4GqNP2AV4ETAM+HxHjImIccCqwI7ARsG9j\nOZ+oy9oAeAA4sKYfCDxQ00+s+SRJkqQxo2+BembenZk/r8N/Bm4FJg8zy27AeZn5WGb+BpgNbFE/\nszPzjsz8G3AesFtEBPBq4II6/0xg98ayZtbhC4DX1PySJEnSmLBI+qjXriebAtfUpMMi4pcR8cWI\nWKWmTQbubMw2p6YNlb4a8GBmPt6VPt+y6vSHav7ucs2IiFkRMWvu3LlPaxslSZKkhanvgXpErAh8\nEzgiMx8GTgPWBzYB7gY+0+8yDCUzT8/MgcwcmDRp0uIqhiRJkvQUfQ3UI+IZlCD9K5n5LYDMvCcz\nn8jMJ4EzKF1bAO4C1mrMPqWmDZV+HzAxIsZ3pc+3rDp95ZpfkiRJGhP6+daXAM4Cbs3MzzbS12xk\n2wP4VR2+CNinvrFlXWAqcC1wHTC1vuFlAuUHpxdlZgJXAXvW+acDFzaWNb0O7wlcWfNLkiRJY8L4\nkbMssK2BNwE3RcQNNe39lLe2bAIk8FvgbQCZeXNEnA/cQnljzKGZ+QRARBwGXAqMA76YmTfX5b0P\nOC8ijgd+QXkwoP49NyJmA/dTgntJkiRpzAgbmouBgYGcNWvWIl9vr++i8TBJkiQtGSLi+swcGCmf\n/5lUkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmS\nWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJ\nklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJck\nSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCX\nJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQ\nlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWqhvgXpErBURV0XELRFxc0S8s6avGhGX\nR8Tt9e8qNT0i4uSImB0Rv4yIzRrLml7z3x4R0xvpm0fETXWekyMihluHJEmSNFb0FKhHxPoRsWwd\n3jYi3hERE0eY7XHgyMzcCNgKODQiNgKOBq7IzKnAFXUcYEdgav3MAE6r61sVOBbYEtgCOLYReJ8G\nHNSYb1pNH2odkiRJ0pjQa4v6N4EnImID4HRgLeCrw82QmXdn5s/r8J+BW4HJwG7AzJptJrB7Hd4N\nOCeLq4GJEbEmsANweWben5kPAJcD0+q0lTLz6sxM4JyuZQ22DkmSJGlM6DVQfzIzHwf2AD6XmUcB\na/a6kohYB9gUuAZYIzPvrpP+CKxRhycDdzZmm1PThkufM0g6w6xDkiRJGhN6DdT/HhH7AtOB79W0\nZ/QyY0SsSGmRPyIzH25Oqy3h2WMZFshw64iIGRExKyJmzZ07t5/FkCRJkkal10D9LcDLgY9l5m8i\nYl3g3JFmiohnUIL0r2Tmt2ryPbXbCvXvvTX9LkqXmo4pNW249CmDpA+3jvlk5umZOZCZA5MmTRpp\ncyRJkqRFpqdAPTNvAd4HdPqc/yYzPzHcPPUNLGcBt2bmZxuTLqK0zFP/XthIP6C+/WUr4KHafeVS\nYPuIWKX+iHR74NI67eGI2Kqu64CuZQ22DkmSJGlM6PWtL7sANwD/Wcc3iYiLRphta+BNwKsj4ob6\n2Qk4AdguIm4HXlvHAS4G7gBmA2cAhwBk5v3AR4Hr6ucjNY2a58w6z6+BS2r6UOuQJEmSxoQoXbhH\nyBRxPfBq4IeZuWlN+1Vmbtzn8i0yAwMDOWvWrEW+3vLm95H1cJgkSZI0BkTE9Zk5MFK+nn9MmpkP\ndaU9OfpiSZIkSerF+B7z3RwR+wHjImIq8A7gp/0rliRJkrR067VF/XDgRcBjwNeAh4Ej+lUoSZIk\naWnXU4t6Zj4K/Gv9SJIkSeqzYQP1iDgpM4+IiO8yyD8Nysxd+1YySZIkaSk2Uot6558afbrfBZEk\nSZI0z7CBemZeXwdnAX/NzCcBImIcsGyfyyZJkiQttXr9MekVwAqN8eWBHyz84kiSJEmC3gP15TLz\nkc5IHV5hmPySJEmSnoZeA/W/RMRmnZGI2Bz4a3+KJEmSJKnXf3h0BPCNiPgDEMBzgH/pW6kkSZKk\npVyv71G/LiJeCLygJt2WmX/vX7EkSZKkpVuvLeoALwPWqfNsFhFk5jl9KZUkSZK0lOspUI+Ic4H1\ngRuAJ2pyAgbqkiRJUh/02qI+AGyUmU/576SSJEmSFr5e3/ryK8oPSCVJkiQtAr22qK8O3BIR1wKP\ndRIzc9e+lEqSJElayvUaqB/Xz0JIkiRJml+vr2f8UUSsDUzNzB9ExArAuP4WTZIkSVp69dRHPSIO\nAi4A/qMmTQa+069CSZIkSUu7Xn9MeiiwNfAwQGbeDjy7X4WSJEmSlna9BuqPZebfOiMRMZ7yHnVJ\nkiRJfdBroP6jiHg/sHxEbAd8A/hu/4olSZIkLd16DdSPBuYCNwFvAy4GPtCvQkmSJElLu17f+vIk\ncEb9SJIkSeqzngL1iPgNg/RJz8z1FnqJJEmSJPX8D48GGsPLAXsBqy784kiSJEmCHvuoZ+Z9jc9d\nmXkSsHOfyyZJkiQttXrt+rJZY3QZSgt7r63xkiRJkkap12D7M43hx4HfAnsv9NJIkiRJAnp/68ur\n+l0QSZIkSfP02vXl3cNNz8zPLpziSJIkSYLRvfXlZcBFdXwX4Frg9n4USpIkSVra9RqoTwE2y8w/\nA0TEccD3M3P/fhVMkiRJWpr1GqivAfytMf63mqbFIGLkPPmUf08lSZKksaTXQP0c4NqI+HYd3x2Y\n2Z8iSZIkSer1rS8fi4hLgH+uSW/JzF/0r1iSJEnS0q2n/0xarQA8nJn/DsyJiHX7VCZJkiRpqddT\noB4RxwLvA46pSc8AvtyvQkmSJElLu15b1PcAdgX+ApCZfwCe1a9CSZIkSUu7XgP1v2VmAgkQEc/s\nX5EkSZIk9Rqonx8R/wFMjIiDgB8AZ/SvWJIkSdLSrde3vnw6IrYDHgZeAHwoMy/va8kkSZKkpdiI\ngXpEjAN+kJmvAgzOJUmSpEVgxK4vmfkE8GRErLwIyiNJkiSJ3vuoPwLcFBFnRcTJnc9wM0TEFyPi\n3oj4VSPtuIi4KyJuqJ+dGtOOiYjZEXFbROzQSJ9W02ZHxNGN9HUj4pqa/vWImFDTl63js+v0dXrc\nRkmSJKk1eg3UvwV8EPgxcH3jM5yzgWmDpJ+YmZvUz8UAEbERsA/wojrP5yNiXO12cyqwI7ARsG/N\nC/CJuqwNgAeAA2v6gcADNf3Emk+SJEkaU4btox4Rz8vM32fmzNEuODN/PIrW7N2A8zLzMeA3ETEb\n2KJOm52Zd9TynAfsFhG3Aq8G9qt5ZgLHAafVZR1X0y8ATomIqK+XlCRJksaEkVrUv9MZiIhvLqR1\nHhYRv6xdY1apaZOBOxt55tS0odJXAx7MzMe70udbVp3+UM0vSZIkjRkjBerRGF5vIazvNGB9YBPg\nbuAzC2GZCywiZkTErIiYNXfu3MVZFEmSJGk+IwXqOcTwAsnMezLzicx8kvIPkzrdW+4C1mpknVLT\nhkq/j/LPl8Z3pc+3rDp95Zp/sPKcnpkDmTkwadKkp7t5kiRJ0kIzUqD+0oh4OCL+DLykDj8cEX+O\niIdHu7KIWLMxugfQeSPMRcA+9Y0t6wJTgWuB64Cp9Q0vEyg/OL2o9je/Ctizzj8duLCxrOl1eE/g\nSvunS5IkaawZ9sekmTluQRccEV8DtgVWj4g5wLHAthGxCaV1/rfA2+p6bo6I84FbgMeBQ+v724mI\nw4BLgXHAFzPz5rqK9wHnRcTxwC+As2r6WcC59Qep91OCe0mSJGlMCRubi4GBgZw1a9YiX2/EyHkA\nmoepl3lGm797HkmSJPVHRFyfmQMj5ev1PeqSJEmSFiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2S\nJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphcYv7gKofSJ6y5fZ33JIkiQt\nzWxRlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmS\npBYyUJckSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJ\nkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJaiEDdUmSJKmFDNQl\nSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWqh8Yu7ABr7InrL\nl9nfckiSJC1JbFGXJEmSWshAXZIkSWohA3VJkiSphfoWqEfEFyPi3oj4VSNt1Yi4PCJur39XqekR\nESdHxOyI+GVEbNaYZ3rNf3tETG+kbx4RN9V5To4oPaWHWockSZI0lvSzRf1sYFpX2tHAFZk5Fbii\njgPsCEytnxnAaVCCbuBYYEtgC+DYRuB9GnBQY75pI6xDkiRJGjP6Fqhn5o+B+7uSdwNm1uGZwO6N\n9HOyuBqYGBFrAjsAl2fm/Zn5AHA5MK1OWykzr87MBM7pWtZg61CLRIz8kSRJWpot6j7qa2Tm3XX4\nj8AadXgycGcj35yaNlz6nEHSh1uHJEmSNGYsth+T1pbwvr5Ze6R1RMSMiJgVEbPmzp3bz6JIkiRJ\no7KoA/V7arcV6t97a/pdwFqNfFNq2nDpUwZJH24dT5GZp2fmQGYOTJo0aYE3SpIkSVrYFnWgfhHQ\neXPLdODCRvoB9e0vWwEP1e4rlwLbR8Qq9Uek2wOX1mkPR8RW9W0vB3Qta7B1SJIkSWPG+H4tOCK+\nBmwLrB4RcyhvbzkBOD8iDgR+B+xds18M7ATMBh4F3gKQmfdHxEeB62q+j2Rm5weqh1DeLLM8cEn9\nMMw6NIb1+uPS7GtnKkmSpEUn0sgGgIGBgZw1a9YiX++CBKC9zDPa/M15+p2/13mezjZIkiS1VURc\nn5kDI+XzP5NKkiRJLdS3ri/S4mQLvCRJGutsUZckSZJayEBdkiRJaiEDdUmSJKmFDNQlSZKkFjJQ\nlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYy\nUJckSZJayEBdkiRJaiEDdUluUGAMAAAgAElEQVSSJKmFDNQlSZKkFjJQlyRJklrIQF2SJElqofGL\nuwBSG0T0li+zv+WQJEnqsEVdkiRJaiEDdUmSJKmF7PoiLQC7ykiSpH6zRV2SJElqIQN1SZIkqYUM\n1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSphQzUJUmSpBYyUJckSZJayEBdkiRJaiEDdUmSJKmF\nDNQlSZKkFjJQlyRJklrIQF2SJElqIQN1SZIkqYUM1CVJkqQWMlCXJEmSWshAXZIkSWohA3VJkiSp\nhQzUJUmSpBYyUJckSZJayEBdkiRJaqHFEqhHxG8j4qaIuCEiZtW0VSPi8oi4vf5dpaZHRJwcEbMj\n4pcRsVljOdNr/tsjYnojffO6/Nl13lj0WylJkiQtuMXZov6qzNwkMwfq+NHAFZk5FbiijgPsCEyt\nnxnAaVACe+BYYEtgC+DYTnBf8xzUmG9a/zdHkiRJWnja1PVlN2BmHZ4J7N5IPyeLq4GJEbEmsANw\neWben5kPAJcD0+q0lTLz6sxM4JzGsiRJkqQxYXEF6glcFhHXR8SMmrZGZt5dh/8IrFGHJwN3Nuad\nU9OGS58zSPpTRMSMiJgVEbPmzp37dLZHkiRJWqjGL6b1/lNm3hURzwYuj4j/aU7MzIyI7HchMvN0\n4HSAgYGBvq9PkiRJ6tViaVHPzLvq33uBb1P6mN9Tu61Q/95bs98FrNWYfUpNGy59yiDp0mIVMfJH\nkiSpY5EH6hHxzIh4VmcY2B74FXAR0Hlzy3Tgwjp8EXBAffvLVsBDtYvMpcD2EbFK/RHp9sClddrD\nEbFVfdvLAY1lSZIkSWPC4uj6sgbw7frGxPHAVzPzPyPiOuD8iDgQ+B2wd81/MbATMBt4FHgLQGbe\nHxEfBa6r+T6SmffX4UOAs4HlgUvqRxpTem1hTzttSZK0RIr0Lg+UPuqzZs1a5OtdkGCsl3lGm785\nT7/z9zpPm7ehjdssSZLGhoi4vvGK8iEtrh+TSlrIDOwlSVqyGKhLS6lF8a2AJElacAbqkvrGVn5J\nkhZcm/4zqSRJkqTKQF2SJElqIbu+SGoNu8pIkjSPLeqSJElSCxmoS5IkSS1koC5JkiS1kIG6JEmS\n1EIG6pIkSVIL+dYXSWPWaN8S41tlJEljiS3qkiRJUgsZqEuSJEktZKAuSZIktZB91CVpGL30a7dP\nuySpHwzUJWkh8geukqSFxUBdksYQA3tJWnoYqEvSEs7uO5I0NhmoS5LmY/cdSWoH3/oiSZIktZCB\nuiRJktRCdn2RJC1SdpWRpN4YqEuSWs3AXtLSykBdkrTEGe2bbnwYkNRGBuqSJI2Sb8aRtCgYqEuS\n1EL9/lbAhwep/XzriyRJktRCtqhLkqQR2WIvLXoG6pIkqRXs7iPNz0BdkiRpEAsS2PvGIS1MBuqS\nJEljhN8iLF0M1CVJkgQsmm8R1DsDdUmSJC0y/f5WYEn6FsHXM0qSJEktZKAuSZIktZCBuiRJktRC\nBuqSJElSCxmoS5IkSS1koC5JkiS1kIG6JEmS1EIG6pIkSVILGahLkiRJLWSgLkmSJLWQgbokSZLU\nQktsoB4R0yLitoiYHRFHL+7ySJIkSaOxRAbqETEOOBXYEdgI2DciNlq8pZIkSZJ6t0QG6sAWwOzM\nvCMz/wacB+y2mMskSZIk9Wz84i5An0wG7myMzwG27M4UETOAGXX0kYi4bRGUrRerA39qJkQs1PyL\nYh1uw8j5nzLP0rjNLSiTx23k/G0sk8dt5PxtLJPHbeT8bSzT0njc+m3tnnJl5hL3AfYEzmyMvwk4\nZXGXaxTln9XP/ItiHW5DO9bRtvxtLNPSuA1L4za3sUxuczvW0bb8bSzT0roNbfgsqV1f7gLWaoxP\nqWmSJEnSmLCkBurXAVMjYt2ImADsA1y0mMskSZIk9WyJ7KOemY9HxGHApcA44IuZefNiLtZonN7n\n/ItiHW5DO9bRtvyLYh1uw8LPvyjW4TYs/PyLYh1uw8LPvyjW4TaMEVH77UiSJElqkSW164skSZI0\nphmoS5IkSS1koL4EiShvBO38HSLPKyLiBYuuVJLUm+HqrsWljWXS2BARL4iIFRZ3OfohIsbXv8aR\nfeYOXrJsCJCZOdjNJSK2As4G/h4Ry4524SPdsBb0htbrfPUNPp3hvv8QekG2p5eHpYW9zpGWM9qK\n9OluQw/Lf0ZjeEzWQd37xmDu6YmITWDouqsr77jG8LOfxjp7PWbLjjJ/37ShDE9XV920Yp/WMSEi\nVq7DK/dh+SNe/xGxHXAmsOriuJc8HT1cg6sB34+I9TLzybFYj4+lOnzM7dwl3VAny3AXQhTjgQsj\n4lx46g2vzr8B8C1gHeBtvQa7naA+h/nlcUREZ3pEPD8intnjspvzbR8RU4fItxLwhohYNSJ2rsMj\nXlhPJ2DuKtemPcwzEejceDYc5fq2jIjlR9jHO0XEy3tYVrPsBwPvH01ZGiYuyEzDnMNRb5pbRcRy\n9ThusqDL62G+ns7B0a6vPmhMqcPbN/f3cMvpXMMLelMbar6u63yBA4LB1rUwbl4RMaleG8Ot+8yI\nuBSGD9Zr+hER8eaI2As4ISKWW8CirdJD2dcDzoqIlYa7Nocp69PO02W+be11/tGsZ7jzYUHW0Zy3\nq26aARzZy30oIl48Up6u9W0LvDoiDgK+GhHP6nX+uoyJUR8C6/1suca0Ya//Ws8tB+wCnAEsD2w7\n2mPQWOaI52nXvCtHxNp1+AUxioehiFi73u9HOt4PUF6DPTMipowUrHe2fUHrvuGWOVLaUPM2zsNV\nO+MLo77rhyXy9YxjVdfJ80bgSWBCZs7MzCeHmXWZzHyc8u742RHx6cx8T/PEqxfSt4HPAAcBL6jz\njFSmdwIvjYg1gY8At2bmg80y1+V3yv1uYBowHfjLSMtvzPdq4EPAjoOUYXxmPlwr9J8CTwCbjXTj\n7NqfOwEB/Cwz7x9FuY4EXg+8eahld8aB1wAb1eHXR8QrgL/2UM53Aa8FDgV+O8TyjwT2ovyX3SHL\n0VX2V1H+S+/rR9remn9T4NHMvC3K6033iogfAt/MzF/2uIzmPt8PeAxYITPPrefjKsCWwDHAesDL\nRlpmY3nbAM/MzIt7KMeGlGNwYWb+aoS8bwGeB/wYuL6ea4MG39VmwMER8SCwE/By4CnnVKPch1Ee\njleNiBMy839HKn+db8Q6oGt/7w88E/iPEco/1PxbAQ8B92fmPRGxTGedNeCZDCTwpcz8fY/b8A5g\nB+CBiLgzM4/pyrI+MDszByLipxHx7czco1N3DXZuR8RMyj+wux9Yq76Od1xmPtFLmWq5ngNcHBH7\nDHY8GuseT6nHxtX0ZQY7DnXa24GpwC+BizPz3uGOQ9e+3x54tG7jT4bIfyCwY0TcDtyWmWf3+vDQ\nWM/ewCPAHzLzhhHKNI1y/v1hpGuoax3vANak/NPBQzLz4UHyvALYA9h7pPtQLfNREbFNZj7aQzme\njIjfU1qzNwDenZl/Hmm+xvrGAy8BtokS4D+bUjd3bEZp6HqIwa//cZn5fxHxTeAK4M7MXLfX9ddt\n6Oynw4Ht6z74vx7uJeOATYFNojR6rUfZzyOq9+5tgAeBn0XEec17fSNf1H38NUpDy7kR8abMnDPY\n9RER+wDrRcQVmXlNL2Xpmv/FQDTvQV3n6W7ACpR65Loeltec993A9sBytS7442jLt0hkC/49qp/5\nP8ARwA8pgdltwH49zjcNOIlS4Z9c05aBf7yGczJwLnArpQIdaXk7ATdQgoyPAl8Aduuc4/Xv+Eb+\nNwI/A1au488BnjPEspdpDO9JCcD3r+PjGtMmARfW4dcC91Dej/+c7uU05omu5b8F+BUlEPt34JXD\nbPNzOtsEbE0J7DvTNgNeN8I++xlwH/DyHo/ZTsDVlACUuq9X6trHW9Q8QQkYtgL2GWG5zwe+Vvfr\n8s3lDZF/WUrL+38CB1P+Qdi2wJeAjwP/NMpz+DDgJ3X7/gLs3nU8HgU+ATxrmGVE1/JuBK4HvkgJ\n0IZb/xa17O8DNhwm327AtcDnKK1f7wZW6WF//Ufdrj2Hywu8HfgB5YZ5PXBKj/tvVHVALfc1wAu7\n0sf1uL5D67nyIWA2sFpj2jvqNryS0op2bI/L3IcSqKxS99f3mseW0jp8K/CJRvq1wLcHOwcaaasB\npwJ/AA4b5XnZuabGAScAW9XxZbryTWoMnwOcOcJyt63772hKHfM5YM3Blj3IvIcAvwA+BfwvJbDs\nzrMfcBOlDnw78BXgPb1ubx3eE7i9Xj9fAPYdoUw/q9fdE8BGPe7ft9VjPqkenxMG2fdTKfXMDxni\n/tCYZ7d6zm3cy/ncWMcE4HDKveJAYINRnh/rUO4XdwPTuo9jPcZ/pev6r9v9XcoD3lp1f/8ZeGkv\n50JXWQ6m1Pvr1/EVe5xvDeAy4F7gX4a7lhrTdgaurMNXAv8xXHkp3xb8HHgvcGEt59qD7Kc3UR5c\nPwL8Edi11+2v8x9Vz6fvUa7DdQbZRz+lND4+DmwzimVPo9Rrk4FTgKuAqaMp36L6LPYC+Ok6ILAy\n8JU6/H5KwDSOGmwNM99ewK8pAdo2wP8AX2hMfxvl5vEZSoX9O+DIrmVMoFbIlJvyScBHG9NnUCrv\nFer4pFq+TiV1APCvwL7AB+rFexZdwUPXOp9dy/y/wGeHyLMC5Ua4Qt0/h9fKoVN5bwgs28jffHjY\nmVJxBvAM4HjKDfGfu9YRlCD99Mb2TQUupgSUn63LuRF4U3O+ruXsC3y97uf1htnuzj47EPhqPWYf\nA/4LuJN5weIywOrAN2vZTga+TQnKDm2UfbAHlu2Ab9Rjv/Jg5e3KvyYlCLgGOKimTaYENB8HXjXM\nvGsx74FgEnA+Jfh/N6WSHVfHdwI2pgTSH6vnS+dGtAb1Rsz8AcYESgt8Z/lfpgRqUwYpx4sa+QZq\nvmMGOxaUIOBq5gVUe9Tj/C4ageoQ2zsAvLOeE69qlHuZruN7bD1+RwLfp9zAl2P4B5SVgHOHqwOA\nF1IfQIDnApfX/bsysDvlxtPrjX2rOv+K9XhcxvwP+J+iXDtHUq6H8ZSv81cYYbm7NfbTpcAzavqm\njf20AaUx4PjGfEMG65TA8ROUB7AXUYKR99Zp0yjfFA5XpnUbw++mBid1vHMMJwHfoTwsPbOOf5Yh\nbuKUYOTnzAvGBoB/owRzk0coz3MpgcYGjf1xF40Hs3o+vL2TVsv0SsrvjZ47Uh3TuLbfQ3lgHEep\nq0+n64GfUpdsSLlmV6zrvbLrfIjB1lHHjwPWrse8c65M4KkPQq+k1OFvBFYdovwrUL59ewj416HW\n2Z1OaSE9ue6nTSmNU++p27MxsGkP18R4yreoJwOfBjZvTNuMUo8dTrk2/3H9N8r9GuY1vuwBPEyt\nP7v3xTDr/xiwed2Gw4CbgelD7Pfu8b2AE+vxeG0jfcIQ63sjJdh9O+UhakJNX3+I/KcBe9Xh5YEP\nUwLdtRp5XkHpavuiOr4nJWjvKVgH/pnyzRSUa/57zXOQ8jB1AaXOeysloG8eh+HudS+j3FM/1Ug7\noS5jyIadxfVZ7AVY2j+DVGCrUm4SZ9STvBN0vBkYGGY5bwA+2Bhfg/IEe1qddjPla6rPUW5Qx1Ja\naD7emGcDyk36K/UkPoCuQLuWbcPG+AqUinFVSvBwar1gd6G0SJ/Ulf8V1JsDpaK7iRIIfINy0z10\niO3bA/h9vSjHUQKY79cK4hvUyp7GwwOlsjuWUklu2Zh+PPB54BWDrGcF4J8oFddKlG4jX6fcMMZT\nKswDat7mDWs7SqXaCbDPprRarUQJxl/ftZ5O4Lxs3acX13UtQ7mBvqKu6/OUbzNeW5f54ppnBvD2\nuowVG8s9iNJ68Zm67F0pQdtBwMRBtncSsHUd3h54NfBJSoXaeWh7Tl3GcQzywEgJ5o8CnlX30fha\n1pMo53DnwecwSiv3pfV8eTElmHlvLfNZdf83W2TeQwkib2Ve69XylJvvOTQCoTrvF+q0znWzBSUQ\n+iyNVkHmBQEPAsc00nev+/8wBn/42Z/Sanpo3c79avk2pwR2J9R8z6/TZ1JaDs9j3jc1h1BuiIO1\nFm9NCaa+zRB1ACVofm/dhyvWad+gtDx+qe73CynX8ZA3q8Y616M8nBxFuf6Xa5Rz9VqWi+vfTrB9\nELDHCMvdv+7fZkv6/6vnUvOhYzIlWP9II+2nwBVdyzuE8iA7hdJH9gRKvfBg3c+3MMTDMaU+WInS\nSno65eFkAiUA+JdGnq0pQeZ7Ka1tH6TUM/9Jo3Wya9lr1fKc3EjbnHJuf4r5g4fu+n5iPXbP7tpv\nxzW2+Z2UB5MbgdVr+jMpgctQDw/Nuuk9dRvuYt419GxK/f61TlqzbqLUrx+v2905H2ZQH2oHWcd+\nlC5OJ1Ku76825juScr0cVqefQXlw2IlyDe/b2a7G8g6m1CEfptSft1OD1O51d823PeUboVc20p5P\nebg/pZ4rQzY41PxbAnfU83IFSt10ah3fm/Jw/p7GdXAZT73+96B8o9A5XtMp37TuMNLxaqQdUY/5\n9+o+eCtwySD7qnkcdqI8SKxPaRD4cD0HN6U8OO/Rlb8TkL+c8oD8w8a0d1Hq5Ald61umpn+4jo+j\nxBY3URrylqfUfUdQGkI+QG1Io9zj5gA7j3AMJlGuq/dRrvPmw8M29e+yddln1OmduulIauv+UPu3\nHsvjKXXaqxvpJ1OulWcMV75F/VnsBfBTDwRs1DiZjwDmAs+v4wdQAqcpdXywi3qneqFMaKSdCPyG\n0rrTqVgmUCrBkyitCz9pXviU1oOHgbfV8TPqxbA35Sn9Fho3lZpnN0rf6mfV8U5gtivlK921G3l3\nrmX6KKUyX59Sub6dchO9g0Yrftd6plEq7E6QO6NeVC/qyrcCJXCeWMc/Vi/Il9TxNShf8T+7jnff\nPN9ECRS6g+vp9Th0dy84su7HMylB4isoFdWXKDeI3wIvbuQ/lBJYHDtIhfIGSlD6fuBHtUKZW4/l\nRo3y3VjPmV2Bsxrlm0Xp538BJdBYqe63symBXneF9VzKjediSmW3CiU4O7rus846n9193LuW8yxK\nv87D6vjxlK/MO+fCfpSuHy+u2/09SmW8cV3XVZ3j01jm1jXfK5nXsrx94xifQddX55RvQU6q0zot\nWh+u27JOHW8GAW+lfPv0lsYyXgesMcg2Hkq58RxBqdB/RgmWp9d9N5tyQzyccp5+hvJwcx9wcF3G\nmynX0FMCLEqg87M6/F7Kg2t3HfC8Oj6e0qL8hbpPV6B0UVm3ca2cwjBdBShdUzotj/9LuYF2HqLP\nodQDp1GusT8Cb2hsw60M0tpG6dZ0HPUbK8p1/gtKK+076zZ0WtgOr8t/B+UB5BbgQ41lXcG8Om8l\nyvW1Zs3fCQbPoNR9hzD4tyadFrhOXTCxHr8TKd8cfZfyDdYbKA0Nt1Cu4ROojRK1nFdTApn1Gss+\nrO7/QymB//3A+xrTN6HRhaarXC9g3gPY6cBlXfXJ6ZRvwq6lPoxSAsZvUn5PsRfw3zQC5yHWs3M9\nfpMoDx03Nvb/GpQHp843SvtQ6sqVKA+fv+s6V37KIC34lOvzR3W48/D7+sZ1fxPlmu90AbsBOKlO\n34vyULkX875leQOlq+ImlPPz3ZT6+mrgXcNs6/h6PF5Xx/ekNLLsTKmftmOQLokMfj89nnLeTqZ0\ntTqW8sB9Rz3eJwHvrHlnUOr5a2m01lOuwd9Qv52rx/P3lGt10G8lKNf5UcAudXxd5t1Xt61lGPTb\nPsr1dTWlPr2Mch9aoR738ygPai9o5D+QEsR3Gp5Oqdu9P+U6/gWlfu5cQy+lPAQ8r35uYV5j0Sso\njSGb0qg7Kfeqk+t50Amkd2X4b5vfRnl43Ypy7l7W2AczKPeRtev4yZT6u9NddC9qd90h9u++tUw7\nUmKhYykP681gfcj73OL6LPYCLK2fekK/ow4fQmnxvoxSSW1Aqax/XU/EnzOvcm2edG+n3HD2p7QG\nfZxyA92GcjP6KqWC3p1SGTZbFH9EqYC6g9QN6on8C8oNcHXKjfn7lODmJUNsz461vJ0W5X0pN4WN\nB8m7HaUi7nTxWZZyQ/xEne+/6Go1aMy7U93GTgv6skPk250SeKxc9837KTe5zer0wVpLmw9Lr6NU\ninvWZWxW99nGg2zL9+rwJ+t+67TYBeWG/JxG/jdTbnjrUir9r1C+Jl2urvM2ys2uOyg5p5bnjZQb\n3sb1+P2AEmCsWY93s0/imcAldXhP5q9AXwXsWIePodxc/60xfS1KhX8lPXwVSLkhvJHS0rJ/Paan\nUCrVyyitS52uSqtSAoJvMC9IWL5uRycYfFmd9yN1fBVK69WFdP1OgBLgfpRy01mvHqtPUc7ZXer+\nfnnNO1QQcA1dXcEay+/cqL4AbNFIfz+173Ld5t9SgtQzKQ+gb6e0CH2JcsM+hRJcPaW/b53/RuoN\ng3mB1WxKy+w/6oA6fZ16zM+s69y4Me2Imr/7waf7IW0tysPPcyn10b2UluH/ojxc7Eh5iH4X5UHq\nbkr/5uuG2IZmf/8vUs7d5SgP+mdSWm873XUOoVxPU+q6PlGP1Y00+qx3LX9ZSrBwVWd7KOft0dTW\n22Hqpisp9eMHGun7UlpK/0SpU66kfstWz6OPUr+lpDwAfoZ5fdqb5b+f0hiyByUY+sggZXgt84LX\nd1Cu/bOAw2vaeZQHk1ModcimlAeFHSjX+cGUIOpuykP4pd3Hd5B1dt7y9Z1G2gcpgcxL6zl0JvCa\nOm068xpoVqc8bJ5Bqc9m0WhsaCzvNZRzuvnN7K6UgPGcet5szFO7gC3LvG+YdqTxAEC5rroblk6k\n1KlXUR62BmtwWJZSB82lnNcfp1wLNzFMK3Qj7WXM/9uED1AeLNekNFSdyLwH4d0p33YeTGlRXpHB\nv7HckfIQ3GlZX3mY49X5XcqMem59gnn143sY5JpuzPvKev6Mo9R9P6E0fHUemNeh0VWwHrdbKA9g\nf6zrnEwJ0M+h1HXNOmVnynl5DuUh7oh6ft1Oqd/mUO7Nh1Lq+08xr5vOWyl12FtpdEsdYju2pn7j\nWsffWdd3BqVh5W5KnHE2877VvIjycH0e5Tx9SsxRl/V2yrm6I+VH0lvX/fLBeixfOdS5sbg/i70A\nS+OHcoPZrp6QH6c89U+kBCKfpwRzEyhf229GV6trXca2zPvx0qmUm8gy9eT9d0pl2GlBnkip5D9W\n17trPaEHbe2p8+xKqeBeRWkZ+DA1CB9mnh0pT7cTKd0lntKHuJF3N0pQ0Awsv8swP/TsmveGur1D\n9vej0QJf9/nxlMB42To+5MNSTduJEgjvS2mRWal7fZRWzXUoFdwPKDfuCyiV5k5deV9HqexXptys\nr6B8g3EhpeJcjXmtZ4MFJfdSWvo6lf6zKC2559Vz6GQaLU51/3yNQVpVKTeltWtZNqr76mfA0Y08\nL6W0uA13HIPyYHhlHX495Rye3tjm/SiB+uGN+bah3Hi+Rqlsg1L5n0b9WpRyo7qE2vrCvN8nfJ3S\nChyUh5rfUG6c36Cc+53g5jOUvvK7NNa7IEHA8yndTb5H7Q9d019C/Tajjm9HuWbOaBzD/SjB+sco\n30oMeqOmBM33A6cOcp1vSgkCm13G7qC0YHUeBD5U98WqlNa+pwRVzWNW/65AOf86LWob12XdDVzU\n2IYNKQ+hr6vLf0q9wVP7+7++luP/t3fm0XtU5R3/PFkwEBIWkU0ggYaAAoZFOKCy2IYqQiQEUCAQ\nAgkEMIYKQqwKxgAJi2CAg0sqKhxaQDQsUg8WD0mtKCGeIxBBrGCoS7XaQ8EN0kJu//g+w8z7/mbe\n7be8b/g933Pm/JaZuXPvnXuf+322O/PJXdYZMauyjn/F2/koInVlZGp3pEjs4/W5E/cyVLT1EHJi\nehUiQOMK59+AFvCrUCz0J7O6ovn/lcK1tyPyXlX/m8lzhd5I35CEDWiML0MK/PuQ7F7o1xzqbcpy\nNs5G5OibXr9z/OdWlCgm9f2FlKQTkbfs3ML/r/R3tTVaP77g/TQLz00pzLcZXsZuFc94A7nXZQK5\nVXx7RF4PojoE7MPF5xXKLDMsrUTEsKzdOyDF8GxEVA/HPVbI8ruKEsMP+TwYSR4WdQWFsYdkyjo0\nLh8D5hbu/zQyJpxX3y8lc6PPelX3+2Q0/kchUv5dHyfXet2OpNYaXr8O7Y5kyGxv7+ZI6ft3+q5D\nByIFPEuS3QsZiOYXrhld+H0cmjfv8r8nef2OQ2NxTy9jNiLCE5HcfZw8f+RDSIaMb7CObI1k2k8p\neLO9X55HXos7kZw61MfIAr93f7SG9ZEFfn47r9NWyGL/ILmFf3skR3vOkv5aG7pdgeF2oMV6D/99\nKVqUVhTOn4Z2SDiH6iSbsuSlq9Gim1m06+PKdkSL5r8gK8uUFup6FLIo/IgGCaF190xHFrdWYmOP\nQYRjkd+3ltaz81tNlMss8JnrO3NDVilLc73/Z/t1x6GFclxdua9Z3/3vK8ldrpciwliMOx2PBO9s\nJFS/XajHc0job1b3jCIpmUYJKUFu0j/5zwlI2TjBhc/J/i62qLsnW4RmIHKSJQUd7ONxARJ61+Hh\nIxX3F4nIHWihfwciHzd4fZ9E5OIbFHIQvG5X+rg8GAn78Wjx/hxwpF+3DClXf1Xox+xdTkDEaVGh\nHhdTO5/G1tW5XRIwH1m1r0GL+K+AM/3cTKSgbFG4/lhvZ0aqRyBL0lLKLW5Z+MR874cXqVUGsnpn\nIWOLyC3285DV73JkcVrqY6VRuMvfoHl2GFoY90fj7y11bahXou+lLpa5cK5RvP+XvG2vJSP6uTJF\n9EW0oFfObb9vIVpon6TJbiQo5+J9yODwKHn405TCNVch5fo4f9cn+/8P93vejEjvfeReoar6n0tf\nWZGR10+jufrFQr8djgjV1fSV2WMQqcqsi6egsdonibeub09FCsF5iLCdhMj0PPIkvBV+7faIGN6I\n1oVbvF1TfaxsVfGMmZJuomUAABNtSURBVGgdynYB+zKa9/UKZbMQsMklbSkzLK2hwsvq95yJy23y\nNfA45D3rk0tR15ZMRmyH5NSncWUUKWA/93d0GhpzZxRk2HJaIHg0HtO7+liYiOTndxFhP92ft6TB\nvXv4OMnG2CXkntIsz2BC4fqzkdfnYcQFMrn6FiS3ynYcGosUsYmF/51M7Y4+b0feyq3QfH8Aje1H\nyA0jjbwJYwrPusLrfWjh/CKkAF9XmH/7oZDGpSXl1SsymyIZfiOSZVnY2QVone05K3pN/btdgeF2\n+KB4EGmJDyGB90PcsuvXzEUEpXSnDqqTl270wTiqauC5QOhDvhrU9000sLxX3NMSifZrp6NY5ntp\nELfWzz6fjiwfGelppiydiitL/ne2e0C99f0BF06boni/F5GQf4pC/C55nPeZiGTsjjwP+yFl6H5K\ndoegBVKCyOpUZIX4ICJg9yHrw8PUui+Li9N4F343I6tLtjXmFL9vddnz6p5dDOk5HimKGaG8nzyB\nuZiDsBaRgV8gZecE5KreDRHHEUjQ30RO1q/1NhV37NgOEf1s94pJhXMrqbAo0wYJ8HPLqQ1j+Sry\nDtyISMBeJfcdjRTcIlnvs8sLjcMnFpVcX2axn4kW5+u8z1px8c9H8uV+5H1YjI/1wjVlSnRZTHpH\n8f5+rsw6PqGF+TwaycCyOTOq7u+pKLRlLXni7V8j4vVGRFwXkxPwaf4u7vX6TGtQdsP6k8ubET7u\nFiNC+wo5wd3E63g11fHsI5B8WUuFW79w7Zxs7CFjziI0t45HHoG5ft02SHk5AMmvi9CcX4es7F9H\n5Kws7r+4deizSBEa4+PgITS3F9FBCFjhGaWGJX/v2fjfh1qldqY/cxaS8SeSe+eq1sMsnO4mRD7H\nIrKezY+/IIXleu+ja5G8ugspdQ13Gap4Zv2GCs8gRfsMNIeW+LnTkbevGLJYFkL1D4iUj0JhjKuQ\nAv9TandheT9SSndCcn4xkhtZOM9kf19WeAdZKGgWHpgR6pOQhXoTf793I2NHtmNS5vVd4f1YanQs\ntONeJLdPRMpjJqMP8/JP9zauw2U78oQciNa6RrkgWZ7UlWhL3ax9H6AQ797LR9crMBwP8oTNLBHj\nKB/c5xeuqdlL23/vOHmplw+keU8c5GcUd0ZpVVm6gTxspsr6/gU8SRMthldQa609BG2FOc8F4Q9c\n6JyCQpceo8HiSwNSUnfdAUjgn4QWzfENhNdbC22cjYT/z/FtJ5Ey10ewUks83oJCJD7q43c0cnWe\nSp6D8DW/PgufuMbPTUCLwDaIDOyDSPptiPCMJLesZ/GzS6iNYx2JhPsKn09zvQ/2RERlQoO+aupd\nQlbUXwBfLrQhC2O5EpGfyi0cvU9+SbUVupXwia3pq6RXWewX1r9vamXHLORtmObvapS/i1U+bn5A\nXyLaUImmg3j/uvvbso43KWvLwu9HILK5l/fPQqQkTkKeorUUCHhJWTO8XVleg2VHJ/Unn29noflx\nDJL/mSdrNA2230Xz8QzqckUo2ZYVyab3ZH2C1pXLC2Pgl+QW+ouRYvU2r8NHkDxrtLtY2fbBGZnb\nBM3ZD9BhCFhF28fW/W9fJMMPQ8rHhYVzH0cE9Uzy0IYqkn4K8hYf4WP9e4gUbu79vQr4TKFtZ/s4\nn4SU8YkdjtUy79hZ3jfXoRCp5Uhhr3/nVSFUN+BbWHrbLyEntCPQmrAWEdPMAHgw8kgur28LeSjk\nrcijuQua2z9BMv9pv+b9FOQtkmcPIWVkNlL4GnlCZiB5MdnLexatYeOQknKv1zkLCf0EUkCzOTWS\nWs92VS5Ilvh7O3nO12oahAj20tH1CgzHg9qEzWxrsAN84JxacU9byUtxNH0HLSlLtGF9x2PfC+c2\n8ff1fRcu70cWmSeQNX0bKqyNHbZpCnIvn9vgmqLisIePuaNQ3N8LVHxYp65dmVVlElrQvossOB9D\nITBbUx0+cXzh72KM/e0+xr/kAnuk/30rcEThnjcX3scuaLFa4PPhQbS4zmixvxp6l9Ai8p/0JcVL\nKAljKbn/SBrvbtBy+ETdfWUW+9LYTz9fTFJbhRSmHf3c9hRiekvurVSi6SDev6SMlhTRJmWM9XFy\nPgrleQIpffcjcjrJz61EitlrVtaq+tHAAthO/amdb3ui+fa3SN5vwC3rLTynzDNSND7M9mdd5uMn\nC+eYgPI8MmVjGiJZWXjIfPIY/k3Jw/Y2o1wRqNo+eC61Xq22QsDa7QN/t0+jrXRvBi7y/+/r7W0l\nAX4uHsbmf78VxfNnOw2VhcmtomJv8TbbUuYdOxUpSzcgT2N9mGMrIVRL6atwZ2NhC58D1xXOvQvJ\n7qLVfjfkZT0UEeiLkQdkc2REOIF8i8RzgI9nc8F/Xojk+cPAvk364YPUJiIfhMILd0WK9reQ129b\nNIc+hcj8s5RvVNFKLsi+3u7K3JZeO7pegeF8IKH5hA/EY5EbddeS69pOXoqjad+3pCzRuvX9empj\nlQ9xgbqXC7770GJ6tguSTw1Su/am+iMVZYrDZ1GozpuQG7FhjoCPv/vQgpXF8o5Di8s9yC2cxZA3\nDZ8gj7HPhP1cFB7wHkTWzyL/Cu1Yr+9D3o/7oNjbzPL5RtzVO1BzgRbDWPpRfqfhH5UWe1pPUmu4\ntV8LdWgr3n+QxnuWmPgO5H6/BzjQ/zcLWYhnkXuDssTWQZeVTebb1ogstB064WUfS77j0NE+H7dE\n4Rt3ICvxOOT1+E7xfdB3h64FSKbtjwhjmTetle2Dd667Z7DnzgoUtvYuRM7vReR9at11VR6R2chC\nvEmhfneSGwLa3oShg3dYVGZGotClRcV3AJ2HUHl5X0fK+dGI1H8f9xT4NcWxsQOSRZ/Lnuk/bwJm\nlrThKGRsKSa6HuN928hLNB0prEvQ2rFpoZ2fJ9/lLkuovs/bdg4K77mIvmtJK4rMNVR89KmXj65X\nYLgf5K7YRyiJdy1c15H1LY6m/d9UWaKNUKXC3zsh0vlDZEmaR+6Sm1MvZIagnVWKw1m0qDhQ7qY8\nz89lOyfsWHdPs/CJCWiBedqF8GbeX3dRt+D69WMQobgTuUF/jxbrndvpjzb7rmEYSz/L7jj8g+YW\n+0ZJamt94Wv4SfYmz2876W+A+24UskK+3cf3apQYu6xwzUzkuj8TkZkhMWa0MN8W9aPsbFvW3ci3\nsy1+YfEClAfyTRTO0WdLP/qS9YVeTkbGO9o+uOI5Azp3qFVEM6PB1ijkps+uYdTufT8PWZ0X+Pi9\n1MfsZB9Lq6n9iFrbmzC02ZaWvWO0GUJFvof9VMQTvoaUqnEo3O0yvy4jyG9Diu3FyFNfzDVZTEk4\nG5L5l3mfHuP1WkMDgw8KbfmNl7kaeXKv8r6YhxTZzOBSn1B9KjLUFL8X044icxUbY3hwtysQRwK5\ndZoOHjq0vsXRtF8bKkt0EKpUuHcKct/9G/B0F9vYtuJAX7d3vZvyQLQI79/k2YfTJJ4Tke+feV3G\nen9XWnyRG3dbFIv5DQrxxIPUfw1JcT/L7nf4h5fTcZJaP545qESmhefv64v+c4gUH4TIW1GJnsUQ\nx6J2Mt/aKDsLGfsKsggvR4aDI+qevxt1HwSrKyfbTjcjQRlp7/f2wXXPGfC5Qy1ZfwB4oP6ct2NT\nlEz8MaTQPepzYxmS3zshhf+fvJyqPbjb2oShzbY0VWZoM4QKEehz8VBGJFMPQ0rrCORBLSow01Bo\n7SNoT/JlXqePIwPNY8XxVfesHZCR5Vso5KVyf38UrvhB8t1msm8nrCb31pZtmtA0oZp+5oL08tH1\nCsTRxssawOSrOPr0bVNliRZDlSrKPtmF4MQut7NtxYHGbsrPDdQ4JI+xP73N+z4BLO/2GOr2QT+S\n1Abg2YNGZJo8dyyKhf0Z+cei3oush00TWoegfoOiqCOr55/J96m+HCU5H9pmOcciA8QIRGz7vX3w\nEPZtkazfDVxddh4lsq9DSYnFLf/+Ht8n30lc6cfzhqgtlcoMbYZQIQ/I+YgrPE6+A8tYFNtfn6C6\nHfK87Ol/fwhZpS9HCswVNEi+rqtnZVgJIuKrvd5zyXOdpiOl4BAqPohERUJ14fyA5IL06tH1CsTR\n5gsbIOtbHB33f0uhSlXvrtv193o0VByoTdpq2U05QHXbm9b30rdCHb/NRmotGeB323aS2sZ+IMXx\nnUiJzqxn5yMXfNflZLP51mGZE8hDxuYgq+YiFId7cJtldbIj1mvbB3e5bzMyfiYK98lyFsq+XPob\nPK7f/7cHSkLuOPxrCNrXVgiVy+RHyXdJWYw8jrugfLaHqfNUor3Pvw+80/8ejRT9uylY+ev7tM12\nTPe+noxC7q5HeRTZ+zqJJl6XqucziLkgvXJkC10gEGgRZrYtkFJKv+92XfoDMxudUvq/uv9ZyqSi\n2S5oofhhSulZM5uGrCwvIxfqVPQl06eGuOo19UUuznUppR93qx69BDM7FoW8LEgp3WFmI9HivjP6\n9sLz3azfYMHMjkEE4Da0WF+UUvpOd2uVo2y+DUCZWb7G5Shu/XRERn/XjzI/g5L4FqaUPm9mRyEC\nuDKldL1fMz6l9Id+N2CAYGZHAL9LKT1VJ8M+jEjuk0hx+w7yPFyFQjAuQiEd/9OVijeBme2EjEPn\nIO/laOD3KaUVZjYHWJVSetav3RSFnnweKVcnIgI7BxH0cWhOPFHynAuR1frulNKPzew9yAL+3+gb\nCy/1ow1vRls9PphSmmNmY5AXdEukeKxMKb3SYdmHkHvQXkIK5ArUT19EO+It6rTuvYIg6oFAAOhD\n0hcga9o4FDZxW0rpZTObjj728wFgTacCNjC4MLOjUejCEifrI5DltGfI1WDAF+65wB0ppQe7XZ+h\ngJlNQdbvDwN3ppRe7Wd5k5CCfgH6+uSdZnYAIoo3ppRu62+dhwJmdh4i4zORt+ULiMAuB36LQpJu\nSSk92bVKtgh/x0uRPH5TSmnPiuvORrHpv0IW5XUoqfxK4KWU0ssV9+2ElLG3I6v0CSjM6ZPAJSml\nx/tZ/xnI23NhSul2MxuFLOsbgEtTSn/psNyWFZmNGaO6XYFAINAbKJD06Uhgn4ZIzz7AwWb2vZTS\nPW4R+a8g6b2LlNI/m9kGYLmZvZJS+jpKrHpdI6X0AzMbVgpkSulxtyi/1F+S7uU9AzxjZi8AV/jP\nMcD/Istsz8PMxqNE15OQZXkNIqzbIW/BZcBNKaX/6Fol24C/49noK6nnm9nElNJzJZfeinIOnk0p\nPW9mp6Ck3/VVJN3L/5WZXYMS0qcgop7tGPXbAaj/CjNbDyw1M5ysX4wSmDsi6Vm9gS+Z2RoKigzK\nq7i5v/XuFYRFPRAIvIbBdFMGhh5mdiRatH/e7boENj6Y2XvR3tN/BuZsDNbnDGb2BpRYuCyl9G4P\nk3sB7WRyS0rpT12tYIdoJYTKPWhnoD3vT243LNDM3o2I77z+WtPryj0KeTQuSCndNVDletnb4ooM\n2v3quYEsv5sIoh4IBGowWG7KQCCw8WFjzskxs93RntrnoQTc09BOORuFJb1TmNlmKOznkZTSTzq4\nfwe0g8uA99NgGw8GIxek2wiiHggE+qAQ47y0QNa32hgX60AgMDzhVvW/Q4nvO6JdgbqW/D6UKOYc\nBTZuBFEPBAKlGEw3ZSAQCAwFzGw0sD2wIaX0627XJxBoF0HUA4FAJSLGORAIBAKB7iGIeiAQCAQC\ngUAg0IMY0e0KBAKBQCAQCAQCgb4Ioh4IBAKBQCAQCPQggqgHAoFAIBAIBAI9iCDqgUAgEAgEAoFA\nDyKIeiAQCAwTmFkys2sLf3/UzBZ1sUqBQCAQaIAg6oFAIDB8sB6YYWbbDERh/iGsQCAQCAwSQsgG\nAoHA8MEr6CNWHwE+0ehCM5sDLAReAB4H1qeU5pvZV4GXgf2Ah83sDuB6YAzwEnBGSumnZjYbmA6M\nBXYHPgNsgj7jvh54X0rpeTNbAJzjdXsqpXTSgLY4EAgENmIEUQ8EAoHhhZuAJ8zs6qoLzGxH4BJg\nf+CPwEOIrGfYCXhHSulVMxsPHJpSesXMpgJLgOP9ur0RoR8DPAMsTCntZ2afBWYBy4CPAbumlNab\n2ZYD2dBAIBDY2BFEPRAIBIYRUkp/MLNbgQXIAl6Gg4B/TSk9D2BmdwGTC+fvSim96r9vAdxiZrsD\nCRhduG5lSumPwB/N7EXgm/7/tcDb/PcngH80s3uAe/rXukAgEHh9IWLUA4FAYPhhGTAHhaVgZiPN\n7DE/Frdw/58Lv1+GCPnewDRkPc+wvvD7hsLfG8gNRUcjK//+wJqIew8EAoEcQdQDgUBgmMEt5V9D\nZJ2U0qsppX39uBRYAxxuZls5cT6+QXFbAL/232e3Uw8zGwHsnFJaieLhtwA2b6sxgUAg8DpGEPVA\nIBAYnrgWKN39JaX0axRr/ijwMPAc8GJFOVcDS83sR7QfTjkSuM3M1gI/Am5IKb3QZhmBQCDwuoWl\nlLpdh0AgEAj0GMxs85TSn9yifjfw5ZTS3d2uVyAQCAwnhEU9EAgEAmVYZGaPAT8G1hGJnoFAIDDk\nCIt6IBAIBAKBQCDQgwiLeiAQCAQCgUAg0IMIoh4IBAKBQCAQCPQggqgHAoFAIBAIBAI9iCDqgUAg\nEAgEAoFADyKIeiAQCAQCgUAg0IMIoh4IBAKBQCAQCPQg/h8uY06jqwHtWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1068fe0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_texts = training[0]\n",
    "plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once youve verified the data, collect the following important metrics that can help characterize your text classification problem:\n",
    "\n",
    "- <b>Number of samples:</b> Total number of examples you have in the data.\n",
    "- <b>Number of classes:</b> Total number of topics or categories in the data.\n",
    "- <b>Number of samples per class:</b> Number of samples per class (topic/category). In a balanced dataset, all classes will have a similar number of samples; in an imbalanced dataset, the number of samples in each class will vary widely.\n",
    "- <b>Number of words per sample:</b> Median number of words in one sample.\n",
    "- <b>Frequency distribution of words:</b> Distribution showing the frequency (number of occurrences) of each word in the dataset.\n",
    "- <b>Distribution of sample length:</b> Distribution showing the number of words per sample in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = training[1]\n",
    "get_num_classes(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(labels):\n",
    "    \"\"\"Plots the class distribution.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    \"\"\"\n",
    "    num_classes = get_num_classes(labels)\n",
    "    count_map = Counter(labels)\n",
    "    counts = [count_map[i] for i in range(num_classes)]\n",
    "    idx = np.arange(num_classes)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Class distribution')\n",
    "    plt.xticks(idx, idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndeb/A3ee0Pm+OaRNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d444208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "plot_class_distribution(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_words_per_sample(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2.5: Choose a Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important inforamtion about choose model.<br>\n",
    "I should learn:(\n",
    "- SVM\n",
    "- PCA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 3: Prepare Your Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple best practice to ensure the model is not affected by data order is to always shuffle the data before doing anything else.\n",
    "- Second, machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "1. <b>Tokenization:</b> Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the vocabulary of the dataset (set of unique tokens present in the data).\n",
    "2. <b>Vectorization:</b> Define a good numerical measure to characterize these texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how to perform these two steps for both<b> n-gram vectors </b>and<b> sequence vectors</b>,and also using feature selection and normalization techniques to optimize the vector representations\n",
    "<h3><b><font color=\"orange\"> [Option A]</font> N-gram vectors :</b></h3><br>\n",
    "<b>[Tokenization]</b><br>\n",
    " text is represented as a collection of unique n-grams: groups of n adjacent tokens (typically, words). Consider the text <font color=\"blue\">The mouse ran up the clock</font>. Here,<br> the word <b>unigrams (n = 1)</b> are <font color=\"blue\"> ['the', 'mouse', 'ran', 'up', 'clock']</font>,<br> the word <b>bigrams (n = 2)</b> are <font color=\"blue\"> ['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock']</font>, and so on.\n",
    " \n",
    "<b><font color=\"orange\">NOTE:</font></b>  We have found that tokenizing into word <b>unigrams + bigrams</b> provides good accuracy while taking less compute time.<br>\n",
    "<b>[Vectorization]</b><br>\n",
    "- Once we have split our text samples into n-grams, we need to turn these n-grams into numerical vectors that our machine learning models can process.The example below shows the indexes assigned to the unigrams and bigrams generated for two texts.\n",
    "\n",
    ">Texts: <font color=\"blue \">'The mouse ran up the clock' and 'The mouse ran down'</font><br>\n",
    "<font color=\"black\"> Index assigned for every token:</font> <font color=\"grey \">{'the': 7, 'mouse': 2, 'ran': 4, 'up': 10,\n",
    "  'clock': 0, 'the mouse': 9, 'mouse ran': 3, 'ran up': 6, 'up the': 11, 'the\n",
    "clock': 8, 'down': 1, 'ran down': 5}\n",
    "</font>\n",
    "\n",
    "- <b>One-hot encoding:</b> Every sample text is represented as a vector indicating the presence or absence of a token in the text.\n",
    "><font color=\"blue\">'The mouse ran up the clock' =</font><font color=\"grey \">[1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]</font>\n",
    "\n",
    "- <b>Count encoding:</b>\n",
    "><font color=\"blue\">'The mouse ran up the clock' =</font><font color=\"grey \">[1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1]</font>\n",
    "\n",
    "- <b>Tf-idf encoding:</b> Ferquency words like \"the\" ,words like a will occur very frequently in all texts. So a higher token count for the than for other more meaningful words is not very useful.\n",
    "><font color=\"blue\">'The mouse ran up the clock' =</font><font color=\"grey \">[0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33,\n",
    "0.23, 0.33, 0.33]</font>, <a src=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\"><font color=\"red\">(Scikit-learn TdidfTransformer)</font></a>\n",
    "\n",
    "<b><font color=\"orange\">NOTE:</font></b> We observed that tf-idf encoding is marginally better than the other two in terms of accuracy (on average: 0.25-15% higher), and recommend using this method for vectorizing n-grams\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"orange\">NOTE:</font></b><br>There are many statistical functions that take features and the corresponding labels and output the feature importance score. Two commonly used functions are<b> f_classif</b> and<b> chi2</b>. Our experiments show that both of these functions perform equally well.\n",
    "\n",
    "<h3>Normalization:</h3><br>\n",
    "Normalization converts all feature/sample values to small and similar values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The following code puts together all of the above steps:</b>\n",
    "\n",
    "1. Tokenize text samples into word uni+bigrams,\n",
    "2. Vectorize using tf-idf encoding,\n",
    "3. Select only the top 20,000 features from the vector of tokens by discarding tokens that appear fewer than 2 times and using f_classif to calculate feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = training[0]\n",
    "train_labels = training[1]\n",
    "val_texts = testing[0]\n",
    "x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x20000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 3485716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"orange\">NOTE:</font></b><br> With n-gram vector representation, we discard a lot of information about word order and grammar (at best, we can maintain some partial ordering information when n > 1). This is called a bag-of-words approach. This representation is used in conjunction with models that dont take ordering into account, such as logistic regression, multi-layer perceptrons, gradient boosting machines, support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b><font color=\"orange\">[Option B]</font> Sequence Vectors:</b></h3><br>\n",
    "word order is critical to the texts meaning. For example, the sentences, I used to hate my commute. My new bike changed that completely can be understood only when read in order. Models such as CNNs/RNNs can infer meaning from the order of words in a sample. For these models, we represent the text as a sequence of tokens, preserving order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Sequence Vectors :</b></h3><br>\n",
    "><font color=\"grey\">Texts: <font color=\"blue\">'The mouse ran up the clock' and 'The mouse ran down'</font>\n",
    "Index assigned for every token:</font><font color=\"blue\"><br> {'clock': 5, 'ran': 3, 'up': 4, 'down': 6, 'the': 1, 'mouse': 2}.</font><font color=\"gray\"><br> \n",
    "<font color=\"orange\"><b>NOTE:</b></font> 'the' occurs most frequently, so the index value of 1 is assigned to it.\n",
    "Some libraries reserve index 0 for unknown tokens, as is the case here.\n",
    "Sequence of token indexes:</font><br><font color=\"blue\"> 'The mouse ran up the clock' = [1, 2, 3, 4, 1, 5]</font>\n",
    "\n",
    "There are two options available to vectorize the token sequences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"black\"><b>One-hot encoding: </b></font></br>\n",
    "<font>Sequences are represented using word vectors in n- dimensional space where n = size of vocabulary.<br>\n",
    "This representation works great when we are tokenizing as characters, and the vocabulary is therefore small.\n",
    "</font>\n",
    "><font color=\"blue\">\n",
    "'The mouse ran up the clock' = [<br>\n",
    "  [0, 1, 0, 0, 0, 0, 0],<br>\n",
    "  [0, 0, 1, 0, 0, 0, 0],<br>\n",
    "  [0, 0, 0, 1, 0, 0, 0],<br>\n",
    "  [0, 0, 0, 0, 1, 0, 0],<br>\n",
    "  [0, 1, 0, 0, 0, 0, 0],<br>\n",
    "  [0, 0, 0, 0, 0, 1, 0]<br>\n",
    "]\n",
    "    </front>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"black\"><b>Word embeddings:</b></font>\n",
    "<font></font><br>\n",
    "<font>Words have meaning(s) associated with them. <br> we can represent word tokens in a dense vector space (~few hundred real numbers).\n",
    "    <br>Sequence models often have such an embedding layer as their first layer.<br>This layer learns to turn word index sequences into word embedding vectors during the training process\n",
    "</font>\n",
    "\n",
    "<img src='https://developers.google.com/machine-learning/guides/text-classification/images/EmbeddingLayer.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature selection:</b>\n",
    "<font>Not all words in our data contribute to label predictions. We can optimize our learning process by discarding rare or irrelevant words from our vocabulary.<br>\n",
    "    we observe that using the most frequent 20,000 features is generally sufficient. This holds true for n-gram models as well.\n",
    "</font><br>\n",
    "<font>Lets put all of the above steps in sequence vectorization together. The following code performs these tasks:</font>\n",
    "1. Tokenizes the texts into words,\n",
    "2. Creates a vocabulary using the top 20,000 tokens,\n",
    "3. Converts the tokens into sequence vectors,\n",
    "4. Pads the sequences to a fixed sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = training[0]\n",
    "train_labels = training[1]\n",
    "val_texts = testing[0]\n",
    "\n",
    "x_train, x_val, word_index = sequence_vectorize(train_texts, val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[    0,     0,     0, ...,    28,  2086,  1383],\n",
      "       [    0,     0,     0, ..., 14360,     2,  1400],\n",
      "       [    0,     0,     0, ...,     4,  3050,  9654],\n",
      "       ...,\n",
      "       [    0,     0,     0, ...,     1,   114,   659],\n",
      "       [    0,     0,     0, ..., 10083,     2,  6170],\n",
      "       [  166,    47,     6, ...,   109,  1052,   638]], dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127acfb38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 4: Build, Train, and Evaluate Your Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>We will use <font color='blue'>TensorFlow</font> with the<font color='blue'> tf.keras</font> API for this.</font><br>\n",
    "<font>\n",
    "\n",
    "<b>Constructing the Last Layer:</b><br>\n",
    "\n",
    "- When we have only 2 classes <b>(binary classification)</b>, our model should output a single probability score. For instance, outputting 0.2 for a given input sample means 20% confidence that this sample is in class 0, 80% that it is in class 1. To output such a probability score, the activation function of the last layer should be a sigmoid function, and the loss function used to train the model should be binary cross-entropy</font>\n",
    "<br>\n",
    "- <font>When there are more than 2 classes <b>(multi-class classification)</b>, our model should output one probability score per class.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>The following code defines a function that takes the number of classes as input, and outputs the appropriate number of layer units (1 unit for binary classification; otherwise 1 unit for each class) and the appropriate activation function:</front>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "units, activation = _get_last_layer_units_and_activation(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"><b>NOTE:</b></font> <br>\n",
    "<font>\n",
    "- When the S/W ratio is small, weve found that n-gram models perform better than sequence models. Sequence models are better when there are a large number of small, dense vectors. This is because embedding relationships are learned in dense space, and this happens best over many samples.<br>\n",
    "    - Sequence models are better when there are a large number of small, dense vectors. ( This is because embedding relationships are learned in dense space, and this happens best over many samples.)\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"><b>[Option A]</b></font><b>  Build n-gram model:</b><br>\n",
    "<div><font>We refer to models that process the tokens independently (not taking into account word order) as n-gram models. Simple multi-layer perceptrons (including<font color=\"blue\"> logistic regression</font>), <font color=\"blue\">gradient boosting machines</font> and <font color=\"blue\">support vector machines </font>models all fall under this category; they cannot leverage any information about text ordering.</font></div>\n",
    "\n",
    "<div><font>We compared the performance of some of the n-gram models mentioned above and observed that<font color=\"black\"><b> multi-layer perceptrons (MLPs)</b></font> typically perform better than other options. MLPs are simple to define and understand, provide good accuracy, and require relatively little computation.</font></div>\n",
    "\n",
    "<div><font>The following code defines a two-layer MLP model in tf.keras, adding a couple of<font color=\"blue\"> Dropout layers for regularization</font> (to prevent <font color=\"blue\">overfitting</font> to training samples).</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_model(layers, units, dropout_rate, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"><b>[Option B]</b></font><b>  Build sequence model:</b><br>\n",
    "<div>We refer to models that can learn from the adjacency of tokens as sequence models. This includes CNN and RNN classes of models. Data is pre-processed as sequence vectors for these models.</div>\n",
    "<div>- The first layer in these models is an embedding layer, which learns the relationship between the words in a dense vector space. </div>\n",
    "<div>- Words in a given dataset are most likely not unique to that dataset. We can thus learn the relationship between the words in our dataset using other dataset(s).we can transfer an embedding learned from another dataset into our embedding layer. These embeddings are referred to as <b>pre-trained embeddings</b>. Using a pre-trained embedding gives the model a head start in the learning process.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>pre-trained embeddings:</b><br>\n",
    "<div>- There are pre-trained embeddings available that have been trained using large corpora, such as GloVe.<br>\n",
    "- GloVe has been trained on multiple corpora (primarily Wikipedia).<br><br>\n",
    "    \n",
    "    (We tested training our sequence models using a version of GloVe embeddings and observed that if we froze the weights of the pre-trained embeddings and trained just the rest of the network, the models did not perform well. This could be because the context in which the embedding layer was trained might have been different from the context in which we were using it.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first run, with the embedding layer weights frozen, we allow the rest of the network to learn. At the end of this run, the model weights reach a state that is much better than their uninitialized values. For the second run, we allow the embedding layer to also learn, making fine adjustments to all weights in the network. We refer to this process as using a <b>fine-tuned embedding</b>.<br>\n",
    "\n",
    "-  We found that sepCNNs, a convolutional network variant that is often more data-efficient and compute-efficient, perform better than the other models.<br>\n",
    "\n",
    "The following code constructs a four-layer sepCNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'><b>Train Your Model:</b></font><br>\n",
    "Now that we have constructed the model architecture, we need to train the model. Training involves :<br>\n",
    "1. making a prediction based on the current state of the model,<br>\n",
    "2. calculating how incorrect the prediction is, <br>\n",
    "3.  updating the weights or parameters of the network to minimize this error.<br>\n",
    "<br>\n",
    "<b>hree key parameters to be chosen for these processes:</b>\n",
    "- <b>Metric:</b> How to measure the performance of our model using a metric. We used accuracy as the metric in our experiments.\n",
    "- <b>Loss function:</b> A function that is used to calculate a loss value that the training process then attempts to minimize by tuning the network weights. For classification problems, cross-entropy loss works well.\n",
    "- <b>Optimizer:</b> A function that decides how the network weights will be updated based on the output of the loss function. We used the popular <font color=\"blue\"><a src=\"https://arxiv.org/abs/1412.6980\">Adam</a></font> optimizer in our experiments.<br>\n",
    "In Keras, we can pass these learning parameters to a model using <b><a src=\"https://keras.io/getting-started/sequential-model-guide/#compilation\"> the compile method.</a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = explore_data.get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val = vectorize_data.ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = build_model.mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('IMDb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Step 5: Tune Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>We had to choose a number of hyperparameters for defining and training the model. We relied on intuition,</div>\n",
    "<div>Lets take a look at some of the hyperparameters we used and what it means to tune them:</div>\n",
    "- <b>Number of layers in the model:</b> <br> oo many layers will allow the model to learn too much information about the training data, causing overfitting. Too few layers can limit the models learning ability, causing underfitting.<br>For text classification datasets, we experimented with one, two, and three-layer MLPs.<br>\n",
    "- <b>Number of units per layer: </b><br>The units in a layer must hold the information for the transformation that a layer performs. <br>For the first layer, this is driven by the number of features.<br> In subsequent layers,Try to minimize the information loss between layers. <br>(We tried unit values in the range [8, 16, 32, 64], and 32/64 units worked well.)<br>\n",
    "- <b>Dropout rate: </b><br>Dropout layers are used in the model for regularization. They define the fraction of input to drop as a precaution for overfitting. Recommended range: 0.20.5.<br>\n",
    "- <b>Learning rate: </b><br>This is the rate at which the neural network weights change between iterations. A large learning rate may cause large swings in the weights, and we may never find their optimal values. A low learning rate is good, but the model will take more iterations to converge. It is a good idea to start low, say at 1e-4. If the training is very slow, increase this value. If your model is not learning, try decreasing learning rate.<br>\n",
    "- <b>Kernel size: </b><br>it is specific to our sepCNN mode<br>The size of the convolution window. Recommended values: 3 or 5.\n",
    "- <b>Embedding dimensions: </b><br>it is specific to our sepCNN mode<br>The number of dimensions we want to use to represent word embeddingsi.e., the size of each word vector. Recommended values: 50300. In our experiments, we used GloVe embeddings with 200 dimensions with a pre- trained embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Step 6: Deploy Your Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Appendix: Batch Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
